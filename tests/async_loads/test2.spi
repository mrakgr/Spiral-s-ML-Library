// Every time I try out Cutlass it feels like I waste nearly a week before giving up. 
// I am depressed that I'll be doing a matrix multiply on my own again. Anyway, I'll be investigating async loads again.

// Here we have an example that mostly writes and reads from memory.

// If I add that `nanosleep 300` call, in Nsight Compute the memory occupancy drop from 92% to 37% and the runtime goes from 8ms to 23ms.

// Ordinarily there'd be no way to speed this up, but if I interleaved memory loads with computation, 
// I should be able to double the memory occupancy...

open corebase
open corecuda
open coreext
open refm
open rangem
open tensorm

// Maps all the elements of a tensor given the mapping function.
inl map forall dim a b. (f : a -> b) (from : tensor dim a) (to : tensor dim b) : () =
    open pipeline
    inl pipe = make_pipeline_thread()
    inl io = 
        zip from to
        |> factorize_sizeof_16
        |> factorize_fst threads_per_block()
        |> reorder (fun ((outer,tid),sizeof_4) => tid,outer,sizeof_4)
    assert (fst io.dim = threads_per_block()) "fst io.dim = threads_per_block()"
    inl io = io |> apply thread_index()
    inl s_from,_ = tensor_cuda.tensor_create_extern_shared 0 (threads_per_block(), snd io.dim)
    inl s_from = s_from |> apply thread_index()
    inl l_from, l_to = tensor_create (snd io.dim) |> unzip
    loop.lookahead blocks_in_grid(fst io.dim) fun is_init i future_i =>
        inl from, to = io |> apply i |> unzip
        if is_init then
            producer_phase pipe fun () =>
                memcpy_async pipe (s_from, from)
        consumer_phase pipe fun () =>
            memcpy_sync (l_from, s_from)
        future_i |> optionm.iter fun i =>
            producer_phase pipe fun () =>
                inl from, to = io |> apply i |> unzip
                memcpy_async pipe (s_from, from)
        pragma.unroll fun _ =>
            loop.linear from.dim fun j =>
                tensor_set j f(tensor_index j l_from) l_to
        // console.write_ln {i l_to tid = rangem.threads_in_grid().from}
        memcpy_sync (to, l_to)

inl main() =
    console.write_ln "Running test 2"
    inl in_ : tensor int float = cupy.ones (1 <<< 28)
    inl out : tensor int float = tensor_create in_.dim
    run fun () =>
        // loop.linear (1 <<< 8) fun (_ : int) =>
            map (fun x => 
                nanosleep 300 // Reduces memory occupancy from 92 to ...
                x * 10
                ) in_ out
    console.write_ln (view (const {from=0; nearTo=16}) out)