open corebase
open corecuda
open coreext
open refm
open rangem
open tensorm

// Maps all the elements of a tensor given the mapping function.
inl map forall dim a b. (f : a -> b) (from : tensor dim a) (to : tensor dim b) : () =
    open pipeline
    inl pipe = make_pipeline_thread()
    inl io = 
        zip from to
        |> factorize_sizeof_16
        |> factorize_fst threads_per_block()
        |> reorder (fun ((outer,tid),sizeof_4) => tid,outer,sizeof_4)
    assert (fst io.dim = threads_per_block()) "fst io.dim = threads_per_block()"
    inl io = io |> apply thread_index()
    inl s_from,_ = tensor_cuda.tensor_create_extern_shared 0 (threads_per_block(), snd io.dim)
    inl s_from = s_from |> apply thread_index()
    inl l_from, l_to = tensor_create (snd io.dim) |> unzip
    loop.projective blocks_in_grid(fst io.dim) fun i =>
        inl from, to = io |> apply i |> unzip
        producer_phase pipe fun () =>
            memcpy_async pipe (s_from, from)
        consumer_phase pipe fun () =>
            memcpy_sync (l_from, s_from)
            pragma.unroll fun _ =>
                loop.linear from.dim fun j =>
                    tensor_set j f(tensor_index j l_from) l_to
            memcpy_sync (to, l_to)

inl main() =
    console.write_ln "Running test 4. Non-interleaved asynchronous loads."
    inl in_ : tensor int float = cupy.ones _2_pow_28()
    inl out : tensor int float = tensor_create in_.dim
    run fun () =>
        map (fun x => 
            // nanosleep_128()
            x + 10
            ) in_ out
    console.write_ln (view (const {from=0; nearTo=16}) out)