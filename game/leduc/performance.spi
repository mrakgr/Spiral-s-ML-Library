// For debugging the performance of full.spi.

open game
open corebase
open corepython
open corecuda
open coreext

union player_type_play = Computer | Random | Human
union player_type_train = T_Computer | T_Random

union event =
    | StartTrainingVsSelf
    | StartTrainingVsRando

union ui_game_state = 
    | GameNotStarted
    | WaitingForActionFromPlayerId : table
    | GameOver : table

union ui_effects =
    | AddRewardsRando : plist (plist float)
    | AddRewardsSelf : plist (plist float)

type event_config =
    {
        // Loop sizes.
        frontend_divider : int
        outer : int
        inner : int
    }

inl event_config () : event_config =
    {
        outer = 1 <<< 2
        inner = 1 <<< 4
        frontend_divider = 1
    }

// Whether the state is public or private depends whether it is being sent over the wire.
type state_public =
    {
        ui_game_state : ui_game_state
        pl_type : sam.sa 2 player_type_play
        messages : messages
    }

type state_private =
    {
        game : option game_node
        deck : deck
    }

type state_neural =
    {
        model_data : ml.layers.model_data
    }

type state_game = 
    {
        private : state_private
        public : state_public
    }

type state =
    {
        game : state_game
        neural : state_neural
    }

inl init_game() =
    {
        public = {
            ui_game_state = GameNotStarted
            pl_type = arraym.fromList [Computer; Human]
            messages = sa_create
        }
        private = {
            deck = deckm.create()
            game = None
        }
    }

inl init() : state = 
    {
        game = init_game()
        neural = {
            model_data = model.game_graph() |> ml.cfr_models.init |> ml.cfr_models.to_model_data
        }
    }

type state_internal_play =
    {
        ui_game_state : refm.ref ui_game_state
        pl_type : refm.ref (sam.sa 2 player_type_play)
        game : refm.ref (option game_node)
        deck : refm.ref deck
        messages : refm.ref messages
        rng : refm.ref random.philox_state
    }

type state_internal_train =
    mut {
        pl_type : sam.sa 2 player_type_train
        deck : deck
        messages : messages
        rewards : sam.sa 2 ml.cfr.reward
        grid : cooperative_groups.grid_group
        rng : random.philox_state
    }

// Converts the given state to internal one.
// The refs will point to the original locations.
inl state_to_internal_train grid rng pl_type : state_internal_train =
    mut {
        pl_type = pl_type
        deck = deckm.create()
        messages = sa_create
        rewards = arraym.fromList [0;0]
        grid = refm.to_local grid
        rng = refm.to_local rng
    }

inl state_internal_train_reset (x : state_internal_train) : () =
    x.deck <- deckm.create()
    x.rewards <- arraym.fromList [0;0]
    sa_listm.unsafe_set_length x.messages 0

inl train_loop is_train (cfr_model : ml.cfr_models.cfr_game_model _ _) (state : state_internal_train) ~node : () = join
    inl set_rewards winner_id (chips_won : int) =
        inl chips_won = conv chips_won
        inl rewards = state.rewards
        set rewards winner_id chips_won
        set rewards (toggle winner_id) -chips_won
    inl push_message = sa_listm.push state.messages
    inl pop_deck () =
        open refm
        inl c,d = deckm.draw_card (refm.from_local state.rng) state.deck
        state.deck <- d
        c

    inl body node =
        open refm
        match node with
        | TerminalFold table =>
            inl msg = 
                inl chips_won = index table.pot table.player_turn
                set_rewards table.player_turn -chips_won
                Showdown{cards_shown=table.pl_card; chips_won winner_id=toggle table.player_turn}
            push_message msg
            None
        | TerminalCall table =>
            inl msg =
                inl chips_won, winner_id =
                    inl chips_won = index table.pot table.player_turn
                    match compare_hands table with
                    | Gt => chips_won, 0
                    | Eq => 0, -1
                    | Lt => chips_won, 1
                set_rewards (abs winner_id) chips_won
                Showdown{cards_shown=table.pl_card; chips_won winner_id}
            push_message msg
            None
        | Round table => 
            inl action = 
                match index state.pl_type table.player_turn with
                // | T_Computer =>
                //     inl data = table, refm.to_local state.messages
                //     if is_train then
                //         ml.cfr_models.trace_and_train state.rng cfr_model ({player_id=table.player_turn}, data) 
                //     else
                //         ml.cfr_models.trace_and_play state.rng cfr_model ({player_id=table.player_turn}, data) 
                //     |> model.action_conv table
                | T_Computer | T_Random =>
                    // Important: If the action is shared across the block in Computer it should also be done for the Random player.
                    random_action (refm.from_local state.rng) table 
                    |> transposing_loop.shuffle 0
            inl msg = PlayerAction(table.player_turn, action)
            push_message msg
            Some (game_round table action)
        | RoundWithAction(table,action) =>
            inl msg = PlayerAction(table.player_turn, action)
            push_message msg
            Some (game_round table action)
        | ChanceCommunityCard table =>
            inl card = pop_deck()
            inl msg = CommunityCardIs card
            push_message msg
            Some (game_chance_community_card table card)
        | ChanceInit () => 
            inl c0,c1 = pop_deck(), pop_deck()
            push_message PlayerGotCard(0, c0)
            push_message PlayerGotCard(1, c1)
            Some (game_chance_init (c0,c1))
    loop.while (function Some => true | None => false) (optionm.bind body) (Some node) |> ignore

open corecuda
open corepython
open coreext
open serializerm

// // Does a training run against the self.
// inl vs_self (neural : state_neural) =
//     inl sizes = model.leduc_sizes()
//     inl config = event_config()
//     inl trace_rewards = cupy.zeros (sizes.player_id, config.outer / config.frontend_divider)
//     inl frontend_rewards = tensorm.tensor_create trace_rewards.dim
//     run' fun () =>
//         inl model = ml.cfr_models.from_model_data model.game_graph() neural.model_data
//         inl grid = cooperative_groups.create_grid()
//         inl rng : _ random.philox_state = random.init {seed = random.clock64(); subsequence=conv rangem.threads_in_grid().from; offset=0}
//         loop.linear config.outer fun (outer : int) =>
//             loop.linear config.inner fun (inner : int) =>
//                 inl player_types = arraym.fromList [T_Computer; T_Computer]
//                 // Does a training run, modifying the trace state.
//                 train_loop true model (state_to_internal_train rng player_types) game_init() |> ml.cfr_models.calculate_updates model
//                 // Does a purely evalutation run, not modifying the trace state.
//                 inl rewards = train_loop false model (state_to_internal_train rng player_types) game_init()
//                 // Returns the integrate path probability.
//                 inl path_prob = ml.cfr_models.extract_integrated_path_prob model
//                 // Adds the rewards to the trace.
//                 loop.linear sizes.player_id fun player_id =>
//                     inl reward = index rewards player_id
//                     tensor_cuda.tensor_atomic_add (player_id,outer / config.frontend_divider)
//                         (reward * path_prob, path_prob)
//                         trace_rewards
//                 ml.cfr_models.reset_trace_state model
//             ml.cfr_models.apply_updates grid rng model
//         ml.primitives.grid_map grid (fun a,b => if b <> 0 then a / b else 0) trace_rewards frontend_rewards

//     frontend_rewards

// Does a training run against the random player.
inl vs_rando (neural : state_neural) =
    inl sizes = model.leduc_sizes()
    inl config = event_config()
    inl trace_rewards : _ _ (float * float) = cupy.zeros (sizes.ensemble_id, config.outer / config.frontend_divider)
    inl frontend_rewards : _ _ float = tensorm.tensor_create trace_rewards.dim
    run' fun () =>
        inl model = ml.cfr_models.from_model_data model.game_graph() neural.model_data
        inl grid = cooperative_groups.create_grid()
        inl rng : _ random.philox_state = random.init {seed = random.clock64(); subsequence=conv rangem.threads_in_grid().from; offset=0}
        loop.linear config.outer fun (outer : int) =>
            loop.linear config.inner fun (inner : int) =>
                // The noinline prefix will force the __noinline__ annotation in the generated code.
                inl noinline_train (player_id : int) =
                    inl player_types = arraym.fromList [T_Random;T_Random]
                    // set player_types player_id T_Computer
                    inl rewards = train_loop true model (state_to_internal_train rng player_types) game_init()
                    ()
                    // Adds the rewards to the trace.
                    // inl reward = index rewards player_id
                    // loop.linear sizes.ensemble_id fun ensemble_id =>
                    //     inl path_prob = ml.cfr_models.extract_ensemble_path_prob model {ensemble_id}
                    //     tensor_cuda.tensor_atomic_add (ensemble_id,outer / config.frontend_divider)
                    //         (reward * path_prob, path_prob)
                    //         trace_rewards
                    // ml.cfr_models.calculate_updates model rewards
                noinline_train 0
                // noinline_train 1
            // ml.cfr_models.apply_updates grid rng model
        // ml.primitives.grid_map grid (fun a,b => if b <> 0 then a / b else 0) trace_rewards frontend_rewards
        ()
    frontend_rewards

inl event_loop_gpu = fun (msg, state : event * state) =>
    open serializerm
    inl seri = {
        game_state = create_serializer
    }

    inl {game neural} = state
    serialize seri.game_state game

    console.write_ln "Going to run the Leduc full kernel."
    global "import time"
    inl p : f64 = $"time.perf_counter()"
    
    inl effects : plist ui_effects = plistm.create'
    match msg with
    | StartTrainingVsRando => vs_rando neural |> plistm.from_2d_tensor |> AddRewardsRando |> plistm.push effects
    // | StartTrainingVsSelf => vs_self neural |> plistm.from_2d_tensor |> AddRewardsSelf |> plistm.push effects

    device_sync()

    console.write "The time it took to run the kernel (in seconds) is: "
    inl p2 : f64 = $"time.perf_counter()"
    console.write_ln (p2 - p)

    // inl state = {
    //     game = deserialize seri.game_state
    //     neural
    //     }

    // jsonm.serialize (state, effects)

inl main() = event_loop_gpu(StartTrainingVsRando, init())