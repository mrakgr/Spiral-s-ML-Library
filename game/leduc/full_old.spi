open game
open corebase
open corepython
open corecuda
open coreext

union player_type_play = Computer | Random | Human
union player_type_train = T_Computer | T_Random

union event =
    | StartGame
    | PlayerChanged : sam.sa 2 player_type_play
    | ActionSelected : action

    | StartTrainingVsSelf
    | StartTrainingVsRando

union ui_game_state = 
    | GameNotStarted
    | WaitingForActionFromPlayerId : table
    | GameOver : table

union ui_effects =
    | AddRewardsRando : plist (plist float)
    | AddRewardsSelf : plist (plist float)

type event_config =
    {
        // Loop sizes.
        frontend_divider : int
        outer : int
        inner : int
    }

inl event_config () : event_config =
    {
        outer = 1 <<< 10
        inner = 1 <<< 4
        frontend_divider = 1 <<< 2
    }

// Whether the state is public or private depends whether it is being sent over the wire.
type state_public =
    {
        ui_game_state : ui_game_state
        pl_type : sam.sa 2 player_type_play
        messages : messages
    }

type state_private =
    {
        game : option game_node
        deck : deck
    }

type state_neural =
    {
        model_data : ml.layers.model_data
    }

type state_game = 
    {
        private : state_private
        public : state_public
    }

type state =
    {
        game : state_game
        neural : state_neural
    }

inl init_game() =
    {
        public = {
            ui_game_state = GameNotStarted
            pl_type = arraym.fromList [Computer; Human]
            messages = sa_create
        }
        private = {
            deck = deckm.create()
            game = None
        }
    }

inl init() : state = 
    {
        game = init_game()
        neural = {
            model_data = model.game_graph() |> ml.cfr_models.init |> ml.cfr_models.to_model_data
        }
    }

type state_internal_play =
    {
        ui_game_state : refm.ref ui_game_state
        pl_type : refm.ref (sam.sa 2 player_type_play)
        game : refm.ref (option game_node)
        deck : refm.ref deck
        messages : refm.ref messages
        rng : refm.ref random.philox_state
    }

type state_internal_train =
    {
        pl_type : refm.ref (sam.sa 2 player_type_train)
        deck : refm.ref deck
        messages : refm.ref messages
        rng : refm.ref random.philox_state
    }

// Converts the given state to internal one.
// The refs will point to the original locations.
inl state_to_internal_play rng (state : state_game) : state_internal_play =
    {
        ui_game_state = refm.from_local state.public.ui_game_state
        pl_type = refm.from_local state.public.pl_type
        game = refm.from_local state.private.game
        deck = refm.from_local state.private.deck
        messages = refm.from_local state.public.messages
        rng
    }

// Sets the internal state to the values held by the regular one.
inl internal_set_play (state_internal : state_internal_play) (state : state_game) =
    open refm
    state_internal.ui_game_state <-# state.public.ui_game_state
    state_internal.pl_type <-# state.public.pl_type
    state_internal.game <-# state.private.game
    state_internal.deck <-# state.private.deck
    state_internal.messages <-# state.public.messages

// Converts the given state to internal one.
// The refs will point to the original locations.
inl state_to_internal_train rng pl_type : state_internal_train =
    {
        pl_type = refm.from_local pl_type
        deck = refm.ref deckm.create()
        messages = refm.from_local sa_create
        rng
    }

type rewards = sam.sa 2 ml.cfr.reward

let play_loop (neural : state_neural) (state : state_internal_play) node =
    inl push_message = sa_listm.push (refm.to_local state.messages)
    inl pop_deck () = 
        open refm
        inl c,d = deckm.draw_card state.rng #state.deck
        state.deck <-# d
        c

    inl body node =
        open refm
        match node with
        | TerminalFold table =>
            inl msg = 
                inl chips_won = index table.pot table.player_turn
                Showdown{cards_shown=table.pl_card; chips_won winner_id=toggle table.player_turn}
            push_message msg
            state.ui_game_state <-# GameOver(table)
            state.game <-# None
            None
        | TerminalCall table =>
            inl msg =
                inl chips_won, winner_id =
                    inl chips_won = index table.pot table.player_turn
                    match compare_hands table with
                    | Gt => chips_won, 0
                    | Eq => 0, -1
                    | Lt => chips_won, 1
                Showdown{cards_shown=table.pl_card; chips_won winner_id}
            push_message msg
            state.ui_game_state <-# GameOver(table)
            state.game <-# None
            None
        | Round table => 
            match index #state.pl_type table.player_turn with
            | Computer =>
                open model
                inl model = ml.cfr_models.from_model_data model.game_graph() neural.model_data
                inl data = table, refm.to_local state.messages
                console.write_ln "Running the GPU model."
                inl action = ml.cfr_models.run state.rng model (Some data) |> action_conv table
                console.write "The action is: "
                console.write_ln action
                inl msg = PlayerAction(table.player_turn, action)
                push_message msg
                Some (game_round table action)
            | Human => 
                state.ui_game_state <-# WaitingForActionFromPlayerId(table)
                state.game <-# Some node
                None
            | Random =>
                inl action = random_action state.rng table
                inl msg = PlayerAction(table.player_turn, action)
                push_message msg
                Some (game_round table action)
        | RoundWithAction(table,action) => 
            inl msg = PlayerAction(table.player_turn, action)
            push_message msg
            Some (game_round table action)
        | ChanceCommunityCard table =>
            inl card = pop_deck()
            inl msg = CommunityCardIs card
            push_message msg
            Some (game_chance_community_card table card)
        | ChanceInit () => 
            inl c0,c1 = pop_deck(), pop_deck()
            push_message PlayerGotCard(0, c0)
            push_message PlayerGotCard(1, c1)
            Some (game_chance_init (c0,c1))
    
    loop.while (function Some => true | None => false) (optionm.bind body) (Some node) |> ignore

inl train_loop is_train (cfr_model : ml.cfr_models.cfr_game_model _ _) (state : state_internal_train) ~node : rewards = join
    inl rewards : rewards = sa_create
    inl set_rewards winner_id (chips_won : int) =
        inl chips_won = conv chips_won
        set rewards winner_id chips_won
        set rewards (toggle winner_id) -chips_won
    inl push_message = sa_listm.push (refm.to_local state.messages)
    inl pop_deck () =
        open refm
        inl c,d = deckm.draw_card state.rng #state.deck
        state.deck <-# d
        c

    inl body node =
        open refm
        match node with
        | TerminalFold table =>
            inl msg = 
                inl chips_won = index table.pot table.player_turn
                set_rewards table.player_turn -chips_won
                Showdown{cards_shown=table.pl_card; chips_won winner_id=toggle table.player_turn}
            push_message msg
            None
        | TerminalCall table =>
            inl msg =
                inl chips_won, winner_id =
                    inl chips_won = index table.pot table.player_turn
                    match compare_hands table with
                    | Gt => chips_won, 0
                    | Eq => 0, -1
                    | Lt => chips_won, 1
                set_rewards (abs winner_id) chips_won
                Showdown{cards_shown=table.pl_card; chips_won winner_id}
            push_message msg
            None
        | Round table => 
            inl action = 
                match index #state.pl_type table.player_turn with
                | T_Computer =>
                    inl data = table, refm.to_local state.messages
                    if is_train then
                        ml.cfr_models.trace_and_train state.rng cfr_model ({player_id=table.player_turn}, data) 
                    else
                        ml.cfr_models.trace_and_play state.rng cfr_model ({player_id=table.player_turn}, data) 
                    |> model.action_conv table
                | T_Random =>
                    // Important: If the action is shared across the block in Computer it should also be done for the Random player.
                    random_action state.rng table 
                    |> transposing_loop.shuffle 0
            inl msg = PlayerAction(table.player_turn, action)
            push_message msg
            Some (game_round table action)
        | RoundWithAction(table,action) =>
            inl msg = PlayerAction(table.player_turn, action)
            push_message msg
            Some (game_round table action)
        | ChanceCommunityCard table =>
            inl card = pop_deck()
            inl msg = CommunityCardIs card
            push_message msg
            Some (game_chance_community_card table card)
        | ChanceInit () => 
            inl c0,c1 = pop_deck(), pop_deck()
            push_message PlayerGotCard(0, c0)
            push_message PlayerGotCard(1, c1)
            Some (game_chance_init (c0,c1))
    loop.while (function Some => true | None => false) (optionm.bind body) (Some node) |> ignore
    rewards

inl event_loop_play state_neural (msg, state_internal : event * state_internal_play) : () =
    open refm
    match msg with
    | StartGame =>
        internal_set_play state_internal init_game()
        // TODO: Make the play loop noinline.
        play_loop state_neural state_internal game_init()
    | PlayerChanged pl_type => 
        state_internal.pl_type <-# pl_type
    | ActionSelected action =>
        match #state_internal.game with
        | Some game =>
            match game with
            | Round table => play_loop state_neural state_internal RoundWithAction(table, action)
            | _ => failwith "Unexpected game node in ActionSelected."
        | None => failwith "The game hasn't been started in ActionSelected."
    | StartTrainingVsRando | StartTrainingVsSelf => failwith "Training is not supported in the `event_loop_play` function."

open corecuda
open corepython
open coreext
open serializerm

inl vs_human (seri : {game_state : serializer state_game; msg : serializer event}) (neural : state_neural) =
    run' fun () =>
        inl rng = random.init {seed = random.clock64(); subsequence=conv rangem.threads_in_grid().from; offset=0}
        inl c : _ int = refm.create_shared_var (const threads_per_block())
        inl from = rangem.threads_in_grid().from
        if from = 0 then
            inl msg, game_state = deserialize seri.msg, deserialize seri.game_state
            // When converting the state to internal form, the refs will point to the original.
            event_loop_play neural (msg, state_to_internal_play rng game_state)
            serialize seri.game_state game_state

        // nn model loop
        intrinsic.atomic_add_ref c -1 |> ignore
        loop.while' (const true) fun () =>
            inl model = ml.cfr_models.from_model_data model.game_graph() neural.model_data

            ml.cfr_models.run rng model None |> ignore
            if refm.deref c = 0 then loop.break()

// Does a training run against the random player.
inl vs_rando (neural : state_neural) =
    inl sizes = model.leduc_sizes()
    inl config = event_config()
    inl trace_rewards = cupy.zeros (sizes.ensemble_id, config.outer / config.frontend_divider)
    inl frontend_rewards = tensorm.tensor_create trace_rewards.dim
    run' fun () =>
        inl model = ml.cfr_models.from_model_data model.game_graph() neural.model_data
        inl grid = cooperative_groups.create_grid()
        inl rng : _ random.philox_state = random.init {seed = random.clock64(); subsequence=conv rangem.threads_in_grid().from; offset=0}
        loop.linear config.outer fun (outer : int) =>
            loop.linear config.inner fun (inner : int) =>
                // The noinline prefix will force the __noinline__ annotation in the generated code.
                let noinline_train player_id =
                    inl player_types = arraym.fromList [T_Random;T_Random]
                    set player_types player_id T_Computer
                    inl rewards = train_loop true model (state_to_internal_train rng player_types) game_init()
                    // Adds the rewards to the trace.
                    inl reward = index rewards player_id
                    loop.linear sizes.ensemble_id fun ensemble_id =>
                        inl path_prob = ml.cfr_models.extract_ensemble_path_prob model {ensemble_id}
                        tensor_cuda.tensor_atomic_add (ensemble_id,outer / config.frontend_divider)
                            (reward * path_prob, path_prob)
                            trace_rewards
                    ml.cfr_models.calculate_updates model rewards
                noinline_train 0 . noinline_train 1
            ml.cfr_models.apply_updates grid rng model
        ml.primitives.grid_map grid (fun a,b => if b <> 0 then a / b else 0) trace_rewards frontend_rewards
    frontend_rewards

// Does a training run against the self.
inl vs_self (neural : state_neural) =
    inl sizes = model.leduc_sizes()
    inl config = event_config()
    inl trace_rewards = cupy.zeros (sizes.player_id, config.outer / config.frontend_divider)
    inl frontend_rewards = tensorm.tensor_create trace_rewards.dim
    run' fun () =>
        inl model = ml.cfr_models.from_model_data model.game_graph() neural.model_data
        inl grid = cooperative_groups.create_grid()
        inl rng : _ random.philox_state = random.init {seed = random.clock64(); subsequence=conv rangem.threads_in_grid().from; offset=0}
        loop.linear config.outer fun (outer : int) =>
            loop.linear config.inner fun (inner : int) =>
                inl player_types = arraym.fromList [T_Computer; T_Computer]
                // Does a training run, modifying the trace state.
                train_loop true model (state_to_internal_train rng player_types) game_init() |> ml.cfr_models.calculate_updates model
                // Does a purely evalutation run, not modifying the trace state.
                inl rewards = train_loop false model (state_to_internal_train rng player_types) game_init()
                // Returns the integrate path probability.
                inl path_prob = ml.cfr_models.extract_integrated_path_prob model
                // Adds the rewards to the trace.
                loop.linear sizes.player_id fun player_id =>
                    inl reward = index rewards player_id
                    tensor_cuda.tensor_atomic_add (player_id,outer / config.frontend_divider)
                        (reward * path_prob, path_prob)
                        trace_rewards
                ml.cfr_models.reset_trace_state model
            ml.cfr_models.apply_updates grid rng model
        ml.primitives.grid_map grid (fun a,b => if b <> 0 then a / b else 0) trace_rewards frontend_rewards

    frontend_rewards

inl event_loop_gpu = fun (msg, state : _ event * _ state) =>
    open serializerm
    inl seri = {
        msg = create_serializer
        game_state = create_serializer
    }
    inl msg = jsonm.deserialize msg
    
    inl {game neural} = jsonm.deserialize state
    serialize seri.game_state game

    console.write_ln "Going to run the Leduc full kernel."
    global "import time"
    inl p : f64 = $"time.perf_counter()"
    
    inl effects : plist ui_effects = plistm.create'
    match msg with
    | StartTrainingVsRando => vs_rando neural |> plistm.from_2d_tensor |> AddRewardsRando |> plistm.push effects
    | StartTrainingVsSelf => vs_self neural |> plistm.from_2d_tensor |> AddRewardsSelf |> plistm.push effects
    | StartGame | PlayerChanged | ActionSelected => serialize seri.msg msg . vs_human seri neural

    device_sync()

    console.write "The time it took to run the kernel (in seconds) is: "
    inl p2 : f64 = $"time.perf_counter()"
    console.write_ln (p2 - p)

    inl state = {
        game = deserialize seri.game_state
        neural
        }

    jsonm.serialize (state, effects)

inl main() =
    named_tuple "Leduc_Full" {
        init = fun () => jsonm.serialize init()
        event_loop_gpu
    }