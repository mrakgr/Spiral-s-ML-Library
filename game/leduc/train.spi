open game
open corebase
open corepython
open corecuda
open coreext

type event_config =
    {
        // Loop sizes.
        frontend_divider : int
        outer : int
        inner : int
    }

inl event_config () : event_config =
    {
        outer = 1 <<< 2
        inner = 1 <<< 3
        frontend_divider = 1
    }

type state_internal_train =
    stack_mut {
        pl_type : sa 2 player_type
        deck : deck
        messages : messages
        rewards : sa 2 ml.cfr.reward
        grid : cooperative_groups.grid_group
        rng : random.philox_state
    }

// Converts the given state to internal one.
// The refs will point to the original locations.
inl state_to_internal_train grid rng : state_internal_train =
    stack_mut {
        pl_type = arraym.fromList [Random;Random]
        deck = deckm.create()
        messages = sa_create
        rewards = arraym.fromList [0;0]
        grid = refm.to_local grid
        rng = refm.to_local rng
    }

inl state_internal_train_reset (x : state_internal_train) (player_id : option ml.cfr.player_id) : () =
    x.deck <- deckm.create()
    x.rewards <- arraym.fromList [0;0]
    sa_listm.unsafe_set_length x.messages 0
    inl player_types = arraym.fromList [Computer;Computer]
    player_id |> optionm.iter fun player_id =>
        set player_types (player_id ^^^ 1) Random
    x.pl_type <- player_types

// (is_train : bool) controls whether the run updates the trace and trace state, and whether the average or the current strategy is used for evaluation.
// (player_id : option int) controls which player is the Computer, and the other is Random. If None then both are Computer players.
inl train_loop is_train (cfr_model : ml.cfr_models.cfr_game_model _ _) (state : state_internal_train) player_id ~node : () = join
    state_internal_train_reset state player_id
    inl set_rewards winner_id (chips_won : int) =
        inl chips_won = conv chips_won
        inl rewards = state.rewards
        set rewards winner_id chips_won
        set rewards (toggle winner_id) -chips_won
    inl push_message = sa_listm.push state.messages
    inl pop_deck () =
        open refm
        inl c,d = deckm.draw_card (refm.from_local state.rng) state.deck
        state.deck <- d
        c

    inl body node =
        open refm
        match node with
        | TerminalFold table =>
            inl msg = 
                inl chips_won = index table.pot table.player_turn
                set_rewards table.player_turn -chips_won
                Showdown{cards_shown=table.pl_card; chips_won winner_id=toggle table.player_turn}
            push_message msg
            None
        | TerminalCall table =>
            inl msg =
                inl chips_won, winner_id =
                    inl chips_won = index table.pot table.player_turn
                    match compare_hands table with
                    | Gt => chips_won, 0
                    | Eq => 0, -1
                    | Lt => chips_won, 1
                set_rewards (abs winner_id) chips_won
                Showdown{cards_shown=table.pl_card; chips_won winner_id}
            push_message msg
            None
        | Round table => 
            inl action =
                match index state.pl_type table.player_turn with
                | Human => failwith "Humans not allowed during training."
                | Computer =>
                    inl data = table, state.messages
                    if is_train then
                        ml.cfr_models.trace_and_train (refm.from_local state.rng) cfr_model ({player_id=table.player_turn}, data) 
                    else
                        ml.cfr_models.trace_and_play (refm.from_local state.rng) cfr_model ({player_id=table.player_turn}, data) 
                    |> model.action_conv table
                | Random =>
                    // Important: If the action is shared across the block in Computer it should also be done for the Random player.
                    random_action (refm.from_local state.rng) table 
                    |> transposing_loop.shuffle 0
            inl msg = PlayerAction(table.player_turn, action)
            push_message msg
            Some (game_round table action)
        | RoundWithAction(table,action) =>
            inl msg = PlayerAction(table.player_turn, action)
            push_message msg
            Some (game_round table action)
        | ChanceCommunityCard table =>
            inl card = pop_deck()
            inl msg = CommunityCardIs card
            push_message msg
            Some (game_chance_community_card table card)
        | ChanceInit () => 
            inl c0,c1 = pop_deck(), pop_deck()
            push_message PlayerGotCard(0, c0)
            push_message PlayerGotCard(1, c1)
            Some (game_chance_init (c0,c1))
    loop.while (function Some => true | None => false) (optionm.bind body) (Some node) |> ignore

open corecuda
open corepython
open coreext
open serializerm

// Does a training run against the self.
inl vs_self (neural : state_neural) =
    inl sizes = model.leduc_sizes()
    inl config = event_config()
    inl trace_rewards = cupy.zeros (sizes.player_id, config.outer / config.frontend_divider)
    inl frontend_rewards = tensorm.tensor_create trace_rewards.dim
    run' fun () =>
        inl state =
            inl grid = cooperative_groups.create_grid()
            random.init {seed = random.clock64(); subsequence=conv rangem.threads_in_grid().from; offset=0}
            |> state_to_internal_train grid
        inl model = ml.cfr_models.from_model_data model.game_graph() neural.model_data
        loop.linear config.outer fun (outer : int) =>
            loop.linear config.inner fun (inner : int) =>
                // Does a training run, modifying the trace state.
                train_loop true model state None game_init() 
                ml.cfr_models.calculate_updates model state.rewards
                // Does a purely evalutation run, not modifying the trace state.
                inl rewards = train_loop false model state None game_init()
                // Returns the integrate path probability.
                inl path_prob = ml.cfr_models.extract_integrated_path_prob model
                // Adds the rewards to the trace.
                loop.linear sizes.player_id fun player_id =>
                    inl reward = index state.rewards player_id
                    tensor_cuda.tensor_atomic_add (player_id,outer / config.frontend_divider)
                        (reward * path_prob, path_prob)
                        trace_rewards
                ml.cfr_models.reset_trace_state model
            ml.cfr_models.apply_updates (refm.from_local state.grid) (refm.from_local state.rng) model
        ml.primitives.grid_map (refm.from_local state.grid) (fun a,b => if b <> 0 then a / b else 0) trace_rewards frontend_rewards

    frontend_rewards

// Does a training run against the random player.
inl vs_rando (neural : state_neural) =
    inl sizes = model.leduc_sizes()
    inl config = event_config()
    inl trace_rewards : _ _ (float * float) = cupy.zeros (sizes.ensemble_id, config.outer / config.frontend_divider)
    inl frontend_rewards : _ _ float = tensorm.tensor_create trace_rewards.dim
    run' fun () =>
        inl state =
            inl grid = cooperative_groups.create_grid()
            random.init {seed = random.clock64(); subsequence=conv rangem.threads_in_grid().from; offset=0}
            |> state_to_internal_train grid
        inl model = ml.cfr_models.from_model_data model.game_graph() neural.model_data
        loop.linear config.outer fun (outer : int) =>
            loop.linear config.inner fun (inner : int) =>
                // The loop uses less registers than a noinline function.
                // Regular functions it always inlines causing the number of instructions in the full program to double.
                loop.linear 2 fun (player_id : int) =>
                    // Does a training run, modifying the trace state.
                    train_loop true model state (Some player_id) game_init()
                    // Adds the rewards to the trace.
                    inl reward = index state.rewards player_id
                    loop.linear sizes.ensemble_id fun ensemble_id =>
                        inl path_prob = ml.cfr_models.extract_ensemble_path_prob model {ensemble_id} // TODO: Upgrade the extract_ensemble_path_prob function.
                        tensor_cuda.tensor_atomic_add (ensemble_id,outer / config.frontend_divider)
                            (reward * path_prob, path_prob)
                            trace_rewards
                    ml.cfr_models.calculate_updates model state.rewards
            ml.cfr_models.apply_updates (refm.from_local state.grid) (refm.from_local state.rng) model
        ml.primitives.grid_map (refm.from_local state.grid) (fun a,b => if b <> 0 then a / b else 0) trace_rewards frontend_rewards
    frontend_rewards