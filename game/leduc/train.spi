open game
open corebase
open corepython
open corecuda
open coreext

inl print0 msg =
    if rangem.threads_in_grid().from = 0 then
        console.write_ln msg
    __syncwarp()


union player_type = Computer | Random
type player_types = sam.sa 2 player_type

union ui_effects =
    | AddRewards : plist (plist float)

type event_config =
    {
        player_types : player_types
        // Loop sizes.
        outer : int
        inner : int
    }

inl event_config() : event_config =
    {
        player_types = arraym.fromList [Computer; Random]
        outer = 1 <<< 1
        inner = 1 <<< 1
    }

union event =
    | StartTraining

type state_neural =
    {
        model_data : ml.layers.model_data
    }

type state =
    {
        game : { private : (); public : () }
        neural : state_neural
    }

inl init() : state =
    {
        game = { private=(); public=() }
        neural = {
            model_data = model.game_graph() |> ml.cfr_models.init |> ml.cfr_models.to_model_data
        }
    }

type state_internal =
    {
        pl_type : refm.ref player_types
        deck : refm.ref deck
        messages : refm.ref messages
        rng : refm.ref random.philox_state
    }

// Converts the given state to internal one.
// The refs will point to the original locations.
inl state_to_internal rng pl_type : state_internal =
    {
        pl_type = refm.from_local pl_type
        deck = refm.ref deckm.create()
        messages = refm.from_local sa_create
        rng
    }

type rewards = sam.sa 2 ml.cfr.reward

inl play_loop (cfr_model : ml.cfr_models.cfr_game_model _ _) (state : state_internal) ~node : rewards = join
    inl rewards : rewards = sa_create
    inl set_rewards winner_id (chips_won : int) =
        inl chips_won = conv chips_won
        set rewards winner_id chips_won
        set rewards (toggle winner_id) -chips_won
    inl push_message = sa_listm.push (refm.to_local state.messages)
    inl pop_deck () = 
        open refm
        inl c,d = deckm.draw_card state.rng #state.deck
        state.deck <-# d
        c

    inl body node =
        open refm
        match node with
        | TerminalFold table =>
            inl msg = 
                inl chips_won = index table.pot table.player_turn
                set_rewards table.player_turn -chips_won
                Showdown{cards_shown=table.pl_card; chips_won winner_id=toggle table.player_turn}
            push_message msg
            None
        | TerminalCall table =>
            inl msg =
                inl chips_won, winner_id =
                    inl chips_won = index table.pot table.player_turn
                    match compare_hands table with
                    | Gt => chips_won, 0
                    | Eq => 0, -1
                    | Lt => chips_won, 1
                set_rewards (abs winner_id) chips_won
                Showdown{cards_shown=table.pl_card; chips_won winner_id}
            push_message msg
            None
        | Round table => 
            inl action = 
                match index #state.pl_type table.player_turn with
                | Computer =>
                    inl data = table, refm.to_local state.messages
                    ml.cfr_models.trace_and_play state.rng cfr_model ({player_id=table.player_turn}, data) |> model.action_conv table
                | Random =>
                    // Important: If the action is shared across the block in Computer it should also be done for the Random player.
                    random_action state.rng table |> transposing_loop.shuffle 0
            inl msg = PlayerAction(table.player_turn, action)
            push_message msg
            Some (game_round table action)
        | RoundWithAction(table,action) =>
            inl msg = PlayerAction(table.player_turn, action)
            push_message msg
            Some (game_round table action)
        | ChanceCommunityCard table =>
            inl card = pop_deck()
            inl msg = CommunityCardIs card
            push_message msg
            Some (game_chance_community_card table card)
        | ChanceInit () => 
            inl c0,c1 = pop_deck(), pop_deck()
            push_message PlayerGotCard(0, c0)
            push_message PlayerGotCard(1, c1)
            Some (game_chance_init (c0,c1))
    
    loop.while (function Some => true | None => false) (optionm.bind body) (Some node) |> ignore

    rewards

open corecuda
open corepython
open coreext
inl main() =
    named_tuple "Leduc_Game" {
        init = fun () => jsonm.serialize init()
        event_loop_gpu = fun (msg, state : _ event * _ state) =>
            open serializer
            inl seri = {
                msg = create_serializer
            }
            serialize seri.msg (jsonm.deserialize msg)
            inl {neural} = jsonm.deserialize state

            console.write_ln "Going to run the Leduc training kernel."
            global "import time"
            inl p : f64 = $"time.perf_counter()"

            inl sizes = model.leduc_sizes()
            inl trace_rewards = 
                inl config = event_config()
                cupy.zeros (config.outer, config.inner, sizes.ensemble_id)

            inl frontend_rewards = tensorm.tensor_create trace_rewards.dim

            run' {shared_mem=model.game_graph() |> ml.cfr_models.smem_used} fun () =>
                inl model = ml.cfr_models.from_model_data model.game_graph() neural.model_data
                inl grid = cooperative_groups.create_grid()
                inl rng : _ random.philox_state = random.init {seed = random.clock64(); subsequence=conv rangem.threads_in_grid().from; offset=0}
                match deserialize seri.msg with
                | StartTraining =>
                    inl config = event_config()
                    loop.linear config.outer fun (outer : int) =>
                        loop.linear config.inner fun (inner : int) =>
                            inl state_internal = state_to_internal rng config.player_types
                            inl rewards = play_loop model state_internal game_init()
                            // console.write_ln {rewards tid = thread_index()}
                            inl () = // Adds the rewards to the trace.
                                inl player_id = 0
                                inl reward = index rewards player_id
                                loop.linear sizes.ensemble_id fun ensemble_id =>
                                    inl path_prob = ml.cfr_models.extract_path_prob model {ensemble_id player_id}
                                    // console.write_ln {reward path_prob ensemble_id tid = thread_index()}
                                    tensor_cuda.tensor_atomic_add (outer,inner,ensemble_id)
                                        (reward * path_prob, path_prob)
                                        trace_rewards
                            ml.cfr_models.calculate_updates model rewards
                        ml.cfr_models.apply_updates grid rng model
                    ml.primitives.grid_map grid (fun a,b => if b <> 0 then a / b else 0) trace_rewards frontend_rewards

            inl p2 : f64 = $"time.perf_counter()"
            device_sync()
            console.write "The time it took to run the kernel (in seconds) is: "
            console.write_ln (p2 - p)

            inl a = { neural game = { private=(); public=() } }
            inl b : plist ui_effects =
                frontend_rewards
                |> tensorm.reshape (fun a,b,c => a*b,c)
                |> plistm.from_2d_tensor
                |> AddRewards
                |> listm.singleton
                |> arraym.fromList

            jsonm.serialize(a, b)
    }