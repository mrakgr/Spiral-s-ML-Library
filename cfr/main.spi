open corebase
open corecuda
open coreext
open tensorm

type world_id = int
type action_id = int
type seq_id = int
type thread_id = int
type prob = float
type reward = float
type count = float

type trace_elem =
    {
        world_id : world_id
        action_id : action_id
        action_sampling_prob : prob
    }
type trace = 
    {
        actions : tensor (thread_id * seq_id) trace_elem
        update : {
            value : tensor (thread_id * seq_id) (reward * count)
            policy : tensor (thread_id * seq_id * action_id) prob
        }
    }

type model =
    tensor (world_id * action_id) {
        policy : {
            average : prob
            current : prob
            update : prob
        }
        value : (reward * count)
    }

inl test() =
    inl size = {
        world_id = 1 <<< 12
        action_id = 1 <<< 2
        seq_id = 1 <<< 4
        thread_id = threads_per_block()
    }
    inl trace : trace = {
        actions = tensor_create (size.thread_id, size.seq_id)
        update = {
            policy = tensor_create (size.thread_id, size.seq_id, size.action_id)
            value = tensor_create (size.thread_id, size.seq_id)
        }
    }
    inl model : model = tensor_create (size.world_id, size.action_id)
    run fun () =>
        open random
        inl rng : _ philox_state = init {seed = clock64(); subsequence=conv thread_index(); offset=0}

        inl mask config x i j = j < 3

        inl get_action (world_id : world_id) : action_id * prob =
            inl current_policy = 
                model 
                |> apply world_id 
                |> rezip (fun x => x.policy.current)
            open ml.primitives
            row_gather_reduce (fun config x i j_tns => 
                inl probs, action_id = local_masked_regret_matching_and_discrete_sampling mask rng config x i j_tns
                inl prob = 
                    local_reduce config (0,limit.max) (fun a b =>
                        if snd a = action_id then a
                        elif snd b = action_id then b
                        else a
                        ) (zip probs j_tns)
                    |> fun prob,i => assert (i <> limit.max) "Expected a valid action id in get_action." . prob
                action_id, prob
                ) current_policy

        // Calculates the policy and the value array updates.
        inl calculate_updates (seq_id : seq_id) (model : model) (trace : trace) (reward : reward) =
            loop.forDown {nearFrom=seq_id; to=0} (fun seq_id (reward : reward) =>
                inl {action_id action_sampling_prob world_id} = tensor_index (thread_index(), seq_id) trace.actions
                inl policy_current_and_value =
                    model
                    |> apply world_id
                    |> rezip (fun x => x.policy.current, x.value)
                inl update_policy =
                    trace.update.policy
                    |> apply thread_index()
                    |> apply seq_id
                inl () =
                    trace.update.value
                    |> tensor_set (thread_index(), seq_id) (reward,1)
                open ml.primitives
                row_gather_map_reduce (fun config x i j_tns =>
                    inl policy_current, reward_and_count = unzip x
                    inl values = local_map (fun reward, count => reward / count) reward_and_count
                    inl action_probs = local_masked_regret_matching mask config policy_current i j_tns
                    inl adjusted_values =
                        local_map (fun value, j => 
                            (if action_id = j then (reward - value) / action_sampling_prob else 0) + value
                            ) (zip values j_tns)
                    inl expected_value =
                        local_map (fun action_prob, value => action_prob * value) (zip action_probs adjusted_values)
                        |> local_sum config
                    inl update_policy =
                        // TODO: Don't forget about the path probabilities!
                        local_map (fun value => value - expected_value) adjusted_values
                    update_policy, expected_value
                    ) policy_current_and_value update_policy
                ) reward
            |> (ignore : reward -> ())

        // Applies the policy and the value array updates.
        inl apply_updates (seq_id : seq_id) (model : model) (trace : trace) () =
            loop.forDown' {nearFrom=seq_id; to=0} (fun seq_id =>
                inl {action_id action_sampling_prob world_id} = tensor_index (thread_index(), seq_id) trace.actions
                inl value_update =
                    trace.update.value
                    |> tensor_index (thread_index(), seq_id)
                inl policy_update,value =
                    model
                    |> apply world_id
                    |> rezip (fun x => x.policy.update, x.value)
                    |> unzip
                inl update_policy =
                    trace.update.policy
                    |> apply thread_index()
                    |> apply seq_id
                
                open tensor_cuda
                open ml.primitives
                // Updates the value arrays.
                tensor_atomic_add action_id value_update value
                // Updates the current policy.
                row_gather_void (fun config x i j_tns =>
                    local_iter (fun x, action_id =>
                        tensor_atomic_add action_id x policy_update
                        ) (zip x j_tns)
                    ) update_policy
                // TODO: Updates the average policy.
                // TODO: Maybe decay the value arrays.
                )
        
        open refm
        inl seq_id = ref (0 : int)
        let push' (world_id : world_id) (action_id, action_sampling_prob : action_id * prob) =
            inl i = #seq_id
            seq_id <-# i+1
            tensor_set (thread_index(), i) {world_id action_id action_sampling_prob} trace.actions
        inl push (world_id : world_id) = push' world_id (get_action world_id)
        push 235
        push 212
        push 790
        push 343
        push 457
        push 3447

        inl reward = -13
        calculate_updates #seq_id model trace reward
