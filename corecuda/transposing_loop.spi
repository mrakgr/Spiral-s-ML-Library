// The transposing loop module.

open corebase
open tensorm
open tensor_cuda
open rangem

// Replaces the free variables in the first argument with those in the second one.
// Both of them have to have the same number of free variables and the pairs of them have to have equal type.
inl free_vars_replace forall a b. (a : a) (b : b) : a = !!!!FreeVarsReplace(a,b)

type config passthrough dim =
    {
        passthrough : passthrough
        dim_inner : dim
        // dim_outer : int // See the comment below for why this has been removed.
    }

// Note(8/30/2024):
// After some thought, I've concluded that it'd be a fool's errand trying to program with partial
// blocks. It's a temptation to get you to squeeze a bit extra performance, but it'd destroy the
// the architecture of the program and make reasoning about it 100 times harder.
// 
// The type system isn't capable of capturing thread divergence in Cuda, and more importantly,
// GPUs have no preemption mechanism to make use of dropped off threads. So once a block's threads diverge
// it's computational capability gets cut accordingly. Block syncing would stop working, and we'd
// get all kinds of undefined behavior.
// 
// When doing RL, it'd be better to use action sharing, or suitable algorithms and games, to make sure that
// all the threads in a block are always flowing in lockstep.


// Uses a tensor in dynamic shared memory to transpose all the free variables in shared memory before running the function.
// The variables passed through `passthrough` won't be transposed and will be passed into the kernel directly.
// Uses a projective loop under the hood.
// The return value from the given function should be the same for every thread in the output dimension.
// If not, the results are undefined.
// 
// Even though the loop itself is blockwise in design, the thread id it passes into the lambda body is the thread id in the grid instead of the block.
// It does block sync at the end. Any grid sync potentially needed would need to be done manually.
inl projective forall shared dim t. ({passthrough dim_inner} : config shared dim) (f : {extern_offset : partitionm.size; passthrough : shared} -> int -> dim -> t) : t =
    inl dim_outer = threads_per_block()
    inl body forall el. (fv : el) : t = 
        inl shared_io, extern_offset = tensor_create_extern_shared 0 dim_outer
        inl shared_in, shared_out : tensor int el * tensor int t = unzip shared_io
        inl tid = thread_index()
        tensor_set tid fv shared_in
        barrier_cta_sync 0
        loop.projective threads_in_block(dim_outer, dim_inner) fun thread_id, dim =>
            tensor_set thread_id
                (free_vars_replace f (tensor_index thread_id shared_in) {extern_offset passthrough} (block_index() * threads_per_block() + thread_id) dim)
                shared_out
        barrier_cta_sync 0
        inl x = tensor_index tid shared_out
        barrier_cta_sync 0
        x
    real
        open real_core
        inl fv = free_vars f
        body `(`fv) fv

// Uses a tensor in dynamic shared memory to transpose all the free variables in shared memory before running the function.
// The variables passed through `passthrough` won't be transposed and will be passed into the kernel directly.
// Uses a (restrictive) linear loop under the hood.
// The inner dimension must be divisible by the number of threads per block.
// The return value from the given function should be the same for every thread in the output dimension.
// If not, the results are undefined.
// 
// Even though the loop itself is blockwise in design, the thread id it passes into the lambda body is the thread id in the grid instead of the block.
// It does block sync at the end. Any grid sync potentially needed would need to be done manually.
inl linear forall shared dim t. ({passthrough dim_inner} : config shared dim) (f : {extern_offset : partitionm.size; passthrough : shared} -> int -> dim -> t) : t =
    inl dim_outer = threads_per_block()
    inl body forall el. (fv : el) : t = 
        inl shared_io, extern_offset = tensor_create_extern_shared 0 dim_outer
        inl shared_in, shared_out : tensor int el * tensor int t = unzip shared_io
        inl tid = thread_index()
        tensor_set tid fv shared_in
        barrier_cta_sync 0

        inl (dim_block,dim_inner),(dim_block',dim_inner') = loop.rigid_split threads_per_block() (dim_outer, dim_inner)
        assert (loop.prod dim_inner' = 1) "The threads per block need to be divisible by the inner dimension."
        inl index_block,index_inner = loop.proj (dim_block,dim_inner) tid
        inl shared_in = reshape const(dim_block',dim_block) shared_in |> reorder (fun dim_block',dim_block => dim_block,dim_block') |> apply index_block
        loop.linear dim_block' fun index_block' =>
            // Calculate the local thread id.
            inl thread_id =
                inl dim = dim_block, dim_block'
                inl index = index_block, index_block'
                loop.rigid_merge dim index
            tensor_set thread_id
                (free_vars_replace f (tensor_index index_block' shared_in) {extern_offset passthrough} (block_index() * threads_per_block() + thread_id) index_inner)
                shared_out
        barrier_cta_sync 0
        inl x = tensor_index tid shared_out
        barrier_cta_sync 0
        x
    real
        open real_core
        inl fv = free_vars f
        body `(`fv) fv

// Broadcasts a value to all the threads in the block from a given one.
// Uses dynamic shared memory, and so shouldn't be composed with functions that use it.
inl shuffle forall t. (i : int) (v : t) : t =
    open tensor_cuda
    assert (lit_is i) "The index should be known at compile time."
    inl q,_ = tensor_create_extern_shared 0 1
    if thread_index() = i then tensor_set i v q
    barrier_cta_sync 0
    inl x = tensor_index 0 q
    barrier_cta_sync 0
    x

// Backs up the second argument in static global memory before running the closure, and returning the result of that as well as the backed up value.
inl backup forall a b. (eval : () -> a) (env : b) : a * b =
    inl {from by} = rangem.threads_in_grid()
    inl ar : array b = $"static `b \v[!by]"
    set ar from env
    eval(), index ar rangem.threads_in_grid().from
