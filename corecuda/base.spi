open corebase

// Controls the asserts. Easier to change this than to comment out the option directly.
inl debug_mode() = false

type warp_size = 32
inl threads_per_warp() : int = real real_core.type_lit_to_lit `warp_size
inl sms_per_gpu() : int = 24 // Marko Grdinic: My RTX 4060 has 24 SMs. A RTX 4090 for example would have 128. This should be changed to reflect your configuration.

inl threads_per_block() : int = 
    // 32 // Debug
    // 512 // Release
    256 // Release

inl blocks_per_grid() : int = 
    // 1 // Debug
    sms_per_gpu() // Release

// Returns the dynamic shared memory that should be used in bytes.
inl dynamic_shared_memory_used() : int = 
    // On a RTX 4060, 99kb is the limit, but we'll take only 80kb in case static shared memory needs to be used.
    80 * (1 <<< 10)

// Synchronizes the device on the host. Should be used on the Python side.
inl device_sync() : () = $"cp.cuda.get_current_stream().synchronize()"

union compiler =
    | NVCC
    | NVRTC

// Executes the lambda on the GPU device.
inl run' f =
    inl blocks_per_grid, threads_per_block = blocks_per_grid(), threads_per_block()
    // Global statements only get executed once.
    global "options = []"
    if debug_mode() = false then
        global "options.append('--define-macro=NDEBUG')" // turns off the macros
    global "options.append('--dopt=on')" // turns on the device optimizations
    global "options.append('--diag-suppress=550,20012,68,39,177')" // suppresses some warnings
    global "options.append('--restrict')" // assumes all the pointers are restricted
    
    match threads_per_block with
    | 1024 => global "options.append('--maxrregcount=64')"
    | 512 => global "options.append('--maxrregcount=128')"
    | 256 => global "options.append('--maxrregcount=256')" // The manual states the max is 255 instead of 256
    | _ => ()

    assert ($"cp.cuda.Device().attributes['MultiProcessorCount']" = sms_per_gpu()) "The number of SMs per GPU at runtime must much that what is declared atop of corecuda.base. Make sure to use the correct constant so it can be propagated at compile time."

    inl compiler = NVCC
    match compiler with
    | NVRTC => global "raw_module = cp.RawModule(code=kernel, backend='nvrtc', enable_cooperative_groups=True, options=tuple(options))"
    | NVCC => 
        global "options.append('--std=c++20')" // We'll need this one for Cutlass later.
        global "options.append('-D__CUDA_NO_HALF_CONVERSIONS__')" // To avoid `instance of overloaded function "__half::__half" matches the specified type` error with NVCC
        global "raw_module = cp.RawModule(code=kernel, backend='nvcc', enable_cooperative_groups=True, options=tuple(options))"
    inl kernel_i, vars = join_backend Cuda
        match compiler with
        | NVRTC => ()
        | NVCC =>
            global "#include <new>"
            global "#include <assert.h>"
            global "#include <stdio.h>"
        f () : ()
    inl entry = $'raw_module.get_function(f"entry{!kernel_i}")' : $"cp.RawKernel"
    inl shared_mem = dynamic_shared_memory_used()
    $'!entry.max_dynamic_shared_size_bytes = !shared_mem '
    real
        match vars with
        | _,_ => $'!entry((!blocks_per_grid,),(!threads_per_block,),!vars,shared_mem=!shared_mem)' : ()
        | () => $'!entry((!blocks_per_grid,),(!threads_per_block,),(),shared_mem=!shared_mem)' : ()
        | _ => $'!entry((!blocks_per_grid,),(!threads_per_block,),(!vars,),shared_mem=!shared_mem)' : ()
    ()
    
// Executes the lambda on the GPU device.
inl run f = run' f

// Synchronizes all the threads in a block using barrier 0.
// Requires all the threads to execute the same instruction which can cause trouble given that the Cuda compiler does inlining.
// It is recommended to use `barrier_cta_sync 0` instead.
inl __syncthreads() : () = $"__syncthreads()"

// Synchronizes all the threads in a warp.
inl __syncwarp() : () = $"__syncwarp()"

// Synchronizes all the threads in the CTA given the barrier id and the thread count.
inl barrier_cta_sync' ({barrier_id thread_count} : {barrier_id : int; thread_count : int}) : () =
    assert (thread_count % threads_per_warp() = 0) "The number of threads has to be divisible by the warp size."
    assert (barrier_id < 16) "The barrier_id has to be less than 16."
    $'asm("barrier.cta.sync %0, %1;" :: "r"(!(conv barrier_id : i32)), "r"(!(conv thread_count : i32)))'

// Synchronizes all the threads in the CTA given the barrier id.
inl barrier_cta_sync (barrier_id : int) : () =
    assert (threads_per_block() % threads_per_warp() = 0) "The number of threads per block has to be divisible by the warp size."
    assert (barrier_id < 16) "The barrier_id has to be less than 16."
    $'asm("barrier.cta.sync %0;" :: "r"(!(conv barrier_id : i32)))'


// The index of a thread in a block.
inl thread_index() : int = $"threadIdx.x"
// The index of a block in the grid.
inl block_index() : int = $"blockIdx.x" 

open tensorm
inl memcpy_sync forall float. (to, from : tensor int float * tensor int float) =
    assert (from.dim = to.dim) "The tensor dimensions have to be equal in both of the tensors."
    inl dim = from.dim
    inl default() =
        loop.linear dim fun j => 
            tensor_set j (tensor_index j from) to
    real
        open real_core
        open struct
        iter2 (fun (tensor_body {array=from offset=from_offset stride=from_stride}) (tensor_body {array=to offset=to_offset stride=to_stride}) =>
            assert (to_stride = 1 && from_stride = 1) "The innermost dimension of every tensor body needs to have a contiguous stride."
            typecase `from with _ ~el =>
            inl load_single forall el. =
                inl from = $"reinterpret_cast<`el*>(!from + !from_offset)" : $'`el*'
                inl to = $"reinterpret_cast<`el*>(!to + !to_offset)" : $'`el*'
                $'assert("Pointer alignment check" && (unsigned long long)(!from) % !dim == 0 && (unsigned long long)(!to) % !dim == 0)' : ()
                $"*!to = *!from" : ()
            match dim * (sizeof `el).value with
            | 16 => load_single `($"int4")
            | 8 => load_single `($"int2")
            | 4 => load_single `($"int")
            | 2 => load_single `($"short int")
            | 1 => $"*(!to + !to_offset) = *(!from + !from_offset)" : ()
            | _ => default()
            ) from.bodies to.bodies

inl tensor_memcpy_sync forall float a. access_size (r : loop.range (tensor a (float * float))) : () =
    loop_rigid {r with nearTo#=factorize_sizeof access_size} (unzip >> memcpy_sync)

// Reinterpters a value of one type as another. It doesn't do any conversion, but simply preserves the addresses of the values being converted.
// Can be used to convert a f32 into an u32 for use in atomicCAS for example.
inl reinterpret_cast_value forall a b. (x : a) : b = 
    assert ((sizeof : _ a).value = (sizeof : _ b).value) "The two values being converted need to be of the same size."
    real
        open real_core
        if `a `= `b then x else $"reinterpret_cast<`b&>(!x)" : b

// Created an undefined variable for the given type.
inl undefined forall t. : t = index (create 1 : array t) 0

// Suspends the thread for a sleep duration of approximately ns nanoseconds. The maximum sleep duration is approximately 1 millisecond.
inl nanosleep (ns : int) = 
    assert (ns <= 1_000_000) "The maximum sleep duration is approximately 1 millisecond."
    $"__nanosleep(!ns)"