open corebase
open corecuda
open refm
open rangem
open tensorm

// Used in `row_`.
nominal map t = ()
nominal reduce t = ()
nominal singleton t = ()
nominal pair t = ()

union rec compound (f : * -> *) t =
    | Singleton :: forall (f : * -> *) t. f t -> compound f (singleton t)
    | Pair :: forall (f : * -> *) a b. compound f a * compound f b -> compound f (a * b)

inl rec compound_map forall (f : * -> *) (g : * -> *) t. (f : forall t. f t -> g t) : compound f t -> compound g t = function
    | Singleton x => Singleton(f x)
    | Pair(a,b) => Pair(compound_map `$f a, compound_map `$f b)

inl rec compound_iter forall (f : * -> *) t. (f : forall t. f t -> ()) : compound f t -> () = function
    | Singleton x => f x
    | Pair(a,b) => compound_iter `$f a . compound_iter `$f b

inl rec compound_iter2 forall (f : * -> *) (g : * -> *) t. (f : forall t. f t -> g t -> ()) : compound f t * compound g t -> () = function
    | Singleton a, Singleton b => f a b
    | Pair(a,b), Pair(a',b') => compound_iter2 `$f (a, a') . compound_iter2 `$f (b, b')

union rec row_op dim t =
    | RowMap :: forall dim t. tensor (dim * int) t -> row_op dim (map t)
    | RowReduce :: forall dim t. tensor dim t -> row_op dim (reduce t)

union rec row_op_applied dim t =
    | RowMapApplied :: forall dim t. tensor (dim * int * int) t -> row_op_applied dim (map t)
    | RowReduceApplied :: forall dim t. tensor dim t -> row_op_applied dim (reduce t)
    
union rec row_op_local t =
    | RowMapLocal :: forall t. tensor (int * int) t -> row_op_local (map t)
    | RowReduceLocal :: forall t. t -> row_op_local (reduce t)

// The primitives here are only attended for execution on the GPU.
// They're all block level primitives. 

// Maps all the elements of a tensor given the mapping function.
inl map forall dim a b. (f : a -> b) (from : tensor dim a) (to : tensor dim b) : () = 
    assert (from.dim = to.dim) "The dimensions of the two inputs to the map kernel need to be the same."
    inl from,to = factorize_sizeof_16 from, factorize_sizeof_16 to
    loop.projective threads_in_block(fst from.dim) fun i => 
        inl from, to = apply i from, apply i to
        inl l_from, l_to = tensor_create from.dim, tensor_create to.dim
        memcpy_sync (l_from, from)
        pragma.unroll fun _ =>
            loop.linear from.dim fun j => 
                tensor_set j (tensor_index j l_from |> f) l_to
        memcpy_sync (to, l_to)

    __syncthreads()

// Does a blockwise reduction in shared memory and stores the final result into the 0th index of the tensor.
inl block_reduce_store forall a. f result (to : tensor int a) = 
    assert (1 = to.dim) "The output answer has to be of size 1."
    global "#include <cooperative_groups.h>"

    open cooperative_groups

    // The final result for each of the warps.
    inl warp_result = cg_reduce create_coalesced_threads() f result
 
    open tensor_cuda
    inl warps_in_block = warps_in_block()
    inl shared = tensor_create_shared warps_in_block.by

    // All the warps in a block store their intermediate results into the shared tensor.
    tensor_set warps_in_block.from warp_result shared

    __syncthreads()
    
    inl threads_in_warp = threads_in_warp()
    assert (shared.dim <= threads_in_warp.by) "The amount of results in their shared array to be reduced should be less than the number of threads in the warp."
    if warps_in_block.from = 0 && threads_in_warp.from < shared.dim then
        // The first warp reduces the intermediate results in the shared tensor.
        inl final_result = cg_reduce create_coalesced_threads() f (tensor_index threads_in_warp.from shared)
        // Stores the final result into global memory.
        tensor_set 0 final_result to
    
// Reduces all the elements of a tensor to a single one, given the neutral element as well as the reducer function.
inl reduce forall dim a. neutral_element (f : a -> a -> a) (from : tensor dim a) (to : tensor int a) : () =
    // The individual threads iterate over the global tensor, reducing the elements as they go along.
    inl from = factorize_sizeof_16 from
    inl result = loop._dup neutral_element
    loop.projective threads_in_block(fst from.dim) fun i =>
        inl from = apply i from
        inl local = tensor_create from.dim
        memcpy_sync (local, from)
        loop.for {from=0; nearTo=local.dim} (fun i x => f x (tensor_index i local)) result
        |> loop._set result

    block_reduce_store f result to
    __syncthreads()

// Adds the input tensor atomically to the output tensor.
inl local_inplace_atomic_add forall a{number}. (from : tensor (int * int) a) (to : tensor (int * int) a) : () = 
    loop.linear from.dim fun i =>
        tensor_cuda.tensor_atomic_add i (tensor_index i from) to

// Allows the user to specify the output for the map operation.
inl local_inplace_map forall a b. f (from : tensor (int * int) a) (to : tensor (int * int) b) : () = 
    loop.linear from.dim fun i =>
        tensor_set i (f (tensor_index i from)) to

// A local map function.
inl local_map forall a b. f (from : tensor (int * int) a) : tensor (int * int) b = 
    inl to = tensor_create from.dim
    local_inplace_map f from to
    to

nominal row_config = {
    threads_per_miniblock : int
    }

// Does a 2d reduction of the result across all the threads of the miniblock in shared memory
// while taking care that the miniblock results are reduced separately from other miniblocks in the block.
// Is intended to be used after the individual threads have done their local reductions.
// 
// Note: Its intended for internal use due to its utilization of `barrier_sync`.
inl block_reduce_2d forall b. 
        (row_config {threads_per_miniblock})
        (neutral_element : b) 
        (f : b -> b -> b) 
        (result : b) =
    open cooperative_groups
    inl group = create_coalesced_threads()
    // The final result for each of the warps.
    if threads_per_miniblock <= threads_per_warp() then
        inl group = create_labeled_partition group miniblocks_in_block(threads_per_miniblock)().from
        cg_reduce group f result
    else
        inl warp_result = cg_reduce group f result
        inl warps_in_block = warps_in_block()
        inl shared = tensor_cuda.tensor_create_shared (threads_per_block() / threads_per_miniblock, threads_per_miniblock / threads_per_warp())
        assert (loop.prod shared.dim = warps_in_block.by) "The product of shared tensor dimensions should equal the number of warps."

        inl index_warp = loop.proj shared.dim warps_in_block.from
        // All the warps in a block store their intermediate results into the shared tensor.
        tensor_set index_warp warp_result shared

        // semaphore.write_ln_system (fst index_warp)
        barrier_sync {threads_per_miniblock miniblock_index=fst index_warp}
    
        inl threads_in_warp = threads_in_warp()
        inl result = if threads_in_warp.from < snd shared.dim then tensor_index (fst index_warp, threads_in_warp.from) shared else neutral_element
        
        barrier_sync {threads_per_miniblock miniblock_index=fst index_warp}
        
        cg_reduce group f result

// Does a 2d exclusive scan of the result across all the threads of the miniblock in shared memory
// while taking care that the miniblock results are reduced separately from other miniblocks in the block.
// Is intended to be used after the individual threads have done their local reductions.
// 
// Note: Its intended for internal use due to its utilization of `barrier_sync`.
inl block_exclusive_scan_2d forall b. 
        (row_config {threads_per_miniblock})
        (neutral_element : b) 
        (f : b -> b -> b) 
        (result : b) =
    open cooperative_groups

    inl group = create_coalesced_threads()
    // Returns the actual block prefix as well as the final sum.
    if threads_per_miniblock <= threads_per_warp() then
        inl group = create_labeled_partition group miniblocks_in_block(threads_per_miniblock)().from
        cg_exclusive_scan group neutral_element f result
    else
        inl warps_in_block = warps_in_block()
        inl shared = tensor_cuda.tensor_create_shared (threads_per_block() / threads_per_miniblock, threads_per_miniblock / threads_per_warp())
        assert (loop.prod shared.dim = warps_in_block.by) "The product of shared tensor dimensions should equal the number of warps."

        inl warp_prefix,warp_sum = cg_exclusive_scan group neutral_element f result

        inl index_warp = loop.proj shared.dim warps_in_block.from
        // All the warps in a block store their intermediate results into the shared tensor.
        tensor_set index_warp warp_sum shared

        barrier_sync {threads_per_miniblock miniblock_index=fst index_warp}

        inl threads_in_warp = threads_in_warp()
        inl warp_sum = if threads_in_warp.from < snd shared.dim then tensor_index (fst index_warp, threads_in_warp.from) shared else neutral_element
        
        barrier_sync {threads_per_miniblock miniblock_index=fst index_warp}
        
        inl block_prefix_in_thread,block_sum = cg_exclusive_scan group neutral_element f warp_sum
        f (cg_shuffle group block_prefix_in_thread (snd index_warp)) warp_prefix, block_sum

// Does a scan on a local tensor. The first boolean if true makes the scan an inclusive one. The second argument is the config
// passed in by either `row_map` or `row_reduce` and the one after it is the neutral element.
inl local_scan forall a. is_inclusive config (neutral_element : a) f (from : tensor (int * int) a) : tensor (int * int) a = 
    inl to = tensor_create from.dim
    inl previous_block_sum = loop._dup neutral_element
    loop.linear (fst from.dim) fun i =>
        inl from, to = apply i from, apply i to

        inl block_prefix,block_sum = 
            // Reduces the individual thread elements.
            loop.for {from=0; nearTo=from.dim} (fun i x => f x (tensor_index i from)) neutral_element
            // Scans the blockwise results.
            |> block_exclusive_scan_2d config neutral_element f

        // Scans the individual thread elements with the calculated prefix.
        loop.for {from=0; nearTo=from.dim} (fun i s => 
            if is_inclusive then
                inl s = f s (tensor_index i from)
                tensor_set i s to
                s
            else
                inl x = tensor_index i from
                tensor_set i s to
                f s x
            ) (f previous_block_sum block_prefix)
        |> ignore
        loop._set previous_block_sum (f previous_block_sum block_sum)
    to

// Local inclusive scan operation.
inl local_inclusive_scan config = local_scan true config
// Local inclusive scan sum operation. Useful for getting the cumulative probability distribution along a tensor's innermost dimension.
inl local_inclusive_scan_sum config = local_inclusive_scan config 0 (+)
// Local exclusive scan operation.
inl local_exclusive_scan config = local_scan false config

// Does a local reduction operation. Takes in the config from `row_map` or `row_reduce` as its first argument and the neutral element as the second.
inl local_reduce forall a. config (neutral_element : a) f (from : tensor (int * int) a) : a = 
    // Reduces the individual thread elements. 
    inl result = loop._dup neutral_element
    loop.linear from.dim fun i =>
        f result (tensor_index i from)
        |> loop._set result

    // Reduces the blockwise results. Broadcasts the final result to all the threads.
    block_reduce_2d config neutral_element f result

// Local sum. Useful for summing up the innermost dimension of a tensor.
inl local_sum config = local_reduce config 0 (+)
// Gets the actual length of the innermost dimension for the tensor projected into the local tensor in `row_reduce` and `row_map`.
inl local_length (config : row_config) (x : tensor (int * int) _) = config.threads_per_miniblock * length x
// Local average.
inl local_average config x = local_sum config x / conv(local_length config x)
// Local product.
inl local_prod config = local_reduce config 1 (*)
// Local max. The neutral element is inferred based on the input type and is expected to be a primitive type.
inl local_max config = local_reduce config limit.min max
// Local min. The neutral element is inferred based on the input type and is expected to be a primitive type.
inl local_min config = local_reduce config limit.max min

// Template from which row_map and row_reduce are derived from. See the row_map for documentation.
inl row_ forall dim a b.
        (f : row_config -> tensor (int * int) a -> dim -> tensor (int * int) int -> compound row_op_local b)
        (from : tensor (dim * int) a) (to : compound (row_op dim) b) : () =
    inl assert_dim forall b'. (x : row_op dim b') : () =
        match x with
        | RowMap(to) => assert (from.dim = to.dim) "The input and the output tensor dimensions have to be equal."
        | RowReduce(to) => assert (fst from.dim = to.dim) "The input and the output tensor dimensions have to be equal."
    compound_iter `$assert_dim to

    global "#include <cooperative_groups.h>"
    open cooperative_groups

    inl from =
        from
        |> factorize_sizeof_16
        |> split_into_swapped_fst (loop.rigid_split threads_per_block())
        |> curry_fst
    inl dim_block,dim_local,sizeof_16 = from.dim
    inl index_block = loop.proj dim_block thread_index()
    inl threads_per_miniblock = snd dim_block
    inl from =
        from
        |> apply index_block
        |> curry_fst

    inl rec apply_outer_index forall b'. (to : row_op dim b') : row_op_applied dim b' =
        match to with
        | RowMap to =>
            to
            |> factorize_sizeof_16
            |> split_into_swapped_fst (loop.rigid_split threads_per_block())
            |> curry_fst
            |> apply index_block
            |> curry_fst
            |> RowMapApplied
        | RowReduce to =>
            to 
            |> split_into_swapped const(fst dim_block, fst dim_local)
            |> apply (fst index_block)
            |> RowReduceApplied
    inl to = compound_map `$apply_outer_index to

    loop.linear (fst from.dim) fun i =>
        inl from = apply i from
        inl local = tensor_create from.dim
        inl local_index = tensor_create from.dim

        // Loads the tile into the registers.
        loop.linear (fst from.dim) fun i =>
            memcpy_sync (apply i local, apply i from)
        
        // Calculates the indices for the tensor elements.
        loop.linear from.dim fun i => // Note: it's very easy to get the reverse indexing wrong.
            inl dim = fst from.dim, snd dim_block, snd from.dim
            inl index = fst i, snd index_block, snd i
            tensor_set i (loop.proj_rev dim index) local_index

        // Map the local tensor using local operations.
        inl local = 
            inl dim = fst dim_block, fst dim_local
            inl index = fst index_block, i
            f row_config{threads_per_miniblock} local (loop.rigid_merge dim index) local_index

        inl rec store forall b'. (local : row_op_local b') (to : row_op_applied dim b') : () =
            match local, to with 
            | RowMapLocal local, RowMapApplied to =>
                inl to = apply i to
                // Stores the result into global memory after mapping it.
                loop.linear (fst to.dim) fun i =>
                    memcpy_sync (apply i to, apply i local)
            | RowReduceLocal result, RowReduceApplied to =>
                // Stores the result into global memory after reducing it.
                tensor_set i result to
        compound_iter2 `$store (local, to)
                
    __syncthreads()

// A very flexible version of the `map_2d` kernel that allows the user to chain arbitrary local operations inside the mapping function.
// Using `local_map`, `local_reduce` and `local_scan` it's possible to implement most kinds of activation functions,
// softmax, layer norm, discrete sampling and etc.
// 
// The mapping function receives the `row_config` as the first argument which is intended to be passed to local functions in the primitives
// that require it. 
// 
// The second argument to the mapping function is a 2d tensor that holds the individial elements for a thread.
// 
// The third argument to the mapping function takes in the local index of the outermost dimension.
// 
// The last argument to the mapping function takes in the local index of the innermost dimension and can be zipped with the individial elements in the second tensor.
// 
// The reason why the thread-local tensors passed into the mapping funtion are 2d is because the innermost dimension is split twice. Because
// of the aggressive factorization being done in this function each thread in a warp has to load at least 4 elements assuming it's a 32 bit datatype.
// And the total number of block elements has to be at least the number of threads per block times that factor.
// 
// If you get a `The integer length must be distributed in its entirety.` type error then there are not enough elements in the tensor to 
// distribute the workload evenly across all the threads in the block.
inl row_map forall dim a b.
        (f : row_config -> tensor (int * int) a -> dim -> tensor (int * int) int -> tensor (int * int) b)
        (from : tensor (dim * int) a) (to : tensor (dim * int) b) : () =
    row_ (fun config x i j_tns => Singleton RowMapLocal(f config x i j_tns)) from Singleton(RowMap to)

// A reduce version of the `row_map` function. It's very similar to it apart from having the mapping function expect a scalar
// output which is intended to be returned from a `local_reduce` operation. Of course, it is possible to return any kind of
// scalar output from it. See the documentation for `row_map` for more information.
inl row_reduce forall dim a b.
        (f : row_config -> tensor (int * int) a -> dim -> tensor (int * int) int -> b)
        (from : tensor (dim * int) a) (to : tensor dim b) : () =
    row_ (fun config x i j_tns => Singleton RowReduceLocal(f config x i j_tns)) from Singleton(RowReduce to)

// A combined version of row_map and row_reduce that returns two outputs.
inl row_map_reduce forall dim a b c.
        (f : row_config -> tensor (int * int) a -> dim -> tensor (int * int) int -> tensor (int * int) b * c)
        (from : tensor (dim * int) a) (to_map : tensor (dim * int) b) (to_reduce : tensor dim c) : () =
    row_ (fun config x i j_tns => 
        inl a,b = f config x i j_tns
        Pair(Singleton (RowMapLocal a), Singleton (RowReduceLocal b))
        ) from (Pair(Singleton (RowMap to_map), Singleton (RowReduce to_reduce)))

// Reduces the innermost dimension of a given tensor. Less flexible than `row_reduce` since it can only be used to do a single reduction, 
// but can be used on smaller tensors than `row_reduce` allows.
// 
// The first argument is the neutral element.
inl reduce_2d forall dim a. neutral_element (f : a -> a -> a) (from : tensor (dim * int) a) (to : tensor dim a) : () =
    assert (fst from.dim = to.dim) "The first dimension of the input has to equal the dimension of the output in the reduce_2d kernel."
    
    global "#include <cooperative_groups.h>"
    open cooperative_groups
    
    loop.projective warps_in_block(fst from.dim) fun i =>
        inl from = apply i from |> factorize_sizeof_16
        inl result = loop._dup neutral_element
        loop.projective threads_in_warp(fst from.dim) fun i =>
            inl from = apply i from
            inl l_from = tensor_create from.dim
            memcpy_sync (l_from, from)
            loop.for {from=0; nearTo=l_from.dim} (fun i x => f x (tensor_index i l_from)) result
            |> loop._set result

        tensor_set i (cg_reduce create_coalesced_threads() f result) to

    __syncthreads()

// Does a scan over the innermost element of a tensor. Less flexible than `row_map` since it can only be used to do a single reduction, 
// but can be used on smaller tensors than `row_map` allows.
// 
// The first argument if true makes the scan an inclusive one.
// 
// The second is the neutral element.
inl scan_2d forall dim a. is_inclusive neutral_element (f : a -> a -> a) (from : tensor (dim * int) a) (to : tensor (dim * int) a) : () = 
    assert (from.dim = to.dim) "The dimensions of the two inputs to the inclusive scan kernel need to be the same."
    
    global "#include <cooperative_groups.h>"
    open cooperative_groups
    
    loop.projective warps_in_block(fst from.dim) fun i =>
        inl from,to = apply i from |> factorize_sizeof_16, apply i to |> factorize_sizeof_16
        inl result = loop._dup neutral_element
        loop.projective threads_in_warp(fst from.dim) fun i =>
            inl group = create_coalesced_threads()
            inl from,to = apply i from, apply i to
            inl l_from = tensor_create from.dim

            memcpy_sync (l_from, from)
            inl prefix,sum = 
                loop.for {from=0; nearTo=l_from.dim} (fun i x => f x (tensor_index i l_from)) neutral_element
                |> cg_exclusive_scan group neutral_element f
            
            loop.for {from=0; nearTo=l_from.dim} (fun i s => 
                if is_inclusive then
                    inl s = f s (tensor_index i l_from)
                    tensor_set i s l_from
                    s
                else
                    inl x = tensor_index i l_from
                    tensor_set i s l_from
                    f s x
                ) (f result prefix)
            |> ignore
            memcpy_sync (to, l_from)
            loop._set result (f result sum)

    __syncthreads()

// Scans the innermost dimension of a given tensor.
inl inclusive_scan_2d forall dim a. : a -> (a -> a -> a) -> tensor (dim * int) a -> tensor (dim * int) a -> () = scan_2d true
// Scans the innermost dimension of a given tensor.
inl exclusive_scan_2d forall dim a. : a -> (a -> a -> a) -> tensor (dim * int) a -> tensor (dim * int) a -> () = scan_2d false

// Numerically stable softmax.
inl local_softmax config x =
    inl average = local_average config x
    inl x = local_map (fun x => exp (x - average)) x
    inl sum = local_sum config x
    inl len = 1 / conv (local_length config x)
    local_map (fun x => if sum <> 0 then x / sum else len) x

inl local_masked_average config x mask =
    inl count = mask |> local_map (fun mask => if mask then 1 else 0 : int) |> local_sum config
    inl sum = zip x mask |> local_map (fun x,mask => if mask then x else 0) |> local_sum config
    sum / conv count

// Numerically stable softmax.
inl local_masked_softmax forall dim t{number;float}. (mask : row_config -> t -> dim -> int -> bool) config x i j_tns =
    inl mask = local_map (fun x,j => mask config x i j : bool) (zip x j_tns)
    inl average = local_masked_average config x mask
    inl x = local_map (fun x,mask => 
        inl x = if mask then x else limit.min
        exp (x - average)) (zip x mask)
    inl sum = local_sum config x
    inl len = 1 / conv (local_length config x)
    local_map (fun x => if sum <> 0 then x / sum else len) x

inl local_ln_l2 config x =
    inl sum = local_map (fun x => x * x) x |> local_sum config
    local_map (fun x => if sum <> 0 then x / sum else 0) x

inl local_argmax config x (j_tns : tensor (int * int) int) =
    zip x j_tns
    |> local_reduce config (limit.min, 0) (fun a b => if fst a > fst b then a else b)
    |> snd

inl local_discrete_sampling rng config x i (j_tns : tensor (int * int) int) =
    inl x = local_inclusive_scan_sum config x
    inl probability = // TODO: Refactor this.
        local_map (fun j => if j = 0 then random.uniform rng |> conv else 0) j_tns
        |> local_reduce config 0 (fun a b => a)
    inl x = local_map (fun x => x - probability) x
    local_reduce config (limit.min, 0) (fun a b =>
        if fst a >= 0 && fst b >= 0 then
            if fst a <= fst b then a else b
        elif fst a >= 0 then a
        elif fst b >= 0 then b
        else a
    ) (zip x j_tns) |> snd

inl local_softmax_and_discrete_sampling rng config x i (j_tns : tensor (int * int) int) =
    inl x = local_softmax config x
    x, local_discrete_sampling rng config x i j_tns

inl local_masked_softmax_and_discrete_sampling mask rng config x i (j_tns : tensor (int * int) int) =
    inl x = local_masked_softmax mask config x i j_tns
    x, local_discrete_sampling rng config x i j_tns

inl test1() =
    inl m,n : int * int = 3, 36*4
    // inl [ma; mb; mc] = listm.map cupy.random_normal{mean=0; std=1} ([swap (m, k) a_trans; swap (k, n) b_trans; m, n])
    inl input : _ _ int = cupy.arange{from=0; nearTo=m,n; by=1}
    // inl input : _ _ int = cupy.ones (m,n)
    inl output_inclusive : _ _ int = cupy.zeros (m,n)
    inl output_exclusive : _ _ int = cupy.zeros (m,n)
    inl output_reduce : _ _ int = cupy.zeros m
    run fun _ =>
        map ((+) 1) input input
        exclusive_scan_2d 0 (+) input output_exclusive
        inclusive_scan_2d 0 (+) input output_inclusive
        reduce_2d 0 (+) input output_reduce
    printing.tensor_print_ln 1024 input
    printing.tensor_print_ln 1024 output_exclusive
    printing.tensor_print_ln 1024 output_inclusive
    printing.tensor_print_ln 1024 output_reduce
    ()

inl test2() =
    // inl m,n : int * int = (1 <<< 4), (1 <<< 8)
    inl m,n : int * int = (1 <<< 8), (1 <<< 4)
    // inl [ma; mb; mc] = listm.map cupy.random_normal{mean=0; std=1} ([swap (m, k) a_trans; swap (k, n) b_trans; m, n])
    inl input_identity : _ _ int = cupy.arange{from=0; nearTo=m,n; by=1}
    inl input : _ _ float = cupy.random_normal{mean=0; std=1} (m,n)
    inl input_action : _ _ float = cupy.random_uniform m
    // inl input : _ _ float = cupy.ones (m,n)
    inl output_reduce = tensor_create 1
    inl output_identity = tensor_create (m,n)
    inl output_softmax = tensor_create (m,n)
    inl output_softmax' = tensor_create (m,n)
    inl output_masked_softmax = tensor_create (m,n)
    inl output_softmax_scan = tensor_create (m,n)
    inl output_ln = tensor_create (m,n)
    inl output_argmax = tensor_create m
    inl output_sampling = tensor_create m
    inl output_indices_map = tensor_create (m,n)
    inl output_indices_reduction = tensor_create m
    inl output_sum_exclusive = tensor_create (m,n)
    run fun _ =>
        open random
        inl rng : _ philox_state = init {seed = clock64(); subsequence=0; offset=0}
        map ((+) 1) input input
        reduce 0 (+) input output_reduce
        // indices (identity)
        row_map (fun config x i j_tns => x) input_identity output_identity
        // indices (map)
        row_map (fun config x i j_tns => local_map (fun j => i,j) j_tns) input output_indices_map
        // indices (reduction)
        row_reduce (fun config x i j_tns => i) input output_indices_reduction
        // numerically stable softmax
        row_map (fun config x _ _ => local_softmax config x) input output_softmax
        // layer normalization L2
        row_map (fun config x _ _ => local_ln_l2 config x) input output_ln
        // argument maximum
        row_reduce (fun config x _ j_tns => local_argmax config x j_tns) input output_argmax
        // numerically stable softmax (scan)
        row_map (fun config x _ _ =>
            inl x' = local_softmax config x
            inl x = local_inclusive_scan_sum config x'
            zip x' x
            ) input output_softmax_scan
        // sum (exclusive scan)
        row_map (fun config x _ _ =>
            local_exclusive_scan config 0 (+) x
            ) input_identity output_sum_exclusive
        row_map (local_masked_softmax (fun config x i j => j < 4)) input output_masked_softmax
        // // discrete sampling
        // row_map_reduce (local_softmax_and_discrete_sampling rng) input output_softmax' output_sampling
        // masked discrete sampling
        row_map_reduce (local_masked_softmax_and_discrete_sampling (fun config x i j => j < 4) rng) input output_softmax' output_sampling

    // printing.tensor_print_ln 1024 input
    // printing.tensor_print_ln 1024 input_identity
    // printing.tensor_print_ln 1024 output_reduce
    // printing.tensor_print_ln 1024 output_softmax
    // printing.tensor_print_ln 1024 output_masked_softmax
    // printing.tensor_print_ln 1024 output_ln
    // printing.tensor_print_ln 1024 output_argmax
    // printing.tensor_print_ln (m * n) output_softmax_scan
    // printing.tensor_print_ln (m * n) output_identity
    // printing.tensor_print_ln (m * n) output_indices_map
    // printing.tensor_print_ln (m * n) output_indices_reduction
    // printing.tensor_print_ln 1024 output_softmax'
    // printing.tensor_print_ln 1024 output_sampling
    // printing.tensor_print_ln (m * n) output_sum_exclusive
    ()

inl main() = test2()