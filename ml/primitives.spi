open corebase
open corecuda
open coreext
open refm
open rangem
open dep_optionm
open tensorm

// The primitives here are only attended for execution on the GPU.
// They're all block level primitives (unless otherwise noted.)

// Multiplies the elements of min by 2 until either their individual maximum or the overall limit has been reached.
// Useful for calculating the dimensions of the shared memory tensor. 
// 
// The max can be derived from the full tensor that is being segmented, and the min can be some minimum shared tensor size.
// The transpose for example uses 32 x 32 to avoid bank conflicts.
inl frame forall t. (max' : t) (min', limit : t * int) : t * int = real
    open real_core
    struct.mapFoldBack2 (fun min' max' limit =>
        if limit > 1 && min' < max' then min' * 2, limit / 2
        else min', limit
        ) min' max' limit

nominal frame_config t float = {iters : int; min : t; max : t}

// Runs frame for a specified number of iterations.
// Calculates the limit based off the `float` type var and the `dynamic_shared_memory_used` constant.
// Note that the this calculation might not account for the space that the shared tensor requires for padding.
inl frame_iter forall t float. (frame_config {iters min max} : _ t float) =
    assert (lit_is iters) "The number of iterations to convergence needs to be known at compile time."
    inl limit = dynamic_shared_memory_used() / (sizeof : _ float).value / loop.prod min // TODO: Might not account for padding.
    inl rec loop i s = if i > 0 then loop (i-1) (frame max s) else s
    loop iters (min, limit) |> fst

// Transposes the two inner dimensions of a tensor.
inl transpose_snd_ forall a b c el. loop' (from : tensor (a * b * c) el) (to : tensor (a * c * b) el) : () =
    inl x = 
        inl a,b,c = from.dim
        inl a',c',b' = to.dim
        assert (a = a') "The outer dimensions must be equal in transpose."
        assert (b = b') "The middle dimensions must be equal in transpose."
        assert (c = c') "The inner dimensions must be equal in transpose."

        zip from (reorder (fun a,c,b => a,b,c) to)
        |> reorder (fun a,b,c => b,a,c)
        |> flatten_fst
        |> reorder (fun b,a,c => c,b,a)
        |> flatten_fst
        |> reorder (fun c,b,a => a,{b},{c})
    
    inl body forall float. (x : tensor (a * {b : i32} * {c : i32}) (float * float)) : () =
        open tensor_cuda
        
        inl shared : tensor _ float =
            inl padding, shared_dim_min =
                match (sizeof : _ float).value with
                | 8 => 1, 16, 16
                | 4 => 1, 32, 32
                | 2 => 2, 64, 64
                | 1 => 4, 128, 128
                | _ => failwith "Only 8,16,32 and 64 bit datatypes (or compounds containing them) are supported in transpose."
            inl a,{b},{c} = x.dim
            inl shared_dim = frame_iter (frame_config {iters=4; min=shared_dim_min; max=b,c} : _ _ float)
            tensor_create_extern_shared 0 ({b=fst shared_dim}, {c=snd shared_dim + padding})
            |> fst
            |> view (fun _ => {from=loop.zeroes; nearTo={b=min (fst shared_dim) b}, {c=min (snd shared_dim) c}})

        inl x = 
            x
            |> split_into_snd (loop.div_fst_by shared.dim)
            |> reorder (fun a,b,c => (a,b),c)

        loop' (fst x.dim) fun i =>
            inl from, to = x |> apply i |> unzip

            inl () = // Copies the data into shared memory from global.
                inl io = zip shared from // Becuase the shared tensor isn't 16 byte aligned, we opt not to factorize it.

                loop.projective threads_in_block(io.dim) fun i =>
                    inl to, from = io |> unzip
                    tensor_set i (tensor_index i from) to

            __syncthreads()
            
            inl () = // Copies the data into global from shared memory.
                inl io = // In addition to the shared tensor not being aligned, loading it into registers locally would result in 4-way bank conflicts.
                    zip to shared
                    |> reorder (fun b,c => c,b)

                loop.projective threads_in_block(io.dim) fun i =>
                    inl to, from = io |> unzip
                    tensor_set i (tensor_index i from) to

            __syncthreads()

    iter2_type `$body x

// Transposes the two inner dimensions of a tensor using a single block.
inl transpose_snd forall a b c el. (from : tensor (a * b * c) el) (to : tensor (a * c * b) el) : () =
    transpose_snd_ loop.linear from to

// Transposes the two dimensions of a tensor using a single block.
inl transpose forall a b el. grid (from : tensor (a * b) el) (to : tensor (b * a) el) : () =
    transpose_snd (from |> reorder (fun a,b => {},a,b)) (to |> reorder (fun b,a => {},b,a))

// Transposes the two inner dimensions of a tensor using the entire grid.
inl grid_transpose_snd forall a b c el. (from : tensor (a * b * c) el) (to : tensor (a * c * b) el) : () =
    transpose_snd_ (loop.projective << blocks_in_grid) from to

// Transposes the two dimensions of a tensor using the entire grid.
inl grid_transpose forall a b el. (from : tensor (a * b) el) (to : tensor (b * a) el) : () =
    grid_transpose_snd (from |> reorder (fun a,b => {},a,b)) (to |> reorder (fun b,a => {},b,a))

// Maps all the elements of a tensor given the mapping function.
inl map_ forall dim a b. loop' (f : a -> b) (from : tensor dim a) (to : tensor dim b) : () = 
    assert (from.dim = to.dim) "The dimensions of the two inputs to the map kernel need to be the same."
    inl from,to = factorize_sizeof_16 from, factorize_sizeof_16 to
    loop' (fst from.dim) fun i => 
        inl from, to = apply i from, apply i to
        inl l_from, l_to = tensor_create from.dim, tensor_create to.dim
        memcpy_sync (l_from, from)
        pragma.unroll fun _ =>
            loop.linear from.dim fun j => 
                tensor_set j (tensor_index j l_from |> f) l_to
        memcpy_sync (to, l_to)

// Maps all the elements of a tensor given the mapping function.
inl map forall dim a b. (f : a -> b) (from : tensor dim a) (to : tensor dim b) : () = 
    map_ (loop.projective << threads_in_block) f from to . __syncthreads()
    
// Maps all the elements of a tensor given the mapping function using the entire grid.
inl grid_map forall dim a b. grid (f : a -> b) (from : tensor dim a) (to : tensor dim b) : () = 
    map_ (loop.projective << threads_in_grid) f from to . cooperative_groups.sync grid

// Does a blockwise reduction in shared memory and stores the final result into the 0th index of the tensor.
inl block_reduce_store forall a. extern_offset f result (to : tensor int a) = 
    assert (1 = to.dim) "The output answer has to be of size 1."
    global "#include <cooperative_groups.h>"

    open cooperative_groups

    // The final result for each of the warps.
    inl warp_result = cg_reduce create_coalesced_threads() f result
 
    open tensor_cuda
    inl warps_in_block = warps_in_block()
    inl shared,_ = tensor_create_extern_shared extern_offset warps_in_block.by

    // All the warps in a block store their intermediate results into the shared tensor.
    tensor_set warps_in_block.from warp_result shared

    __syncthreads()
    
    inl threads_in_warp = threads_in_warp()
    assert (shared.dim <= threads_in_warp.by) "The amount of results in their shared array to be reduced should be less than the number of threads in the warp."
    if warps_in_block.from = 0 && threads_in_warp.from < shared.dim then
        // The first warp reduces the intermediate results in the shared tensor.
        inl final_result = cg_reduce create_coalesced_threads() f (tensor_index threads_in_warp.from shared)
        // Stores the final result into global memory.
        tensor_set 0 final_result to
    __syncthreads() // We need to make sure the shared memory isn't reused by other threads.
    
// Reduces all the elements of a tensor to a single one, given the neutral element as well as the reducer function.
inl reduce forall dim a. extern_offset neutral_element (f : a -> a -> a) (from : tensor dim a) (to : tensor int a) : () =
    // The individual threads iterate over the global tensor, reducing the elements as they go along.
    inl from = factorize_sizeof_16 from
    inl result = loop._dup neutral_element
    loop.projective threads_in_block(fst from.dim) fun i =>
        inl from = apply i from
        inl local = tensor_create from.dim
        memcpy_sync (local, from)
        loop.for {from=0; nearTo=local.dim} (fun i x => f x (tensor_index i local)) result
        |> loop._set result
    block_reduce_store extern_offset f result to

// Allows the user to specify the output for the map operation.
inl local_map' forall a b. f (from : tensor (int * int) a) (to : tensor (int * int) b) : () = 
    loop.linear from.dim fun i =>
        tensor_set i (f (tensor_index i from)) to

// A local map function.
inl local_map forall a b. f (from : tensor (int * int) a) : tensor (int * int) b = 
    inl to = tensor_create from.dim
    local_map' f from to
    to

// A local iter function.
inl local_iter forall a. f (from : tensor (int * int) a) : () = 
    loop.linear from.dim fun i =>
        f (tensor_index i from)

nominal row_config = {
    threads_per_miniblock : int
    extern_offset : size
    }

// Does a 2d reduction of the result across all the threads of the miniblock in shared memory
// while taking care that the miniblock results are reduced separately from other miniblocks in the block.
// Is intended to be used after the individual threads have done their local reductions.
// 
// Note: Its intended for internal use due to its utilization of `barrier_sync`.
inl block_reduce_2d forall b. 
        (row_config {threads_per_miniblock extern_offset})
        (neutral_element : b) 
        (f : b -> b -> b) 
        (result : b) =
    open cooperative_groups
    inl group = create_coalesced_threads()
    // The final result for each of the warps.
    if threads_per_miniblock <= threads_per_warp() then
        inl group = create_labeled_partition group miniblocks_in_block(threads_per_miniblock)().from
        cg_reduce group f result
    else
        inl warp_result = cg_reduce group f result
        inl warps_in_block = warps_in_block()
        inl shared,_ = tensor_cuda.tensor_create_extern_shared extern_offset (threads_per_block() / threads_per_miniblock, threads_per_miniblock / threads_per_warp())
        assert (loop.prod shared.dim = warps_in_block.by) "The product of shared tensor dimensions should equal the number of warps."

        inl index_warp = loop.proj shared.dim warps_in_block.from
        // All the warps in a block store their intermediate results into the shared tensor.
        tensor_set index_warp warp_result shared

        barrier_cta_sync' {thread_count=threads_per_miniblock; barrier_id=fst index_warp + 1}
    
        inl threads_in_warp = threads_in_warp()
        inl result = if threads_in_warp.from < snd shared.dim then tensor_index (fst index_warp, threads_in_warp.from) shared else neutral_element
        
        __syncthreads() // We need to make sure the shared memory isn't reused by other threads.
        
        cg_reduce group f result

// Does a 2d exclusive scan of the result across all the threads of the miniblock in shared memory
// while taking care that the miniblock results are reduced separately from other miniblocks in the block.
// Is intended to be used after the individual threads have done their local reductions.
// 
// Note: Its intended for internal use due to its utilization of `barrier_sync`.
inl block_exclusive_scan_2d forall b. 
        (row_config {threads_per_miniblock extern_offset})
        (neutral_element : b) 
        (f : b -> b -> b) 
        (result : b) =
    open cooperative_groups

    inl group = create_coalesced_threads()
    // Returns the actual block prefix as well as the final sum.
    if threads_per_miniblock <= threads_per_warp() then
        inl group = create_labeled_partition group miniblocks_in_block(threads_per_miniblock)().from
        cg_exclusive_scan group neutral_element f result
    else
        inl warps_in_block = warps_in_block()
        inl shared,_ = tensor_cuda.tensor_create_extern_shared extern_offset (threads_per_block() / threads_per_miniblock, threads_per_miniblock / threads_per_warp())
        assert (loop.prod shared.dim = warps_in_block.by) "The product of shared tensor dimensions should equal the number of warps."

        inl warp_prefix,warp_sum = cg_exclusive_scan group neutral_element f result

        inl index_warp = loop.proj shared.dim warps_in_block.from
        // All the warps in a block store their intermediate results into the shared tensor.
        tensor_set index_warp warp_sum shared

        barrier_cta_sync' {thread_count=threads_per_miniblock; barrier_id=fst index_warp + 1}

        inl threads_in_warp = threads_in_warp()
        inl warp_sum = if threads_in_warp.from < snd shared.dim then tensor_index (fst index_warp, threads_in_warp.from) shared else neutral_element
        
        __syncthreads() // We need to make sure the shared memory isn't reused by other threads.

        inl block_prefix_in_thread,block_sum = cg_exclusive_scan group neutral_element f warp_sum
        f (cg_shuffle group block_prefix_in_thread (snd index_warp)) warp_prefix, block_sum

// Does a scan on a local tensor. The first boolean if true makes the scan an inclusive one. The second argument is the config
// passed in by either `row_map` or `row_reduce` and the one after it is the neutral element.
inl local_scan forall a. is_inclusive config (neutral_element : a) f (from : tensor (int * int) a) : tensor (int * int) a = 
    inl to = tensor_create from.dim
    inl previous_block_sum = loop._dup neutral_element
    loop.linear (fst from.dim) fun i =>
        inl from, to = apply i from, apply i to

        inl block_prefix,block_sum = 
            // Reduces the individual thread elements.
            loop.for {from=0; nearTo=from.dim} (fun i x => f x (tensor_index i from)) neutral_element
            // Scans the blockwise results.
            |> block_exclusive_scan_2d config neutral_element f

        // Scans the individual thread elements with the calculated prefix.
        loop.for {from=0; nearTo=from.dim} (fun i s => 
            if is_inclusive then
                inl s = f s (tensor_index i from)
                tensor_set i s to
                s
            else
                inl x = tensor_index i from
                tensor_set i s to
                f s x
            ) (f previous_block_sum block_prefix)
        |> ignore
        loop._set previous_block_sum (f previous_block_sum block_sum)
    to

// Local inclusive scan operation.
inl local_inclusive_scan config = local_scan true config
// Local inclusive scan sum operation. Useful for getting the cumulative probability distribution along a tensor's innermost dimension.
inl local_inclusive_scan_sum config = local_inclusive_scan config 0 (+)
// Local exclusive scan operation.
inl local_exclusive_scan config = local_scan false config

// Does a local reduction operation. Takes in the config from `row_map` or `row_reduce` as its first argument and the neutral element as the second.
inl local_reduce forall a. config (neutral_element : a) f (from : tensor (int * int) a) : a = 
    // Reduces the individual thread elements. 
    inl result = loop._dup neutral_element
    loop.linear from.dim fun i =>
        f result (tensor_index i from)
        |> loop._set result

    // Reduces the blockwise results. Broadcasts the final result to all the threads.
    block_reduce_2d config neutral_element f result

// Local sum. Useful for summing up the innermost dimension of a tensor.
inl local_sum config = local_reduce config 0 (+)
// Gets the actual length of the innermost dimension for the tensor projected into the local tensor in `row_reduce` and `row_map`.
inl local_length (config : row_config) (x : tensor (int * int) _) = config.threads_per_miniblock * length x
// Local average.
inl local_average config x = local_sum config x / conv(local_length config x)
// Local product.
inl local_prod config = local_reduce config 1 (*)
// Local max. The neutral element is inferred based on the input type and is expected to be a primitive type.
inl local_max config = local_reduce config limit.min max
// Local min. The neutral element is inferred based on the input type and is expected to be a primitive type.
inl local_min config = local_reduce config limit.max min

// The template function from which `row_gather_map`, `row_gather_reduce` and `row_gather_void` are derived from.
inl row_gather_ forall passthrough a b c.
        (passthrough : passthrough)
        (f : passthrough -> row_config -> tensor (int * int) a -> int -> tensor (int * int) int -> tensor (int * int) b * c)
        (io : tensor int (a * b)) : c =
    assert (lit_is io.dim) "The tensor dimension needs to be known at compile time." 

    inl dim_inner,dim_sizeof16 = (factorize_sizeof_16 io).dim
    inl (dim_block,dim_inner),(dim_block',dim_inner') = loop.rigid_split threads_per_block() (threads_per_block(), dim_inner)
    inl threads_per_miniblock = dim_inner

    inl io = apply_ptr io

    transposing_loop.linear {dim_inner passthrough} fun {extern_offset passthrough} thread_id index_inner =>
        inl from,to =
            io
            |> reshape const(dim_inner',dim_inner,dim_sizeof16) 
            |> reorder (fun dim_inner',dim_inner,dim_sizeof16 => dim_inner,dim_inner',dim_sizeof16) 
            |> apply index_inner
            |> unzip

        // Creates tensors in local memory.
        inl local = tensor_create from.dim
        inl local_index = tensor_create from.dim

        // Loads the tile into the registers.
        loop.linear (fst from.dim) fun i =>
            memcpy_sync (apply i local, apply i from)
        
        // Calculates the indices for the tensor elements.
        loop.linear from.dim fun i => // Note: it's very easy to get the reverse indexing wrong.
            inl dim = dim_inner', dim_inner, dim_sizeof16
            inl index = fst i, index_inner, snd i
            tensor_set i (loop.proj_rev dim index) local_index

        // Map the local tensor using local operations.
        inl local_out, local_reduce = f passthrough row_config{threads_per_miniblock extern_offset} local thread_id local_index
        
        // Stores the result into global memory after mapping it.
        loop.linear (fst to.dim) fun i =>
            memcpy_sync (apply i to, apply i local_out)
        
        local_reduce

inl row_gather_map_reduce forall passthrough a b c.
        (passthrough : passthrough)
        (f : passthrough -> row_config -> tensor (int * int) a -> int -> tensor (int * int) int -> tensor (int * int) b * c)
        (from : tensor int a) (to : tensor int b) : c =
    assert (from.dim = to.dim) "The input and the output tensor dimensions have to be equal." // `zip` will already assert this, but this is just to make the error messages better.
    row_gather_ passthrough f (zip from to)

inl row_gather_map forall passthrough a b.
        (passthrough : passthrough)
        (f : passthrough -> row_config -> tensor (int * int) a -> int -> tensor (int * int) int -> tensor (int * int) b)
        (from : tensor int a) (to : tensor int b) : () =
    assert (from.dim = to.dim) "The input and the output tensor dimensions have to be equal."
    inl {} = row_gather_ passthrough (fun passthrough config x i j_tns => f passthrough config x i j_tns, {}) (zip from to)
    ()

inl row_gather_reduce forall passthrough a b.
        (passthrough : passthrough)
        (f : passthrough -> row_config -> tensor (int * int) a -> int -> tensor (int * int) int -> b)
        (from : tensor int a) : b =
    row_gather_ passthrough (fun passthrough config x i j_tns => (tensor_create x.dim : _ _ {}), f passthrough config x i j_tns) (zip from (tensor_create from.dim))

inl row_gather_void forall passthrough a.
        (passthrough : passthrough)
        (f : passthrough -> row_config -> tensor (int * int) a -> int -> tensor (int * int) int -> ())
        (from : tensor int a) : () =
    inl {} = row_gather_ passthrough (fun passthrough config x i j_tns => (tensor_create x.dim : _ _ {}), (f passthrough config x i j_tns . {})) (zip from (tensor_create from.dim))
    ()

// Template from which row_map and row_reduce are derived from. See the row_map for documentation.
inl row_ forall dim a kb b kc c.
        loop_
        (f : row_config -> tensor (int * int) a -> dim -> tensor (int * int) int -> dep_option kb (tensor (int * int) b) * dep_option kc c)
        (from : tensor (dim * int) a) (to_map : dep_option kb (tensor (dim * int) b)) (to_reduce : dep_option kc (tensor dim c)) : () =
    // Note(8/20/2024): I didn't realize that instead of using dep_option, I could have used tensor _ {} for empty tensors like in row_gather_.
    to_map |> dep_optionm.iter fun to => assert (from.dim = to.dim) "The input and the output tensor dimensions have to be equal."
    to_reduce |> dep_optionm.iter fun to => assert (fst from.dim = to.dim) "The (first) input and the output tensor dimensions have to be equal."

    global "#include <cooperative_groups.h>"
    open cooperative_groups

    inl from =
        from
        |> factorize_sizeof_16
        |> split_into_swapped_fst (loop.rigid_split threads_per_block())
        |> curry_fst
    inl dim_block,dim_local,sizeof_16 = from.dim
    inl index_block = loop.proj dim_block thread_index()
    inl threads_per_miniblock = snd dim_block
    inl from =
        from
        |> apply index_block
        |> curry_fst

    inl to_map =
        to_map |> dep_optionm.map fun to =>
            to
            |> factorize_sizeof_16
            |> split_into_swapped_fst (loop.rigid_split threads_per_block())
            |> curry_fst
            |> apply index_block
            |> curry_fst

    inl to_reduce =
        to_reduce |> dep_optionm.map fun to =>
            to 
            |> split_into_swapped const(fst dim_block, fst dim_local)
            |> apply (fst index_block)

    loop_ (fst from.dim) fun i =>
        inl from = apply i from
        inl local = tensor_create from.dim
        inl local_index = tensor_create from.dim

        // Loads the tile into the registers.
        loop.linear (fst from.dim) fun i =>
            memcpy_sync (apply i local, apply i from)
        
        // Calculates the indices for the tensor elements.
        loop.linear from.dim fun i => // Note: it's very easy to get the reverse indexing wrong.
            inl dim = fst from.dim, snd dim_block, snd from.dim
            inl index = fst i, snd index_block, snd i
            tensor_set i (loop.proj_rev dim index) local_index

        // Map the local tensor using local operations.
        inl local = 
            inl dim = fst dim_block, fst dim_local
            inl index = fst index_block, i
            f row_config{threads_per_miniblock extern_offset=0} local (loop.rigid_merge dim index) local_index

        (fst local, to_map) ||> dep_optionm.iter2 fun local to =>
            inl to = apply i to
            // Stores the result into global memory after mapping it.
            loop.linear (fst to.dim) fun i =>
                memcpy_sync (apply i to, apply i local)
        (snd local, to_reduce) ||> dep_optionm.iter2 fun result to =>
            // Stores the result into global memory after reducing it.
            tensor_set i result to
                
inl loop_block dim f = loop.linear dim f . __syncthreads()
inl loop_grid dim f = loop.projective rangem.blocks_in_grid(dim) f

// A very flexible version of the `map_2d` kernel that allows the user to chain arbitrary local operations inside the mapping function.
// Using `local_map`, `local_reduce` and `local_scan` it's possible to implement most kinds of activation functions,
// softmax, layer norm, discrete sampling and etc.
// 
// The mapping function receives the `row_config` as the first argument which is intended to be passed to local functions in the primitives
// that require it. 
// 
// The second argument to the mapping function is a 2d tensor that holds the individial elements for a thread.
// 
// The third argument to the mapping function takes in the local index of the outermost dimension.
// 
// The last argument to the mapping function takes in the local index of the innermost dimension and can be zipped with the individial elements in the second tensor.
// 
// The reason why the thread-local tensors passed into the mapping funtion are 2d is because the innermost dimension is split twice. Because
// of the aggressive factorization being done in this function each thread in a warp has to load at least 4 elements assuming it's a 32 bit datatype.
// And the total number of block elements has to be at least the number of threads per block times that factor.
// 
// If you get a `The integer length must be distributed in its entirety.` type error then there are not enough elements in the tensor to 
// distribute the workload evenly across all the threads in the block.
inl row_map forall dim a b.
        (f : row_config -> tensor (int * int) a -> dim -> tensor (int * int) int -> tensor (int * int) b)
        (from : tensor (dim * int) a) (to : tensor (dim * int) b) : () =
    row_ loop_block (fun config x i j_tns => Some'(f config x i j_tns),None') from (Some' to) (None' : _ _ (_ _ ()))

// A reduce version of the `row_map` function. It's very similar to it apart from having the mapping function expect a scalar
// output which is intended to be returned from a `local_reduce` operation. Of course, it is possible to return any kind of
// scalar output from it. See the documentation for `row_map` for more information.
inl row_reduce forall dim a b.
        (f : row_config -> tensor (int * int) a -> dim -> tensor (int * int) int -> b)
        (from : tensor (dim * int) a) (to : tensor dim b) : () =
    row_ loop_block (fun config x i j_tns => None',Some'(f config x i j_tns)) from (None' : _ _ (_ _ ())) (Some' to) 

// A combined version of row_map and row_reduce that returns two outputs.
inl row_map_reduce forall dim a b c.
        (f : row_config -> tensor (int * int) a -> dim -> tensor (int * int) int -> tensor (int * int) b * c)
        (from : tensor (dim * int) a) (to_map : tensor (dim * int) b) (to_reduce : tensor dim c) : () =
    row_ loop_block (fun config x i j_tns => 
        inl a,b = f config x i j_tns
        Some' a, Some' b
        ) from (Some' to_map) (Some' to_reduce)

// A version of `row_map` for when the output is a C void type. It's necessary because the Cuda compiler cannot have variables that are void.
// Outputs could potentially be extracted from it using manual stores using the provided indices.
inl row_void forall dim a b.
        (f : row_config -> tensor (int * int) a -> dim -> tensor (int * int) int -> ())
        (from : tensor (dim * int) a) (to : tensor (dim * int) b) : () =
    row_ loop_block (fun config x i j_tns => f config x i j_tns . None',None') from (None' : _ _ (_ _ ())) (None' : _ _ (_ _ ()))

// Gridwise version of row_map. Uses the entire grid to iterate over a tensor instead of just a single block.
inl grid_row_map' forall dim a b.
        (f : row_config -> tensor (int * int) a -> dim -> tensor (int * int) int -> tensor (int * int) b)
        (from : tensor (dim * int) a) (to : tensor (dim * int) b) : () =
    row_ loop_grid (fun config x i j_tns => Some'(f config x i j_tns),None') from (Some' to) (None' : _ _ (_ _ ()))

// Gridwise version of row_map. Does grid synchronization at the end. Uses the entire grid to iterate over a tensor instead of just a single block.
inl grid_row_map forall dim a b.
        grid (f : row_config -> tensor (int * int) a -> dim -> tensor (int * int) int -> tensor (int * int) b)
        (from : tensor (dim * int) a) (to : tensor (dim * int) b) : () =
    grid_row_map' f from to . cooperative_groups.sync grid

// Gridwise version of row_reduce. Uses the entire grid to iterate over a tensor instead of just a single block.
inl grid_row_reduce' forall dim a b.
        (f : row_config -> tensor (int * int) a -> dim -> tensor (int * int) int -> b)
        (from : tensor (dim * int) a) (to : tensor dim b) : () =
    row_ loop_grid (fun config x i j_tns => None',Some'(f config x i j_tns)) from (None' : _ _ (_ _ ())) (Some' to) 

// Gridwise version of row_reduce. Does grid synchronization at the end. Uses the entire grid to iterate over a tensor instead of just a single block.
inl grid_row_reduce forall dim a b.
        grid (f : row_config -> tensor (int * int) a -> dim -> tensor (int * int) int -> b)
        (from : tensor (dim * int) a) (to : tensor dim b) : () =
    grid_row_reduce' f from to . cooperative_groups.sync grid

// Gridwise version of row_map_reduce. Uses the entire grid to iterate over a tensor instead of just a single block.
inl grid_row_map_reduce' forall dim a b c.
        (f : row_config -> tensor (int * int) a -> dim -> tensor (int * int) int -> tensor (int * int) b * c)
        (from : tensor (dim * int) a) (to_map : tensor (dim * int) b) (to_reduce : tensor dim c) : () =
    row_ loop_grid (fun config x i j_tns => 
        inl a,b = f config x i j_tns
        Some' a, Some' b
        ) from (Some' to_map) (Some' to_reduce)
// Gridwise version of row_map_reduce. Does grid synchronization at the end. Uses the entire grid to iterate over a tensor instead of just a single block.
inl grid_row_map_reduce forall dim a b c.
        grid (f : row_config -> tensor (int * int) a -> dim -> tensor (int * int) int -> tensor (int * int) b * c)
        (from : tensor (dim * int) a) (to_map : tensor (dim * int) b) (to_reduce : tensor dim c) : () =
    grid_row_map_reduce' f from to_map to_reduce . cooperative_groups.sync grid

// Gridwise version of row_void. Uses the entire grid to iterate over a tensor instead of just a single block.
inl grid_row_void' forall dim a b.
        (f : row_config -> tensor (int * int) a -> dim -> tensor (int * int) int -> ())
        (from : tensor (dim * int) a) (to : tensor (dim * int) b) : () =
    row_ loop_grid (fun config x i j_tns => f config x i j_tns . None',None') from (None' : _ _ (_ _ ())) (None' : _ _ (_ _ ()))
// Gridwise version of row_void. Does grid synchronization at the end. Uses the entire grid to iterate over a tensor instead of just a single block.
inl grid_row_void forall dim a b.
        grid (f : row_config -> tensor (int * int) a -> dim -> tensor (int * int) int -> ())
        (from : tensor (dim * int) a) (to : tensor (dim * int) b) : () =
    grid_row_void' f from to . cooperative_groups.sync grid

// Reduces the innermost dimension of a given tensor. Less flexible than `row_reduce` since it can only be used to do a single reduction, 
// but can be used on smaller tensors than `row_reduce` allows.
// 
// The first argument is the neutral element.
inl reduce_2d forall dim a. neutral_element (f : a -> a -> a) (from : tensor (dim * int) a) (to : tensor dim a) : () =
    assert (fst from.dim = to.dim) "The first dimension of the input has to equal the dimension of the output in the reduce_2d kernel."
    
    global "#include <cooperative_groups.h>"
    open cooperative_groups
    
    loop.projective warps_in_block(fst from.dim) fun i =>
        inl from = apply i from |> factorize_sizeof_16
        inl result = loop._dup neutral_element
        loop.projective threads_in_warp(fst from.dim) fun i =>
            inl from = apply i from
            inl l_from = tensor_create from.dim
            memcpy_sync (l_from, from)
            loop.for {from=0; nearTo=l_from.dim} (fun i x => f x (tensor_index i l_from)) result
            |> loop._set result

        tensor_set i (cg_reduce create_coalesced_threads() f result) to

    __syncthreads()

// Does a scan over the innermost element of a tensor. Less flexible than `row_map` since it can only be used to do a single reduction, 
// but can be used on smaller tensors than `row_map` allows.
// 
// The first argument if true makes the scan an inclusive one.
// 
// The second is the neutral element.
inl scan_2d forall dim a. is_inclusive neutral_element (f : a -> a -> a) (from : tensor (dim * int) a) (to : tensor (dim * int) a) : () = 
    assert (from.dim = to.dim) "The dimensions of the two inputs to the inclusive scan kernel need to be the same."
    
    global "#include <cooperative_groups.h>"
    open cooperative_groups
    
    loop.projective warps_in_block(fst from.dim) fun i =>
        inl from,to = apply i from |> factorize_sizeof_16, apply i to |> factorize_sizeof_16
        inl result = loop._dup neutral_element
        loop.projective threads_in_warp(fst from.dim) fun i =>
            inl group = create_coalesced_threads()
            inl from,to = apply i from, apply i to
            inl l_from = tensor_create from.dim

            memcpy_sync (l_from, from)
            inl prefix,sum = 
                loop.for {from=0; nearTo=l_from.dim} (fun i x => f x (tensor_index i l_from)) neutral_element
                |> cg_exclusive_scan group neutral_element f
            
            loop.for {from=0; nearTo=l_from.dim} (fun i s => 
                if is_inclusive then
                    inl s = f s (tensor_index i l_from)
                    tensor_set i s l_from
                    s
                else
                    inl x = tensor_index i l_from
                    tensor_set i s l_from
                    f s x
                ) (f result prefix)
            |> ignore
            memcpy_sync (to, l_from)
            loop._set result (f result sum)

    __syncthreads()

// Scans the innermost dimension of a given tensor.
inl inclusive_scan_2d forall dim a. : a -> (a -> a -> a) -> tensor (dim * int) a -> tensor (dim * int) a -> () = scan_2d true
// Scans the innermost dimension of a given tensor.
inl exclusive_scan_2d forall dim a. : a -> (a -> a -> a) -> tensor (dim * int) a -> tensor (dim * int) a -> () = scan_2d false

inl local_ln_l2 config x =
    inl sum = local_map (fun x => x * x) x |> local_sum config
    local_map (fun x => if sum <> 0 then x / sum else 0) x

inl local_argmax config x (j_tns : tensor (int * int) int) =
    zip x j_tns
    |> local_reduce config (limit.min, 0) (fun a b => if fst a > fst b then a else b)
    |> snd

// Numerically stable softmax.
inl local_softmax config x =
    inl average = local_average config x
    inl x = local_map (fun x => exp (x - average)) x
    inl sum = local_sum config x
    local_map (fun x => x / sum) x

inl local_regret_matching config x =
    inl x = local_map (max 0) x
    inl sum = local_sum config x
    inl inv_len = 1 / conv (local_length config x)
    local_map (fun x => if sum <> 0 then x / sum else inv_len) x

// Clamps the norm of a vector to a threeshold if it is above it.
inl local_l1_normalize threeshold config x =
    inl x = local_map abs x
    inl sum = local_sum config x
    inl v = if sum > threeshold then threeshold / sum else 1
    local_map ((*) v) x

inl local_masked_reduce forall t. config neutral_element (f : t -> t -> t) (x' : tensor (int * int) (t * bool)) : t =
    local_reduce config (neutral_element,false) (fun a b =>
        match a, b with
        | (a,true), (b,true) => f a b, true
        | (_,true as x), _ | _, (_,true as x) => x
        | _ => a
        ) x'
    |> fun (x,cond) => 
        // if not cond then
        //     console.write_ln {x' tid=threads_in_grid().from}
        assert cond "The local reduce must be true." . x

inl local_masked_count config = local_map (fun mask => if mask then 1 else 0 : int) >> local_sum config
inl local_masked_sum config mask x = zip x mask |> local_map (fun x,mask => if mask then x else 0) |> local_sum config
inl local_masked_average config mask x = local_masked_sum config mask x / conv (local_masked_count config mask)

// Numerically stable softmax.
inl local_masked_softmax forall dim t{number;float}. (mask : row_config -> t -> dim -> int -> bool) config x i j_tns =
    inl mask = local_map (fun x,j => mask config x i j : bool) (zip x j_tns)
    inl average = local_masked_average config mask x
    inl x = 
        local_map (fun x,mask => 
            inl q = exp ((if mask then x else limit.min) - average)
            assert (q < inf) "The softmax values must not grow too large."
            assert (not (nan_is q)) "The softmax values must not be nans."
            q
            ) (zip x mask)
    inl sum = local_sum config x
    local_map (fun x => x / sum) x

inl local_masked_regret_matching forall dim t{number;float}. (mask : row_config -> t -> dim -> int -> bool) config x i j_tns =
    inl mask = local_map (fun x,j => mask config x i j : bool) (zip x j_tns)
    inl x = local_map (fun x,mask => if mask then max 0 x else 0) (zip x mask)
    inl sum = local_sum config x
    inl inv_len = 1 / conv (local_masked_count config mask)
    local_map (fun x, mask => 
        if not mask then 0
        elif sum <> 0 then x / sum
        else inv_len
        ) (zip x mask)

inl local_discrete_sampling rng config x i (j_tns : tensor (int * int) int) =
    inl cdf = local_inclusive_scan_sum config x
    inl x = local_map (fun (sum, orig) => sum, orig > 0) (zip cdf x)
    inl max_probability = local_masked_reduce config limit.min max x
    inl probability = // TODO: Needs to be optimized and factored out into local_shared.
        j_tns
        |> local_map (fun j => random.uniform rng, j)
        // It's important that here the ordering is being preserved so all the threads get the same value.
        |> local_reduce config (0, limit.max) (fun a b => if snd a < snd b then a else b)
        |> fst |> conv |> (*) max_probability
        
    local_map (fun (x,j) => 
        match x with
        | x, true => j, x - probability >= 0
        | _ => limit.max, false
        ) (zip x j_tns)
    |> local_masked_reduce config limit.max min

inl local_softmax_and_discrete_sampling rng config x i (j_tns : tensor (int * int) int) =
    inl x = local_softmax config x
    x, local_discrete_sampling rng config x i j_tns

inl local_masked_softmax_and_discrete_sampling mask rng config x i (j_tns : tensor (int * int) int) =
    inl x = local_masked_softmax mask config x i j_tns
    x, local_discrete_sampling rng config x i j_tns

inl local_regret_matching_and_discrete_sampling rng config x i (j_tns : tensor (int * int) int) =
    inl x = local_regret_matching config x
    x, local_discrete_sampling rng config x i j_tns

inl local_masked_regret_matching_and_discrete_sampling mask rng config x i (j_tns : tensor (int * int) int) =
    inl x = local_masked_regret_matching mask config x i j_tns
    x, local_discrete_sampling rng config x i j_tns

// Turns the vector into a hash.
inl local_binarize (modulo : uint) config x i (j_tns : tensor (int * int) int) : uint =
    // Shifts have different semantics on the GPU than they do on the CPU. See:
    // https://forums.developer.nvidia.com/t/32bit-shift-on-32bit-integer/188953
    inl x = local_map (fun x,j => (if x <= 0 then 0 else 1) <<< j) (zip x j_tns)
    local_reduce config 0 (|||) x % modulo

// Gets the probability for the selected action.
inl local_get_prob config (action_id : int) (probs : tensor _ (float * int)) =
    local_reduce config (0,limit.max) (fun a b =>
        if snd a = action_id then a
        elif snd b = action_id then b
        else a
        ) probs
    |> fun prob,i => assert (i <> limit.max) "Expected a valid action id in get_prob." . prob

inl test1() =
    inl m,n : int * int = 3, 36*4
    // inl [ma; mb; mc] = listm.map cupy.random_normal{mean=0; std=1} ([swap (m, k) a_trans; swap (k, n) b_trans; m, n])
    inl input : _ _ int = cupy.arange{from=0; nearTo=m,n; by=1}
    // inl input : _ _ int = cupy.ones (m,n)
    inl output_inclusive : _ _ int = cupy.zeros (m,n)
    inl output_exclusive : _ _ int = cupy.zeros (m,n)
    inl output_reduce : _ _ int = cupy.zeros m
    run fun _ =>
        map ((+) 1) input input
        exclusive_scan_2d 0 (+) input output_exclusive
        inclusive_scan_2d 0 (+) input output_inclusive
        reduce_2d 0 (+) input output_reduce
    console.tensor.write_ln 1024 input
    console.tensor.write_ln 1024 output_exclusive
    console.tensor.write_ln 1024 output_inclusive
    console.tensor.write_ln 1024 output_reduce
    ()

inl test3 (n : int) head_dir =
    cupy.set_random_seed(cupy.constant_seed())
    inl m : int = threads_per_block()
    // inl m,n : int * int = threads_per_block(), (1 <<< 8)
    // inl [ma; mb; mc] = listm.map cupy.random_normal{mean=0; std=1} ([swap (m, k) a_trans; swap (k, n) b_trans; m, n])
    inl input_identity : _ _ float = cupy.arange{from=0; nearTo=m,n; by=1}
    // inl input_action : _ _ float = cupy.random_uniform m
    inl input : _ _ float = cupy.random_normal{mean=0; std=1} (m,n)
    inl output_indices_map = tensor_create (m,n)
    inl output_indices_reduce = tensor_create m
    inl output_sample_reduce = tensor_create m
    inl output_identity_map = tensor_create (m,n)
    inl output_op_map : _ _ float = tensor_create (m,n)
    
    open testing
    redirect_io_into "test_text_outputs/primitives/" head_dir "kernel_params.txt" fun () =>
        run fun _ =>
            open random
            inl rng() : _ philox_state = init {seed = cupy.constant_seed(); subsequence=conv rangem.threads_in_grid().from; offset=0}
            inl input = apply (thread_index()) input
            inl input_identity = apply (thread_index()) input_identity
            inl output_op_map = apply (thread_index()) output_op_map
            inl output_indices_map = apply (thread_index()) output_indices_map
            inl output_identity_map = apply (thread_index()) output_identity_map
            row_gather_map () (fun () config x i j_tns => 
                local_map (fun j => i, j) j_tns
                ) input output_indices_map
            row_gather_reduce () (fun () config x i j_tns =>
                i
                ) input |> fun v => tensor_set thread_index() v output_indices_reduce
            row_gather_map () (fun () config x i j_tns => 
                x
                ) input output_identity_map
            row_gather_map () (fun () config x i j_tns => 
                local_masked_regret_matching (fun config x i j => j < 3) config x i j_tns
                ) input output_op_map
            row_gather_reduce rng() (fun rng config x i j_tns =>
                local_masked_softmax_and_discrete_sampling (fun config x i j => j < 3) rng config x i j_tns |> snd
                ) input |> fun v => tensor_set thread_index() v output_sample_reduce

    redirect_io_into "test_text_outputs/primitives/" head_dir "input_identity.txt" fun () =>
        console.tensor.write_ln limit.max input_identity
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_sample_reduce.txt" fun () =>
        console.tensor.write_ln limit.max output_sample_reduce
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_indices_map.txt" fun () =>
        console.tensor.write_ln limit.max output_indices_map
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_indices_reduce.txt" fun () =>
        console.tensor.write_ln limit.max output_indices_reduce
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_op_map.txt" fun () =>
        console.tensor.write_ln limit.max output_op_map
    redirect_io_into "test_text_outputs/primitives/" head_dir "zip_input_output_identity_map.txt" fun () =>
        console.tensor.write_ln limit.max (zip input output_identity_map)
    ()

inl test2 (m,n : int * int) head_dir =
    cupy.set_random_seed(cupy.constant_seed())
    inl input_identity : _ _ int = cupy.arange{from=0; nearTo=m,n; by=1}
    inl input : _ _ float = cupy.random_normal{mean=0; std=1} (m,n)
    inl output_reduce = tensor_create 1
    inl output_identity = tensor_create (m,n)
    inl output_softmax = tensor_create (m,n)
    inl output_masked_softmax = tensor_create (m,n)
    inl output_softmax_scan = tensor_create (m,n)
    inl output_ln = tensor_create (m,n)
    inl output_argmax = tensor_create m
    inl output_indices_map = tensor_create (m,n)
    inl output_indices_reduction = tensor_create m
    inl output_sum_exclusive = tensor_create (m,n)
    inl output_softmax' = tensor_create (m,n)
    inl output_sampling = tensor_create m
    inl output_softmax'' = tensor_create (m,n)
    inl output_sampling' = tensor_create m

    open testing
    redirect_io_into "test_text_outputs/primitives/" head_dir "kernel_params.txt" fun () =>
        run fun _ =>
            open random
            inl rng() : _ philox_state = init {seed = cupy.constant_seed(); subsequence=conv rangem.threads_in_grid().from; offset=0}
            reduce 0 0 (+) input output_reduce
            // indices (identity)
            row_map (fun config x i j_tns => x) input_identity output_identity
            // indices (map)
            row_map (fun config x i j_tns => local_map (fun j => i,j) j_tns) input output_indices_map
            // indices (reduction)
            row_reduce (fun config x i j_tns => i) input output_indices_reduction
            // numerically stable softmax
            row_map (fun config x _ _ => local_softmax config x) input output_softmax
            // layer normalization L2
            row_map (fun config x _ _ => local_ln_l2 config x) input output_ln
            // argument maximum
            row_reduce (fun config x _ j_tns => local_argmax config x j_tns) input output_argmax
            // numerically stable softmax (scan)
            row_map (fun config x _ _ =>
                inl x' = local_softmax config x
                inl x = local_inclusive_scan_sum config x'
                zip x' x
                ) input output_softmax_scan
            // sum (exclusive scan)
            row_map (fun config x _ _ =>
                local_exclusive_scan config 0 (+) x
                ) input_identity output_sum_exclusive
            row_map (local_masked_softmax (fun config x i j => j < 4)) input output_masked_softmax
            // discrete sampling
            row_map_reduce (local_softmax_and_discrete_sampling rng()) input output_softmax' output_sampling
            // masked discrete sampling
            row_map_reduce (local_masked_softmax_and_discrete_sampling (fun config x i j => j < 11) rng()) input output_softmax'' output_sampling'

    redirect_io_into "test_text_outputs/primitives/" head_dir "input.txt" fun () =>
        console.tensor.write_ln 1024 input
    redirect_io_into "test_text_outputs/primitives/" head_dir "input_identity.txt" fun () =>
        console.tensor.write_ln 1024 input_identity
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_reduce.txt" fun () =>
        console.tensor.write_ln 1024 output_reduce
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_softmax.txt" fun () =>
        console.tensor.write_ln 1024 output_softmax
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_masked_softmax.txt" fun () =>
        console.tensor.write_ln 1024 output_masked_softmax
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_ln.txt" fun () =>
        console.tensor.write_ln 1024 output_ln
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_argmax.txt" fun () =>
        console.tensor.write_ln 1024 output_argmax
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_softmax_scan.txt" fun () =>
        console.tensor.write_ln (m * n) output_softmax_scan
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_identity.txt" fun () =>
        console.tensor.write_ln (m * n) output_identity
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_indices_map.txt" fun () =>
        console.tensor.write_ln (m * n) output_indices_map
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_indices_reduction.txt" fun () =>
        console.tensor.write_ln (m * n) output_indices_reduction
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_sum_exclusive.txt" fun () =>
        console.tensor.write_ln (m * n) output_sum_exclusive
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_softmax'.txt" fun () =>
        console.tensor.write_ln 1024 output_softmax'
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_sampling.txt" fun () =>
        console.tensor.write_ln 1024 output_sampling
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_softmax'.txt" fun () =>
        console.tensor.write_ln 1024 output_softmax''
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_sampling.txt" fun () =>
        console.tensor.write_ln 1024 output_sampling'
    ()


inl test4 (m,n : int * int) head_dir =
    cupy.set_random_seed(cupy.constant_seed())
    inl input_identity : _ _ int = cupy.arange{from=0; nearTo=m,n; by=1}
    inl input : _ _ float = cupy.random_normal{mean=0; std=1} (m,n)
    inl output_identity = tensor_create (m,n)
    inl output_softmax = tensor_create (m,n)
    inl output_masked_softmax = tensor_create (m,n)
    inl output_softmax_scan = tensor_create (m,n)
    inl output_ln = tensor_create (m,n)
    inl output_argmax = tensor_create m
    inl output_indices_map = tensor_create (m,n)
    inl output_indices_reduction = tensor_create m
    inl output_sum_exclusive = tensor_create (m,n)
    inl output_softmax' = tensor_create (m,n)
    inl output_sampling = tensor_create m
    inl output_softmax'' = tensor_create (m,n)
    inl output_sampling' = tensor_create m
    open testing
    redirect_io_into "test_text_outputs/primitives/" head_dir "kernel_params.txt" fun () =>
        run fun _ =>
            inl grid = cooperative_groups.create_grid()
            open random
            inl rng() : _ philox_state = init {seed = cupy.constant_seed(); subsequence=conv rangem.threads_in_grid().from; offset=0}
            // indices (identity)
            grid_row_map grid (fun config x i j_tns => x) input_identity output_identity
            // indices (map)
            grid_row_map grid (fun config x i j_tns => local_map (fun j => i,j) j_tns) input output_indices_map
            // indices (reduction)
            grid_row_reduce grid (fun config x i j_tns => i) input output_indices_reduction
            // numerically stable softmax
            grid_row_map grid (fun config x _ _ => local_softmax config x) input output_softmax
            // layer normalization L2
            grid_row_map grid (fun config x _ _ => local_ln_l2 config x) input output_ln
            // argument maximum
            grid_row_reduce grid (fun config x _ j_tns => local_argmax config x j_tns) input output_argmax
            // numerically stable softmax (scan)
            grid_row_map grid (fun config x _ _ =>
                inl x' = local_softmax config x
                inl x = local_inclusive_scan_sum config x'
                zip x' x
                ) input output_softmax_scan
            // sum (exclusive scan)
            grid_row_map grid (fun config x _ _ =>
                local_exclusive_scan config 0 (+) x
                ) input_identity output_sum_exclusive
            grid_row_map grid (local_masked_softmax (fun config x i j => j < 4)) input output_masked_softmax
            // discrete sampling
            grid_row_map_reduce grid (local_softmax_and_discrete_sampling rng()) input output_softmax' output_sampling
            // masked discrete sampling
            grid_row_map_reduce grid (local_masked_softmax_and_discrete_sampling (fun config x i j => j < 11) rng()) input output_softmax'' output_sampling'

    redirect_io_into "test_text_outputs/primitives/" head_dir "input.txt" fun () =>
        console.tensor.write_ln 1024 input
    redirect_io_into "test_text_outputs/primitives/" head_dir "input_identity.txt" fun () =>
        console.tensor.write_ln 1024 input_identity
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_softmax.txt" fun () =>
        console.tensor.write_ln 1024 output_softmax
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_masked_softmax.txt" fun () =>
        console.tensor.write_ln 1024 output_masked_softmax
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_ln.txt" fun () =>
        console.tensor.write_ln 1024 output_ln
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_argmax.txt" fun () =>
        console.tensor.write_ln 1024 output_argmax
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_softmax_scan.txt" fun () =>
        console.tensor.write_ln (m * n) output_softmax_scan
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_identity.txt" fun () =>
        console.tensor.write_ln (m * n) output_identity
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_indices_map.txt" fun () =>
        console.tensor.write_ln (m * n) output_indices_map
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_indices_reduction.txt" fun () =>
        console.tensor.write_ln (m * n) output_indices_reduction
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_sum_exclusive.txt" fun () =>
        console.tensor.write_ln (m * n) output_sum_exclusive
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_softmax'.txt" fun () =>
        console.tensor.write_ln 1024 output_softmax'
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_sampling.txt" fun () =>
        console.tensor.write_ln 1024 output_sampling
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_softmax''.txt" fun () =>
        console.tensor.write_ln 1024 output_softmax''
    redirect_io_into "test_text_outputs/primitives/" head_dir "output_sampling'.txt" fun () =>
        console.tensor.write_ln 1024 output_sampling'
    ()


inl test5 (l,m,n : int * int * int) head_dir =
    open testing
    redirect_io_into "test_text_outputs/primitives/" head_dir "transpose.txt" fun () =>
    inl input_identity : _ _ int = cupy.arange{from=0; nearTo=l,m,n; by=1}
    inl output : _ _ int = tensor_create (l,n,m)
    console.tensor.write_ln limit.max input_identity
    run fun _ =>
        // inl grid = cooperative_groups.create_grid()
        grid_transpose_snd input_identity output
    console.tensor.write_ln limit.max output


inl main() : () = 
    // test2((1 <<< 6), (1 <<< 7)) "test2/a/"
    // test2((1 <<< 7), (1 <<< 6)) "test2/b/"
    // test3((1 <<< 4)) "test3/a"
    // test3((1 <<< 8)) "test3/b"
    // test4((1 <<< 7), (1 <<< 6)) "test4/b/"
    // test4((1 <<< 6), (1 <<< 7)) "test4/a/"
    // test5 (2,4,8) "test5/a"
    // test5 (2,128,256) "test5/b"
    frame_iter (frame_config {iters=7; min=16,16,8; max=1024,1024,1024} : _ (int * int * int) f32)
    |> console.write_ln


    