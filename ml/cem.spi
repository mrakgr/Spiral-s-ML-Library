open corebase
open corecuda
open primitives

type prob = float
type log_prob = f64
type ensemble_id = int
type player_id = int
type action_id = int
type thread_id = int
type seq_id = int
type action_prob = {sampling : prob; policy : prob}
type log_path_prob = {sampling : log_prob; policy : log_prob}
type reward = float
type count = float
type ratio = reward * count

type sizes =
    {
        ensemble_id : ensemble_id
        player_id : player_id
        action_id : action_id
        number_of_valid_actions : action_id
        thread_id : thread_id
    }
type trace_state =
    {
        log_path_probs : tensor (ensemble_id * thread_id * player_id) log_path_prob
    }
type model =
    {
        exploratory_ensemble_id : tensor {} ensemble_id
        rewards : tensor ensemble_id ratio // actual reward is reward / count
    }

inl graph (size : sizes) : _ (model * trace_state) =
    open partitionm

    inl model' : partition model =
        !{} *. !size.ensemble_id
        |> reorder (fun exploratory_ensemble_id,rewards => {exploratory_ensemble_id rewards})

    inl trace_state : partition trace_state =
        !(size.ensemble_id, size.thread_id, size.player_id)
        |> reorder (fun log_path_probs => {log_path_probs})

    inl (<|) a b = layers.pair a b
    open layers
    (key_graph .cem_model (layer weight_id() model' init_zero))
    <| (key_graph .cem_trace_state (layer workspace_id() trace_state init_zero))

// According to CEM papers, adding noise helps performance significantly in games like tetris.
inl cross_entropy_noise() = 0.3

// This is the trick that we talked about in the Ghostlike's Cross Entropy Method video.
// It's supposed to encourage the cross entropy method to gradually reduce the weights towards 0.
// But I changed my mind on having it.
// 
// We're going to be using a curriculum training, so I don't want the weights being pruned prematurely.
inl mean_decay() = 1

type weights = exists dim. tensor (ensemble_id * dim) float * tensor (dim * ensemble_id) float

// Sets the log_path_probs to 0.
inl reset_trace_state (trace_state : trace_state) =
    open tensorm
    open primitives
    inl thread_id = rangem.threads_in_grid().from
    loop.linear (fst trace_state.log_path_probs.dim) fun ensemble_id =>
        inl log_path_probs = trace_state.log_path_probs |> apply ensemble_id |> apply thread_id
        // Sets the log_path_probs to 0.
        (log_path_probs,log_path_probs) ||> row_gather_map () (fun () config x i j_tns => local_map (const loop.zeroes) x)

inl calculate_updates forall dim. 
        ({rewards} : model) 
        (trace_state : trace_state) 
        (reward : sa dim reward) =
    open tensorm
    inl log_path_probs =
        trace_state.log_path_probs
        |> reorder (fun ensemble_id,thread_id,player_id => thread_id,ensemble_id,player_id)
        |> apply rangem.threads_in_grid().from
        |> apply_ptr
    path_probs.integrate_rewards_over_players log_path_probs (path_probs.sa_to_tensor reward)
    |> iteri (fun ensemble_id (reward, path_prob) =>
        inl path_prob = conv path_prob
        tensor_cuda.tensor_atomic_add ensemble_id (reward * path_prob, path_prob) rewards
        )
    reset_trace_state trace_state


inl cem forall dim. rng (transposed_weights : tensor (dim * ensemble_id) float) ({rewards} : model) =
    grid_row_map' (fun config x i j_tns =>
        open tensorm
        inl rewards = 
            local_map (fun ensemble_id => 
                inl reward,count = tensor_index ensemble_id rewards 
                if count <> 0 then reward / count else 0
                ) j_tns
        inl mask =
            inl average = local_average config rewards
            local_map (fun x => x >= average) rewards
        inl winner_count = conv (local_masked_count config mask)
        // TODO: Instead of getting the average, it might be better to take the max of the rewards.
        inl winner_mean = local_masked_sum config mask x / winner_count * mean_decay()
        inl winner_std = // TODO: The filtering will cause the variance to collapse, so research should be done on alternatives to adding noise.
            inl variance = 
                local_map (fun x => inl r = x - winner_mean in r * r) x
                |> local_masked_sum config mask
                |> fun x => x / winner_count
            inl biased_std = sqrt variance
            if winner_count > 1 then biased_std * winner_count / (winner_count - 1) else 0
        local_map (fun x,b => 
            inl r = conv (random.normal rng) * (winner_std + cross_entropy_noise()) + winner_mean
            if b then x else r
            ) (tensorm.zip x mask)
        ) 
        transposed_weights transposed_weights

inl apply_updates rng grid (weights : list weights) (model : model) : () =
    cooperative_groups.sync grid // TODO: Get rid of the grid sync on this line here. It needs to be moved to a better location.

    if rangem.threads_in_grid().from = 0 then // We want to only change the id after all the threads have finished running the game.
        tensorm.tensor_set {} (random.int_range {from=0; nearTo=model.rewards.dim} rng) model.exploratory_ensemble_id // Pick a random ensemble id
    __syncwarp() // Converge the first warp.

    weights |> listm.iter (fun (exists dim. weights, transposed_weights) =>
        grid_transpose weights transposed_weights
        )
    cooperative_groups.sync grid
    weights |> listm.iter (fun (exists dim. _, transposed_weights) =>
        cem rng transposed_weights model
        )
    cooperative_groups.sync grid
    weights |> listm.iter (fun (exists dim. weights, transposed_weights) =>
        grid_transpose transposed_weights weights
        )
    grid_map grid (const loop.zeroes) model.rewards model.rewards // grid map will do a sync

open layers
open tensorm

inl get_action rng model ensemble_id =
    // Extract the output probabilities (already calculated.)
    inl output_probs = (key_extract model .output_probs : tensor d4 float)

    open primitives
    output_probs
    |> apply ensemble_id
    |> apply block_index()
    |> apply thread_index()
    |> row_gather_reduce rng (fun rng config policy_probs i j_tns =>
        inl action_id = local_discrete_sampling rng config policy_probs i j_tns
        local_get_prob config action_id (zip policy_probs j_tns), action_id
        )

// Returns policy probability of an action given the ensemble_id and the action_id.
// If the first argument is true, it does that using the `policy.current` else it uses `policy.average`.
inl get_policy_probs model ensemble_id action_id =
    // Extract the output probabilities (already calculated.)
    inl output_probs = (key_extract model .output_probs : tensor d4 float)
    tensor_index (ensemble_id, block_index(), thread_index(), action_id) output_probs

// Updates the trace state log path probabilities.
inl push_trace (trace_state : trace_state) {thread_id ensemble_id player_id} (action_prob, action_id : action_prob * action_id) : () =
    // Updates the log path probs in the trace_state.
    inl v = {
        sampling = log (conv action_prob.sampling)
        policy = log (conv action_prob.policy)
    }
    // Updates the log path probs in the trace_state.
    tensor_update (ensemble_id, thread_id, player_id) (loop.add v) trace_state.log_path_probs

// Updates the trace state log path probabilities.
inl update_trace_state_path_probs x = push_trace x
