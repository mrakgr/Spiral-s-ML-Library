open corebase
open corecuda
open primitives

type prob = float
type log_prob = f64
type ensemble_id = int
type player_id = int
type action_id = int
type thread_id = int
type seq_id = int
type action_prob = {sampling : prob; policy : prob}
type log_path_prob = {sampling : log_prob; policy : log_prob}
type reward = float
type count = float
type ratio = reward * count

type sizes =
    {
        ensemble_id : ensemble_id
        player_id : player_id
        thread_id : thread_id
    }
type trace_state =
    {
        log_path_probs : tensor (ensemble_id * thread_id * player_id) log_path_prob
    }
type model =
    {
        exploratory_ensemble_id : tensor {} ensemble_id
        rewards : tensor ensemble_id ratio // actual reward is reward / count
    }

inl graph (size : sizes) : _ (trace_state) =
    open partitionm

    inl trace_state : partition trace_state = 
        !(size.ensemble_id, size.thread_id, size.player_id)
        |> reorder (fun log_path_probs => {log_path_probs})
    // Initializes all the elements of the arrays to zero.
    inl init_zero forall t. (x : t) : () = real
        struct.iter (fun x =>
            struct.iter (fun x =>
                $"!(x.array)[:] = 0" : ()
                ) x.bodies
        ) x

    inl (<|) a b = layers.pair a b
    (layers.key_graph .cem_trace_state (layers.weight' trace_state init_zero))

// According to CEM papers, adding noise helps performance significantly in games like tetris.
inl cross_entropy_noise() = 0.1

// This is the trick that we talked about in the Ghostlike's Cross Entropy Method video.
// It's supposed to encourage the cross entropy method to gradually reduce the weights towards 0.
// But I changed my mind on having it.
// 
// We're going to be using a curriculum training, so I don't want the weights being pruned prematurely.
inl mean_decay() = 1

type weights = list (exists dim. tensor (ensemble_id * dim) float * tensor (dim * ensemble_id) float)

inl calculate_updates forall dim. ({rewards} : model) ({log_path_probs} : trace_state) (ensemble_id : ensemble_id) (reward : sa dim reward) =
    open tensorm
    inl log_path_probs =
        log_path_probs
        |> apply ensemble_id
        |> apply thread_index()
        |> apply_ptr
    inl path_prob player_id =
        inl {policy sampling} = tensor_index player_id log_path_probs
        conv (exp (policy - sampling))
    arraym.iteri (fun player_id reward =>
        inl path_prob = path_prob player_id
        tensor_cuda.tensor_atomic_add ensemble_id (reward * path_prob, path_prob) rewards
        ) reward

inl cem forall dim. rng (transposed_weights : tensor (dim * ensemble_id) float) ({rewards} : model) =
    grid_row_map' (fun config x i j_tns =>
        open tensorm
        inl rewards = local_map (fun ensemble_id => inl reward,count = tensor_index ensemble_id rewards in reward / count) j_tns
        inl mask =
            inl average = local_average config rewards
            local_map (fun x => x >= average) rewards
        inl winner_count = conv (local_masked_count config mask)
        // TODO: Instead of getting the average, it might be better to take the argmax of the rewards.
        inl winner_mean = local_masked_sum config mask x / winner_count * mean_decay()
        inl winner_std = // TODO: The filtering will cause the variance to collapse, so research should be done on alternatives to adding noise.
            inl variance = 
                local_map (fun x => inl r = x - winner_mean in r * r) x
                |> local_masked_sum config mask
                |> fun x => x / winner_count
            inl biased_std = sqrt variance
            if winner_count > 1 then biased_std * winner_count / (winner_count - 1) else 0
        local_map (fun x,b => 
            inl r = conv (random.normal rng) * (winner_std + cross_entropy_noise()) + winner_mean
            if b then x else r
            ) (tensorm.zip x mask)
        ) 
        transposed_weights transposed_weights

inl apply_updates rng grid (weights : weights) model : () =
    weights |> listm.iter (fun (exists dim. weights, transposed_weights) =>
        grid_transpose weights transposed_weights
        )
    cooperative_groups.sync grid
    weights |> listm.iter (fun (exists dim. _, transposed_weights) =>
        cem rng transposed_weights model
        )
    cooperative_groups.sync grid
    weights |> listm.iter (fun (exists dim. weights, transposed_weights) =>
        grid_transpose transposed_weights weights
        )
    cooperative_groups.sync grid

open layers
open tensorm

// Sets the log_path_probs to 0.
inl reset_trace_state (trace_state : trace_state) (ensemble_id : ensemble_id) =
    open primitives
    inl thread_id = rangem.threads_in_grid().from
    inl log_path_probs = trace_state.log_path_probs |> apply ensemble_id |> apply thread_id |> apply_ptr
    // Sets the log_path_probs to 0.
    (log_path_probs,log_path_probs) ||> row_gather_map () (fun () config x i j_tns => local_map (const loop.zeroes) x)

inl get_action rng model ensemble_id =
    // Extract the output probabilities (already calculated.)
    inl output_probs = (key_extract model .output_probs : tensor d4 float)

    open primitives
    output_probs
    |> apply ensemble_id
    |> apply block_index()
    |> apply thread_index()
    |> row_gather_reduce rng (fun rng config policy_probs i j_tns =>
        inl action_id = local_discrete_sampling rng config policy_probs i j_tns
        local_get_prob config action_id (zip policy_probs j_tns), action_id
        )

// Returns policy probability of an action given the ensemble_id and the action_id.
// If the first argument is true, it does that using the `policy.current` else it uses `policy.average`.
inl get_policy_probs model ensemble_id action_id =
    // Extract the output probabilities (already calculated.)
    inl output_probs = (key_extract model .output_probs : tensor d4 float)
    tensor_index (ensemble_id, block_index(), thread_index(), action_id) output_probs

// Updates the trace state log path probabilities.
inl push_trace (trace_state : trace_state) {thread_id ensemble_id player_id} (action_prob, action_id : action_prob * action_id) : () =
    // Updates the log path probs in the trace_state.
    inl v = {
        sampling = log (conv action_prob.sampling)
        policy = log (conv action_prob.policy)
    }
    // Updates the log path probs in the trace_state.
    tensor_update (ensemble_id, thread_id, player_id) (loop.add v) trace_state.log_path_probs

// Updates the trace state log path probabilities.
inl update_trace_state_path_probs x = push_trace x
