open corebase
open tensorm

type ensemble_id = int
type player_id = int
type reward = float
type prob = float
type log_prob = f64
type log_path_prob = {sampling : log_prob; policy : log_prob}

inl from_log_path_prob ({policy sampling} : log_path_prob) = exp (policy - sampling)

// Integrates the rewards given the path probabilities.
inl integrate_rewards
        (log_path_probs : tensor (ensemble_id * player_id) log_path_prob) 
        (rewards : tensor player_id reward)
        : tensor ensemble_id reward =
    inl ensemble, player = log_path_probs.dim
    inl individual_prob (ensemble_id, player_id) = from_log_path_prob(tensor_index (ensemble_id, player_id) log_path_probs)
    inl ensemble_probs =
        init ensemble fun ensemble_id =>
            loop.for {from = 0; nearTo=player} (fun player_id s =>
                s + individual_prob (ensemble_id, player_id)
                ) 0
    inl ensemble_prob ensemble_id = tensor_index ensemble_id ensemble_probs
    inl total_prob = fold (*) 1 ensemble_probs
    inl reward player_id = tensor_index player_id rewards
    init ensemble fun ensemble_id =>
        loop.for {from = 0; nearTo=player} (fun player_id s =>
            inl path_prob = total_prob / ensemble_prob ensemble_id * individual_prob (ensemble_id, player_id)
            s + conv path_prob * reward player_id
            ) 0
