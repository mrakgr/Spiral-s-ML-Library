open corebase
open corecuda
open tensorm
open compile_time

type ptr = partitionm.ptr
type size = partitionm.size
type graph_array = ptr * size
type graph_dims = {body : .hashmap; head : .hashmap}
type graph_offset = .hashmap * size
type graph_data = {array : graph_array; offset : graph_offset}
type dim = int * int * int

union rec graph t =
    | Map : exists a. (a -> t) * graph a
    | Map2 : exists a b. (a -> b -> t) * graph a * graph b
    | Matmul : graph t * graph t
    | Input : exists key{symbol}. key
    | Weight : int // Takes in the output dimension as the ergument.

inl memoize (h : .hashmap) f k =
    match hashmap.try_get h k with
    | Some v => v
    | None => inl v = f k in hashmap.add h k v . v

// TODO: Make a check to make sure that there aren't input nodes with the same key, but different
// dimensions or type.

// Propagates the dimensions for each of the nodes and returns a hash map of them pointing to each node.
// Takes in a record of input dimensions and the ensemble size as the last arguments.
inl pass_dim forall t_top dims. (x : graph t_top) (dims : dims) ensemble_size : graph_dims =
    inl h_body = hashmap.create()
    inl h_head = hashmap.create()
    inl get_dim forall k{symbol}. (k : k) : dim = real dims k : dim
    inl rec f forall t. (dim : option dim) : graph t -> dim =
        memoize h_body function
            | Weight n => 
                match dim with
                | Some (_, m, k) => ensemble_size, n, k
                | None => error_type "Cannot infer the dimensions of the weight layer. The inference procedure passes the information left to right through the graph."
            | Input (exists key. k) => get_dim k
            | Map(exists a. _, a) => f dim a
            | Map2(exists a b. _,a,b) =>
                inl a = f dim a
                inl b = f (Some a) b
                assert (a = b) "The dimensions of the two inputs to the Map2 node have to be equal."
                a
            | Matmul(a,b) =>
                inl (o,m,k as a) = f dim a
                inl (_,n,k') = f (Some a) b
                assert (k = k') "The inner dimensions of the matrix multiplication node have to be equal."
                o,m,n
    inl dim = f None x
    assert (hashmap.remove h_body x) "Has to work."
    hashmap.add h_head x (ensemble_size, dim)
    hashmap.set_immutable(h_body)
    hashmap.set_immutable(h_head)
    {body=h_body; head=h_head}

// Calculates the parameter offsets.
inl pass_offset_param forall t_top. (x : graph t_top) (dims : graph_dims) : graph_offset =
    inl h_offset_param = hashmap.create() // stores the offsets pointing to the start of the weight parameter array for each node
    inl h = hashmap.create()
    inl get_dim forall k. (k : k) : dim = match hashmap.try_get dims.body k with Some v => v | None => error_type "Cannot get the node dimension from the dictionary."
    inl rec f forall t. (offset : size) : graph t -> size =
        memoize h function
            | Weight as k =>
                inl partition_offsets : _ (_ _ t) = 
                    partitionm.to_partition(get_dim k)
                    |> partitionm.calculate_offsets offset 
                hashmap.add h_offset_param k partition_offsets
                partition_offsets.offset_end
            | Input => offset
            | Map(exists a. _,a) => f offset a
            | Map2(exists a b. _,a,b) => f (f offset a) b
            | Matmul(a,b) => f (f offset a) b
    inl offset = f 0 x
    hashmap.set_immutable(h_offset_param)
    h_offset_param, offset

inl pass_offset_output forall t_top. (x : graph t_top) (dims : graph_dims) : graph_offset =
    inl h_offset_output = hashmap.create() // stores the offsets pointing to the start of the output array for each node
    inl h = hashmap.create()
    inl get_dim forall k. (k : k) : dim = match hashmap.try_get dims.body k with Some v => v | None => error_type "Cannot get the node dimension from the dictionary."
    inl rec f forall t. (offset : size) : graph t -> size =
        memoize h fun k =>
            inl g (offset : size) =
                inl partition_offsets : _ (_ _ t) = 
                    partitionm.to_partition(get_dim k)
                    |> partitionm.calculate_offsets offset 
                hashmap.add h_offset_output k partition_offsets
                partition_offsets.offset_end
            match k with
            | Weight => offset
            | Input => g offset
            | Map(exists a. _,a) => g (f offset a)
            | Map2(exists a b. _,a,b) => g (f (f offset a) b)
            | Matmul(a,b) => g (f (f offset a) b)
    inl offset = f 0 x
    hashmap.set_immutable(h_offset_output)
    h_offset_output, offset

inl param_init forall t_top. (x : graph t_top) (graph : graph_data) =
    assert (snd graph.array = snd graph.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."
    inl rec f forall t. (x : graph t) : () =
        match x with
        | Weight =>
            inl x : tensor dim t = partitionm.from_partition_offsets graph.array (hashmap.get (fst graph.offset) x)
            cupy.copy_to {from=cupy.random_normal{mean=0; std=1} x.dim; to=x}
        | Input => ()
        | Map(exists a. _,a) => f a
        | Map2(exists a b. _,a,b) => f a . f b
        | Matmul(a,b) => f a . f b
    f x

inl param_print forall t_top. (x : graph t_top) (graph : graph_data) =
    assert (snd graph.array = snd graph.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."
    inl rec f forall t. (x : graph t) : () =
        match x with
        | Weight =>
            inl x : tensor dim t = partitionm.from_partition_offsets graph.array (hashmap.get (fst graph.offset) x)
            console.write_ln x
        | Input => ()
        | Map(exists a. _,a) => f a
        | Map2(exists a b. _,a,b) => f a . f b
        | Matmul(a,b) => f a . f b
    f x

inl create_graph_data (offset : graph_offset) : graph_data =
    inl array = partitionm.create_array (snd offset)
    {array offset}

// Tests whether randonmly initializing the params of the graph works.
inl test1() =
    inl blocks = blocks_per_grid()
    inl ensemble_size : int = 4 // The number of weight layers in the ensemble
    inl tanh forall t{float}. (x : graph t) : graph t = Map (exists tanh, x)
    inl matmul dim x = Matmul(x, Weight dim)
    inl input forall key{symbol}. (key : key) = Input (exists key)

    inl graph : graph float =
        input .input
        |> matmul 4
        |> tanh
        |> matmul 4
        |> tanh
        |> matmul 2
        |> tanh
    
    inl dims = pass_dim graph {input = blocks, 1, 1 : dim} ensemble_size
    inl param = create_graph_data (pass_offset_param graph dims)
    console.write_ln "---"
    param_print graph param
    param_init graph param
    console.write_ln "Done initing."
    param_print graph param
    ()

inl input_extract forall float key{symbol} input. (x : graph float) (input : graph_data) (_ : key) : tensor dim input =
    assert (snd input.array = snd input.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."
    inl rec f forall t. (x : graph t) : option (tensor dim input) =
        match x with
        | Weight _ => None
        | Input (exists key'. _) =>
            if (real open real_core in `key `= `key') then // `= operator compares the two types for equality.
                inl x : tensor dim input = partitionm.from_partition_offsets input.array (hashmap.get (fst input.offset) x)
                Some x
            else None
        | Map(exists a. _,a) => f a
        | Map2(exists a b. _,a,b) => 
            match f a with
            | None => f b
            | a => a
        | Matmul(a,b) =>
            match f a with
            | None => f b
            | a => a
    match f x with
    | Some x => x
    | None => error_type "Cannot find the input tensor with the given key."

// Tests whether extracting the inputs of the graph works. Also randomly inits them.
inl test2() =
    inl blocks = blocks_per_grid()
    inl ensemble_size : int = 4 // The number of weight layers in the ensemble
    inl tanh forall t{float}. (x : graph t) : graph t = Map (exists tanh, x)
    inl matmul dim x = Matmul(x, Weight dim)
    inl input forall key{symbol}. (key : key) = Input (exists key)

    inl graph : graph float =
        input .input
        |> matmul 4
        |> tanh
        |> matmul 4
        |> tanh
        |> matmul 2
        |> tanh

    inl dims' = {input = blocks, 1, 2 : dim}
    inl dims = pass_dim graph dims' ensemble_size
    inl param = create_graph_data (pass_offset_param graph dims)
    inl input = create_graph_data (pass_offset_output graph dims)
    param_init graph param
    console.write_ln "Here are the weight matrices."
    param_print graph param
    console.write_ln "Here is the input tensor."
    inl input : _ _ float = input_extract graph input .input
    cupy.copy_to {
        from = cupy.random_normal{mean=0; std=1} dims'.input
        to = input
    }
    console.write_ln input
    ()

inl graph_tensor_extract forall t. (x : graph t) (param : graph_data) (output : graph_data) block_id ensemble_id : tensor (int * int) t =
    match hashmap.try_get (fst param.offset) x with
    | Some (x : tensor dim t) => apply ensemble_id x
    | None =>
        match hashmap.try_get (fst param.offset) x with
        | Some (x : tensor dim t) => apply block_id x
        | None => ...0
    // match x with
    // | Weight => partitionm.from_partition_offsets param.array (hashmap.get (fst param.offset) x)
    // | _ => partitionm.from_partition_offsets output.array (hashmap.get (fst output.offset) x)

// Calculated the max of dynamic shared memory used for all the nodes.
inl pass_shared_memory forall t_top. (x : graph t_top) =
    inl rec f forall t. (x : graph t) : size =
        match x with
        | Weight | Input => 0
        | Map(exists a. g,a) => f a
        | Map2(exists a b. g,a,b) => max (f a) (f b)
        | Matmul(a,b) => 
            inl memory : size =
                inl tf32 () : size = fst(matmul.matmul_tf32' false true).offset_end
                real
                    typecase t with
                    | f32 => tf32()
                    | _ => error_type "The type is not supported in the matrix multiply node. Only f32 is supported at the moment."
            max (f a) (f b) |> max memory
    f x

inl graph_run forall t_top. (x : graph t_top) (param : graph_data) (output : graph_data) ensemble_id =
    assert (snd param.array = snd param.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."
    assert (snd output.array = snd output.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."

    inl dynamic_shared_memory_used = pass_shared_memory x
    run' {shared_mem=conv dynamic_shared_memory_used} fun () => 
        loop.projective rangem.blocks_in_grid(blocks_per_grid()) fun block_id =>
            inl tensor_extract x = graph_tensor_extract x param output block_id ensemble_id
            inl rec f forall t. (x : graph t) : () =
                open primitives
                match x with
                | Weight => ()
                | Input => ()
                | Map(exists a. g,a) =>
                    f a
                    join 
                        inl a,out = tensor_extract a, tensor_extract x
                        join
                            map g a out
                | Map2(exists a b. g,a,b) =>
                    f a . f b
                    join 
                        inl a,b,out = tensor_extract a, tensor_extract b, tensor_extract x
                        join
                            map (fun a,b => g a b) (zip a b) out
                | Matmul(a,b) =>
                    f a . f b
                    inl alpha, beta : float * float = 1, 0
                    join
                        inl a,b,out = tensor_extract a, tensor_extract b, tensor_extract x
                        real
                            open real_core
                            typecase t with
                            | f32 => 
                                inl memory, body = matmul.matmul_tf32' false true
                                assert (memory.offset_end <= dynamic_shared_memory_used) "The shared memory used in the matmult node is lower than the allocated amount."
                                join body alpha a b beta out
                            | _ => error_type "The type is not supported in the matrix multiply node. Only f32 is supported at the moment."
            
            f x

// Tests whether extracting the graph can be executed.
inl test3() =
    inl blocks = blocks_per_grid()
    inl ensemble : int = 4 // The number of weight layers in the ensemble
    inl tanh forall t{float}. (x : graph t) : graph t = Map (exists tanh, x)
    inl matmul dim x = Matmul(x, Weight {dim=ensemble,dim})
    inl input forall key{symbol}. (key : key) = Input (exists key)

    inl graph : graph float =
        input .input
        |> matmul (8, 16)
        |> tanh
        |> matmul (16, 16)
        |> tanh
        |> matmul (16, 16)
        |> tanh

    inl dims' = {input = blocks, 16, 8 : dim}
    inl dims = pass_dim graph dims'
    inl param = create_graph_data (pass_offset_param graph dims)
    inl input = create_graph_data (pass_offset_output graph dims)
    param_init graph param
    console.write_ln "Here are the weight matrices."
    param_print graph param
    inl tns_input : _ _ float = input_extract graph input .input
    // Immitates the passing of data into the output vector.
    // In the poker game the serialization function will be responsible for this.
    cupy.copy_to {
        from = cupy.random_normal{mean=0; std=1} dims'.input
        to = tns_input
    }
    console.write_ln tns_input
    console.write_ln "Here is the output tensor."
    graph_run graph param input 0
    inl tns_output : _ _ float = graph_tensor_extract graph param input
    console.write_ln tns_output
    ()

inl main() = test3()