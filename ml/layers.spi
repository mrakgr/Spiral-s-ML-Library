open corebase
open corecuda
open tensorm
open compile_time

type dim = int * int * int

union dims =
    | Dims_Body : dim
    | Dims_Head : int * dim

union po t =
    | Po_Body : partitionm.partition_offsets (tensor dim t)
    | Po_Head : partitionm.partition_offsets (tensor (int * dim) t)

nominal layer_state = 
    {
        rng : refm.ref random.philox_state
    }

union rec graph t =
    | Map : exists a. (layer_state -> a -> t) * graph a
    | RowMap : exists a. (layer_state -> primitives.row_config -> tensor (int * int) a -> int -> tensor (int * int) int -> tensor (int * int) t) * graph a
    | RowReduce : exists a. (layer_state -> primitives.row_config -> tensor (int * int) a -> int -> tensor (int * int) int -> t) * graph a
    | Zip : exists a b. (a * b -> t) * graph a * graph b
    | Matmul : graph t * graph t
    | Input : exists key{symbol}. key * int // Takes in the innermost dim as an argument.
    | Weight : int // Takes in the output dimension as the ergument.

type ptr = partitionm.ptr
type size = partitionm.size
type graph_array = ptr * size
type graph_dims = hashmapm.hashmap
type graph_offset = hashmapm.hashmap * size
type graph_data = {array : graph_array; offset : graph_offset}
type model_sizes =
    {
        ensemble : int
        block : int
        minibatch : int
    }
nominal model t =
    {
        graph : graph t
        dims : graph_dims
        output : graph_data
        param : graph_data
        size : model_sizes
    }

inl memoize (h : hashmapm.hashmap) f k =
    match hashmapm.try_get h k with
    | Some v => v
    | None => inl v = f k in hashmapm.add h k v . v

// TODO: Make a check to make sure that there aren't input nodes with the same key, but different
// dimensions or type.

// Propagates the dimensions for each of the nodes and returns a hash map of them pointing to each node.
// Takes in a record of input dimensions and the ensemble size as the last arguments.
inl pass_dim forall t_top. (x : graph t_top) (size : model_sizes) : graph_dims =
    inl h_dims = hashmapm.create()
    inl h = hashmapm.create()
    inl rec f forall t. (dim : option dim) : graph t -> dim =
        inl body = function
            | Weight n => 
                match dim with
                | Some (_, m, k) => size.ensemble, n, k
                | None => error_type "Cannot infer the dimensions of the weight layer. The inference procedure passes the information left to right through the graph."
            | Input (exists key. k, dim) => size.block, size.minibatch, dim
            | Map(exists a. _, a)
            | RowMap(exists a. _, a) => f dim a
            | RowReduce(exists a. _, a) => 
                inl c,b,a = f dim a
                c,b,1
            | Zip(exists a b. _,a,b) =>
                inl a = f dim a
                inl b = f (Some a) b
                assert (a = b) "The dimensions of the two inputs to the Zip node have to be equal."
                a
            | Matmul(a,b) =>
                inl (o,m,k as a) = f dim a
                inl (_,n,k') = f (Some a) b
                assert (k = k') "The inner dimensions of the matrix multiplication node have to be equal."
                o,m,n
        memoize h (fun k => body k |> fun dim => hashmapm.add h_dims k (Dims_Body dim) . dim)
    inl dim = f None x
    assert (hashmapm.remove h_dims x) "Has to work."
    hashmapm.add h_dims x (Dims_Head (size.ensemble, dim))
    hashmapm.set_immutable(h_dims)
    h_dims

// Calculates the parameter offsets.
inl pass_offset_param forall t_top. (x : graph t_top) (dims : graph_dims) : graph_offset =
    inl h_offset_param = hashmapm.create() // stores the offsets pointing to the start of the weight parameter array for each node
    inl h = hashmapm.create()
    inl get_dim forall k. (k : k) : dim = 
        match hashmapm.try_get dims k with 
        | Some (Dims_Body v) => v 
        | Some (Dims_Head v) => error_type "The weight layer cannot be in the top level."
        | None => error_type "Cannot get the node dimension from the dictionary."
    inl rec f forall t. (offset : size) : graph t -> size =
        memoize h function
            | Input => offset
            | Weight as k =>
                inl partition_offsets : _ (_ _ t) = 
                    partitionm.to_partition(get_dim k)
                    |> partitionm.calculate_offsets offset 
                hashmapm.add h_offset_param k partition_offsets
                partition_offsets.offset_end
            | Map(exists a. _,a)
            | RowMap(exists a. _,a)
            | RowReduce(exists a. _,a) => f offset a
            | Zip(exists a b. _,a,b) => f (f offset a) b
            | Matmul(a,b) => f (f offset a) b
    inl offset = f 0 x
    hashmapm.set_immutable(h_offset_param)
    h_offset_param, offset

inl pass_offset_output forall t_top. (x : graph t_top) (dims : graph_dims) : graph_offset =
    inl h_offset_output = hashmapm.create() // stores the offsets pointing to the start of the output array for each node
    inl h = hashmapm.create()
    inl rec f forall t. (offset : size) : graph t -> size =
        memoize h fun k =>
            inl g (offset : size) =
                match hashmapm.try_get dims k with 
                | Some (Dims_Body dim) => 
                    inl partition_offsets : _ (_ _ t) = partitionm.to_partition dim |> partitionm.calculate_offsets offset
                    hashmapm.add h_offset_output k (Po_Body partition_offsets)
                    partition_offsets.offset_end
                | Some (Dims_Head dim) => 
                    inl partition_offsets : _ (_ _ t) = partitionm.to_partition dim |> partitionm.calculate_offsets offset
                    hashmapm.add h_offset_output k (Po_Head partition_offsets)
                    partition_offsets.offset_end
                | None => error_type "Cannot get the node dimension from the dictionary."
            match k with
            | Weight => offset
            | Zip(exists a b. zipper,a,b) => f (f offset a) b // Zip is handled in a passthrough fashion. There is a special case in graph_tensor_extract to handle it.
            | Input => g offset
            | Map(exists a. _,a)
            | RowMap(exists a. _,a)
            | RowReduce(exists a. _,a) => g (f offset a)
            | Matmul(a,b) => g (f (f offset a) b)
    inl offset = f 0 x
    hashmapm.set_immutable(h_offset_output)
    h_offset_output, offset

inl param_init forall t_top. (model {graph param} : model t_top) =
    assert (snd param.array = snd param.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."
    inl h = hashmapm.create()
    inl rec f forall t. : graph t -> () =
        memoize h function
            | Weight as x =>
                inl x : tensor dim t = partitionm.from_partition_offsets param.array (hashmapm.get (fst param.offset) x)
                cupy.copy_to {from=cupy.random_normal{mean=0; std=1} x.dim; to=x} // TODO: Don't forget to init the layers properly.
            | Input => ()
            | Map(exists a. _,a) 
            | RowMap(exists a. _,a)
            | RowReduce(exists a. _,a) => f a
            | Zip(exists a b. _,a,b) => f a . f b
            | Matmul(a,b) => f a . f b
    f graph

inl param_print forall t_top. (model {graph param} : model t_top) =
    assert (snd param.array = snd param.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."
    inl h = hashmapm.create()
    inl rec f forall t. : graph t -> () =
        memoize h function
            | Weight as x =>
                inl x : tensor dim t = partitionm.from_partition_offsets param.array (hashmapm.get (fst param.offset) x)
                console.write_ln x
            | Input => ()
            | Map(exists a. _,a)
            | RowMap(exists a. _,a)
            | RowReduce(exists a. _,a) => f a
            | Zip(exists a b. _,a,b) => f a . f b
            | Matmul(a,b) => f a . f b
    f graph

inl create_graph_data (offset : graph_offset) : graph_data =
    inl array = partitionm.create_array (snd offset)
    {array offset}

inl input_extract forall t_top key{symbol} input. (model {graph output} : model t_top) (_ : key) : tensor dim input =
    assert (snd output.array = snd output.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."
    inl h = hashmapm.create()
    inl rec f forall t. : graph t -> option (tensor dim input) =
        memoize h function
            | Weight _ => None
            | Input (exists key'. _) as x =>
                if (real open real_core in `key `= `key') then // `= operator compares the two types for equality.
                    match hashmapm.get (fst output.offset) x with
                    | Po_Body v => Some (partitionm.from_partition_offsets output.array v)
                    | Po_Head => error_type "The input layer shouldn't be in last position."
                else None
            | Map(exists a. _,a)
            | RowMap(exists a. _,a)
            | RowReduce(exists a. _,a) => f a
            | Zip(exists a b. _,a,b) => 
                match f a with
                | None => f b
                | a => a
            | Matmul(a,b) =>
                match f a with
                | None => f b
                | a => a
    match f graph with
    | Some x => x
    | None => error_type "Cannot find the input tensor with the given key."

inl graph_tensor_extract forall t_top. (model {graph param output} : model t_top) ({ensemble_id block_id} : {ensemble_id : int; block_id : int}) : tensor (int * int) t_top =
    inl rec loop forall t. (x : graph t) : tensor (int * int) t =
        match x with
        | Zip(exists a b. f,a,b) => zip (loop a) (loop b) |> rezip f
        | _ =>
            match hashmapm.try_get (fst param.offset) x with
            | Some x => partitionm.from_partition_offsets param.array x |> apply ensemble_id
            | None =>
                match hashmapm.try_get (fst output.offset) x with
                | Some (Po_Body x) => partitionm.from_partition_offsets output.array x |> apply block_id
                | Some (Po_Head x) => partitionm.from_partition_offsets output.array x |> apply ensemble_id |> apply block_id
                | None => error_type "Cannot find the offset in the dictionaries"
    loop graph

// Calculates the max of dynamic shared memory used for all the nodes.
inl pass_calculate_dynamic_shared_memory forall t_top. (x : graph t_top) =
    inl h = hashmapm.create()
    inl rec f forall t. : graph t -> size =
        memoize h function
            | Weight | Input => 0
            | Map(exists a. _,a)
            | RowMap(exists a. _,a)
            | RowReduce(exists a. _,a) => f a
            | Zip(exists a b. _,a,b) => max (f a) (f b)
            | Matmul(a,b) => 
                inl memory : size =
                    inl tf32 () : size = fst(matmul.matmul_tf32' false true).offset_end
                    real
                        typecase t with
                        | f32 => tf32()
                        | _ => error_type "The type is not supported in the matrix multiply node. Only f32 is supported at the moment."
                max (f a) (f b) |> max memory
    f x

inl graph_run_device forall t_top. (model ({graph param output} & m) : model t_top) {ensemble_id} =
    ()

inl graph_run_host forall t_top. (model ({graph param output} & m) : model t_top) {ensemble_id} =
    assert (snd param.array = snd param.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."
    assert (snd output.array = snd output.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."

    inl dynamic_shared_memory_used = pass_calculate_dynamic_shared_memory graph
    run' {shared_mem=conv dynamic_shared_memory_used} fun () =>
        inl h = hashmapm.create()
        inl ls : layer_state =
            open random
            layer_state {
                rng = init {seed = conv rangem.threads_in_grid().from; subsequence = conv ensemble_id; offset=0}
            }

        loop.projective rangem.blocks_in_grid(blocks_per_grid()) fun block_id =>
            inl tensor_extract graph = graph_tensor_extract (model {m with graph}) {ensemble_id block_id}
            inl rec f forall t. : graph t -> () =
                open primitives
                memoize h fun x =>
                    match x with
                    | Weight => ()
                    | Input => ()
                    | Map(exists a. g,a) =>
                        f a
                        inl g = g ls
                        join 
                            inl a,out = tensor_extract a, tensor_extract x
                            join map g a out
                    | RowMap(exists a. g,a) =>
                        f a
                        inl g = g ls
                        join
                            inl a,out = tensor_extract a, tensor_extract x
                            join row_map g a out
                    | RowReduce(exists a. g,a) =>
                        f a
                        inl g = g ls
                        join
                            inl a,out = tensor_extract a, flatten (tensor_extract x)
                            join row_reduce g a out
                    | Zip(exists a b. g,a,b) => f a . f b
                    | Matmul(a,b) =>
                        f a . f b
                        inl alpha, beta : float * float = 1, 0
                        join
                            inl a,b,out = tensor_extract a, tensor_extract b, tensor_extract x
                            real
                                open real_core
                                typecase t with
                                | f32 => 
                                    inl memory, body = matmul.matmul_tf32' false true
                                    assert (memory.offset_end <= dynamic_shared_memory_used) "The shared memory used in the matmult node is lower than the allocated amount."
                                    join body alpha a b beta out
                                | _ => error_type "The type is not supported in the matrix multiply node. Only f32 is supported at the moment."
            
            f graph

inl tanh forall t{float}. (x : graph t) : graph t = Map (exists const tanh, x)
inl sigmoid forall t{float; number}. (x : graph t) : graph t = Map (exists const sigmoid, x)
inl relu forall t{float; number}. (x : graph t) : graph t = Map (exists const (max 0), x)
inl softmax forall t{float; number}. (x : graph t) : graph t = RowMap (exists (fun _ config x _ _ => primitives.local_softmax config x), x)
inl ln_l2 forall t{float; number}. (x : graph t) : graph t = RowMap (exists (fun _ config x _ _ => primitives.local_ln_l2 config x), x)
inl argmax forall t{number}. (x : graph t) : graph int = RowReduce (exists (fun _ config x _ j_tns => primitives.local_argmax config x j_tns), x)
inl discrete_sample forall t{float; number}. (x : graph t) : graph int = 
    inl f (layer_state {rng}) config x i j_tns = primitives.local_discrete_sampling rng config x i j_tns
    RowReduce (exists f, x)
inl matmul dim x = Matmul(x, Weight dim)
inl input forall key{symbol}. (key : key) (dim : int) = Input (exists key, dim)

inl create_model forall t. (graph : graph t) (size : model_sizes) =
    inl dims = pass_dim graph size
    inl param = create_graph_data (pass_offset_param graph dims)
    inl output = create_graph_data (pass_offset_output graph dims)
    model {graph param output dims size}

// Tests whether randonmly initializing the params of the graph works.
inl test1() =
    inl size = {
        ensemble = 4 // The third dimension of each weight layer.
        block = blocks_per_grid()
        minibatch = 1
    }

    inl graph : graph float =
        (input .input 1 : _ float)
        |> matmul 4
        |> tanh
        |> matmul 4
        |> tanh
        |> matmul 2
        |> tanh

    inl model = create_model graph size 
    
    console.write_ln "---"
    param_print model
    param_init model
    console.write_ln "Done initing."
    param_print model
    ()

// Tests whether extracting the inputs of the graph works. Also randomly inits them.
inl test2() =
    inl size = {
        ensemble = 4 // The third dimension of each weight layer.
        block = blocks_per_grid()
        minibatch = 1
    }

    inl graph : graph float =
        (input .input 2 : _ float)
        |> matmul 4
        |> tanh
        |> matmul 4
        |> tanh
        |> matmul 2
        |> tanh

    inl model = create_model graph size
    console.write_ln "---"
    param_init model
    console.write_ln "Here are the weight matrices."
    param_print model
    console.write_ln "Here is the input tensor."
    inl input : _ _ float = input_extract model .input
    cupy.copy_to {
        from = cupy.random_normal{mean=0; std=1} input.dim
        to = input
    }
    console.write_ln input
    ()

// Tests whether extracting the graph can be executed.
inl test3() =
    inl size = {
        ensemble = 4 // The third dimension of each weight layer.
        block = blocks_per_grid()
        minibatch = 16
    }

    inl graph =
        (input .input 8 : _ float)
        |> matmul 16
        |> ln_l2
        |> relu
        |> matmul 16
        |> ln_l2
        |> relu
        |> matmul 16
        |> discrete_sample

    inl model = create_model graph size
    param_init model
    console.write_ln "Here are the weight matrices."
    param_print model
    inl tns_input : _ _ float = input_extract model .input
    // Immitates the passing of data into the output vector.
    // In the poker game the serialization function will be responsible for this.
    cupy.copy_to {
        from = cupy.random_normal{mean=0; std=1} tns_input.dim
        to = tns_input
    }
    console.write_ln tns_input
    console.write_ln "Here is the output tensor."
    loop.linear size.ensemble fun ensemble_id =>
        graph_run_host model {ensemble_id}
        inl tns_output = graph_tensor_extract model {ensemble_id block_id=0}
        console.write_ln tns_output
        console.write_ln "==="
    ()

inl main() = test3()