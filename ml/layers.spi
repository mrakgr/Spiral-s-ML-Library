open corebase
open corecuda
open coreext
open tensorm

type space_id = int
inl weight_id() : space_id = 0
// For input layers and intermediates.
inl workspace_id() : space_id = 1
inl weight_adjoint_id() : space_id = 2
type num_spaces = 3
inl num_spaces() : space_id = real real_core.type_lit_to_lit `num_spaces

type graph_dims = compile_time.hashmap
type graph_partitions = list {partition : compile_time.hashmap; size : size}
type graph_data_elem = {array : ptr; partition : compile_time.hashmap; size : size}
type graph_data = list graph_data_elem

nominal layer_state = 
    {
        rng : ref random.philox_state
    }

union rec graph t =
    | BlockMap :: forall dim t. 
        (exists a. (layer_state -> a -> t) 
            * graph (tensor (int * dim) a) 
            * option (graph (tensor (int * dim) t))) 
        -> graph (tensor (int * dim) t)
    | BlockRowMap :: forall dim t.
        (exists a. (layer_state -> primitives.row_config -> tensor (int * int) a -> dim -> tensor (int * int) int -> tensor (int * int) t) 
            * graph (tensor (int * dim * int) a)
            * option (graph (tensor (int * dim * int) t)))
        -> graph (tensor (int * dim * int) t)
    | BlockRowReduce :: forall dim t.
        (exists a. (layer_state -> primitives.row_config -> tensor (int * int) a -> dim -> tensor (int * int) int -> t) 
            * graph (tensor (int * dim * int) a)
            * option (graph (tensor (int * dim) t)))
        -> graph (tensor (int * dim) t)
    | BlockRowMapReduce :: forall dim out_a out_b.
        (exists a. (layer_state -> primitives.row_config -> tensor (int * int) a -> dim -> tensor (int * int) int -> tensor (int * int) out_a * out_b) 
            * graph (tensor (int * dim * int) a)
            * option (graph (tensor (int * dim * int) out_a * tensor (int * dim) out_b)))
        -> graph (tensor (int * dim * int) out_a * tensor (int * dim) out_b)
    | BlockMatmul :: forall t. 
        graph (tensor (int * int * int) t) * graph (tensor (int * int) t) 
        -> graph (tensor (int * int * int) t)
    | Zip :: forall dim a b. graph (tensor dim a) * graph (tensor dim b) -> graph (tensor dim (a * b))
    | Pair :: forall a b. graph a * graph b -> graph (a * b)
    | Layer :: forall t. space_id * partitionm.partition t * (t -> ()) -> graph t
    | Apply :: forall b el. (exists a. graph (tensor (a * b) el) * graph a) -> graph (tensor b el)
    | KeyGraph :: forall t. (exists key{symbol}. key) * graph t -> graph t
    | KeyScalar :: forall t. (exists key{symbol}. key) -> graph t
    | Fst :: forall a. (exists b. graph (a * b)) -> graph a
    | Snd :: forall b. (exists a. graph (a * b)) -> graph b
union rec dim_of t =
    | D_D :: forall dim t. dim -> dim_of (tensor dim t)
    | D_P :: forall a b. dim_of a * dim_of b -> dim_of (a * b)

nominal model t =
    {
        graph : graph t
        dims : graph_dims
        data : graph_data
    }

inl memoize (h : compile_time.hashmap) f k =
    match compile_time.hashmapm.try_get h k with
    | Some v => v
    | None => inl v = f k in compile_time.hashmapm.add h k v . v

// Returns the top level dimension for an arbitrary graph type. Takes in a hashmap for memoization purposes.
// Results in a type errors error on the KeyScalar node.
inl rec get_dim_pair forall t_top. (h : graph_dims) : graph t_top -> dim_of t_top =
    inl f x = get_dim h x
    inl f' x = get_dim_pair h x
    inl check x = optionm.iter (fun x' => assert (D_D x = f' x') "The dimensions coming from the input and the dimensions of the output have to match.")
    memoize h (function
        | BlockRowMapReduce(exists a. _, a, out) =>
            inl q,w,e = f a
            assert (q = blocks_per_grid()) "The batch dimension in the row reduce node needs to equal the number of blocks per grid."
            out |> optionm.iter (fun out => 
                inl (D_P(D_D a, D_D b)) = f' out
                assert ((q,w,e) = a) "The dimensions coming from the input and the dimensions of the first output have to match."
                assert ((q,w) = b) "The dimensions coming from the input and the dimensions of the second output have to match."
                )
            D_P(D_D(q,w,e),D_D(q,w))
        | Pair(a,b) => D_P(f' a, f' b)
        | BlockMap(exists a. _, a, out) => 
            inl a = f a
            assert (fst a = blocks_per_grid()) "The batch dimension in the map node needs to equal the number of blocks per grid."
            check a out
            D_D a
        | BlockRowMap(exists a. _, a, out) => 
            inl a = f a
            assert (fst a = blocks_per_grid()) "The batch dimension in the row map node needs to equal the number of blocks per grid."
            check a out
            D_D a
        | BlockRowReduce(exists a. _, a, out) => 
            inl q,w,_ = f a
            assert (q = blocks_per_grid()) "The batch dimension in the row reduce node needs to equal the number of blocks per grid."
            check (q,w) out
            D_D(q,w)
        | BlockMatmul(a,b) =>
            inl (q,m,k as a) = f a
            assert (q = blocks_per_grid()) "The batch dimension in the matmul node needs to equal the number of blocks per grid."
            inl (n,k') = f b
            assert (k = k') "The inner dimensions of the matrix multiplication node have to be equal."
            D_D(q,m,n)
        | Zip(a,b) =>
            inl a,b = f a,f b
            assert (a = b) "The dimensions of the two inputs to the Zip node have to be equal."
            D_D a
        | Layer(sid, dim, init) => real
            typecase `dim with
            | _ (tensor ~dim ~t) => D_D `dim `t (partitionm.dim_of `dim `t dim)
            | _ ~p => 
                !!!!PrintStatic(`p)
                error_type "Can only get the dimension of a singleton layer. Check the terminal to see the type of the Layer node."
        | KeyGraph(_,x) => f' x
        | Apply(exists a. a,b) => D_D(snd (f a))
        | Fst(exists b. x) => inl (D_P(a,b)) = f' x in a
        | Snd(exists a. x) => inl (D_P(a,b)) = f' x in b
        )
and inl get_dim forall dim t. (h : graph_dims) (x : graph (tensor dim t)) : dim = match get_dim_pair h x with D_D x => x

inl pass_calculate_partitions forall t_top. (x : graph t_top) (dims : graph_dims) : graph_partitions =
    inl h_data = listm.init num_spaces() fun _ => compile_time.hashmapm.create()
    inl offset_key = .offset

    inl h = compile_time.hashmapm.create()
    inl get_offset' h_data : size = match compile_time.hashmapm.try_get h_data offset_key with Some x => x | None => 0
    inl get_offset (sid : space_id) : size = get_offset' (index h_data sid)
    inl set_offset (sid : space_id) (offset : size) : () = compile_time.hashmapm.set (index h_data sid) offset_key offset
    inl get_dim x = get_dim dims x
    inl get_dim_pair x = get_dim_pair dims x
    inl rec g forall t. (k : graph t) (out : option (graph t)) : () =
        match out with
        | Some x => f x
        | None =>
            inl sid = workspace_id()
            inl rec loop forall t'. (x : dim_of t') : partition_offsets t' =
                match x with
                | D_P(a,b) => open partitionm in loop a +. loop b
                | D_D dim =>
                    inl partition_offsets = partitionm.to_partition dim |> partitionm.calculate_offsets get_offset(sid)
                    set_offset sid partition_offsets.offset_end
                    partition_offsets
            compile_time.hashmapm.add (index h_data sid) k (loop (get_dim_pair k))
    and inl f forall t. : graph t -> () =
        memoize h fun k =>
            match k with
            | BlockMap(exists a. _,a,out) => f a . g k out
            | BlockRowMap(exists a. _,a,out) => f a . g k out
            | BlockRowReduce(exists a. _,a,out) => f a . g k out
            | BlockRowMapReduce(exists a. _,a,out) => f a . g k out
            | BlockMatmul(a,b) => f a . f b . g k None
            | Zip(a,b) => f a . f b
            | Pair(a,b) => f a . f b
            | Apply(exists a. a,b) => f a . f b
            | Fst(exists a. x) => f x
            | Snd(exists b. x) => f x
            | Layer(sid,p,_) as k =>
                inl partition_offsets : _ t = p |> partitionm.calculate_offsets get_offset(sid)
                compile_time.hashmapm.add (index h_data sid) k partition_offsets
                set_offset sid partition_offsets.offset_end
            | Apply(exists a. a,b) => f a . f b
            | KeyGraph(_,x) => f x
            | KeyScalar => ()
    f x
    listm.iter compile_time.hashmapm.set_immutable h_data
    listm.map (fun h => {partition=h; size=get_offset' h}) h_data

// Allocates an array given the graph offset.
inl create_graph_data (offset : graph_partitions) : graph_data =
    offset |> listm.map fun x =>
        inl array,size = partitionm.create_array x.size
        assert (size >= x.size) "The created array has to be equal or greater to the given size."
        {x with array}

inl graph_extract_ forall t_top indices. layer_state (model {graph data} : model t_top) (indices : indices) : t_top =
    inl abi t = apply block_index() t
    inl rec loop forall t. (graph : graph t) : t =
        inl handle_body = function
            | None => print_static graph . error_type "Cannot find the offset in the dictionaries. Check the output in the terminal to see which node it is."
            | Some x => x
        inl handle_data_elem (x : graph_data_elem) =
            compile_time.hashmapm.try_get x.partition graph
            |> optionm.map (partitionm.from_partition_offsets (x.array, x.size))
        inl extract_current () : t = data |> listm.chooseFirst handle_data_elem |> handle_body
        inl try_out = function
            | Some out => loop out
            | None => extract_current()
        match graph with
        | BlockMap(exists a. g,a,out) => 
            inl out = try_out out
            layer_state |> optionm.iter fun ls =>
                inl a = loop a
                inl g = g ls
                let block_map() = primitives.map g (abi a) (abi out)
                block_map()
            out
        | BlockRowMap(exists a. g,a,out) => 
            inl out = try_out out
            layer_state |> optionm.iter fun ls =>
                inl a = loop a
                inl g = g ls
                let block_row_map() = primitives.row_map g (abi a) (abi out)
                block_row_map()
            out
        | BlockRowReduce(exists a. g,a,out) =>
            inl out = try_out out
            layer_state |> optionm.iter fun ls =>
                inl a = loop a
                inl g = g ls
                let block_row_reduce() = primitives.row_reduce g (abi a) (abi out)
                block_row_reduce()
            out
        | BlockRowMapReduce(exists a. g,a,out) =>
            inl (out_map, out_reduce as out) = try_out out
            layer_state |> optionm.iter fun ls =>
                inl a = loop a
                inl g = g ls
                let block_row_map_reduce () = primitives.row_map_reduce g (abi a) (abi out_map) (abi out_reduce)
                block_row_map_reduce()
            out
        | BlockMatmul(a,b) =>
            inl a,b,out = loop a, loop b, extract_current()
            inl alpha, beta : float * float = 1, 0
            let block_matmul() =
                inl a, out = abi a, abi out
                real
                    open real_core
                    typecase t with
                    | tensor _ f32 => matmul.matmul_tf32' false true alpha a b beta out
                    | _ => 
                        !!!!PrintStatic(`t)
                        error_type "The type is not supported in the matrix multiply node. Only f32 is supported at the moment. Check the terminal to see which type was being passed in."
            block_matmul()
            out
        | Zip(a,b) => zip (loop a) (loop b)
        | Pair(a,b) => loop a, loop b
        | Layer(sid,_,_) => index data sid |> handle_data_elem |> handle_body
        | Apply(exists a. a,b) => apply (loop b) (loop a)
        | KeyGraph(_,x) => loop x
        | KeyScalar(exists k. k) => real 
            match indices with
            | {$k=x} => x : t
            | _ => real_core.error_type "Cannot extract the node for the graph due to the missing key."

    loop graph

// Extracts the tensor at the top level node.
inl graph_extract model indices = graph_extract_ None model indices

// Runs the graph and returns the result in the top level node. Is intended to be used in device code.
// Does not do any safety checking and is intended for internal use in other graph_run functions.
// Inserts the block index into the indices variable automatically.
inl graph_run forall t_top indices. (model : model t_top) (ls : layer_state) (indices : indices) : t_top = graph_extract_ (Some ls) model indices

type layer = exists t. space_id * t * (t -> ())
union extractor t =
    | Ex_Layer :: space_id -> extractor layer // passing -1 as the space_id selects all the layer spaces.
    | Ex_KeyGraph : (exists key. key)

// Extracts all the layers into a list.
inl extract_layers_ forall t_top ex_t. (ex : extractor ex_t) (model ({graph data} & m) : model t_top) : list ex_t =
    inl h = compile_time.hashmapm.create()
    inl list_key = .list_key
    inl get_current() : list ex_t = compile_time.hashmapm.try_get h list_key |> optionm.defaultWith []
    inl add_item (x : ex_t) = compile_time.hashmapm.set h list_key (x :: get_current())
    inl rec f forall t. : graph t -> () =
        memoize h function
            | BlockMap(exists a. _,a,out) => f a . optionm.iter f out
            | BlockRowMap(exists a. _,a,out) => f a . optionm.iter f out
            | BlockRowReduce(exists a. _,a,out) => f a . optionm.iter f out
            | BlockRowMapReduce(exists a. _,a,out) => f a . optionm.iter f out
            | BlockMatmul(a,b) => f a . f b
            | Apply(exists a. a,b) => f a . f b
            | Zip(a,b) => f a . f b
            | Pair(a,b) => f a . f b
            | Fst(exists a. x) => f x
            | Snd(exists b. x) => f x
            | KeyScalar => ()
            | Layer(sid,_,init) as graph =>
                match ex with
                | Ex_Layer sid' when sid' = -1 || sid = sid' =>
                    inl g = graph_extract (model {m with graph}) {}
                    add_item (exists sid, g, init)
                | _ => ()
            | KeyGraph((exists key'. _),graph) =>
                match ex with
                // `= operator compares the two types for equality.
                | Ex_KeyGraph(exists key. _) when real open real_core in `key `= `key' =>
                    inl g = graph_extract (model {m with graph}) {}
                    add_item (real g : ex_t) // If you get a type error here, then the input type doesn't match graph type.
                | _ => f graph
    f graph
    get_current() |> listm.rev

// Extracts all the layers into a list.
inl extract_layers forall t_top. (m : model t_top) : list layer = extract_layers_ (Ex_Layer -1) m

// Initializes the layers on the host.
inl pass_init forall t_top. (m : model t_top) = extract_layers m |> listm.iter fun (exists t. sid, g, init) => init g

// Prints all the layer parameters.
inl print_layers forall t_top. (m : model t_top) = extract_layers m |> listm.iter fun (exists t. sid, g, init) => console.write_ln g

// Prints the weight space layer parameters.
inl print_weights forall t_top. (m : model t_top) = extract_layers_ (Ex_Layer weight_id()) m |> listm.iter fun (exists t. sid, g, init) => console.write_ln g

// Extracts the input tensors from the Key nodes given the key.
inl key_extract_list forall t_top key{symbol} input. (m : model t_top) (key : key) : list input = extract_layers_ (Ex_KeyGraph exists key) m

// Extracts the input tensor from the Key nodes given the key. Gives a type error if there is more than one input tensor with the key, or if there isn't one.
inl key_extract forall t_top key{symbol} input. (m : model t_top) (key : key) : input =
    match key_extract_list m key with
    | [] => error_type "Cannot find the input tensor with the given key."
    | [x] => x
    | _ => error_type "More than one tensor with the given key found."

// Runs the kernel on the device.
inl graph_run_device forall t_top indices. (model {graph data} & m : model t_top) (ls : layer_state) (indices : indices) =
    inl _ = graph_run m ls indices
    ()

// Creates the layer state.
inl create_layer_state rng =
    layer_state {
        rng
    }

// Runs the graph on the host.
inl graph_run_host forall t_top sizes. (model {graph} & m : model t_top) (sizes : sizes) =
    run' fun () =>
        inl rng = random.init {seed = cupy.constant_seed(); subsequence = conv rangem.threads_in_grid().from; offset=0}
        inl ls : layer_state = create_layer_state rng
        loop.linear sizes fun indices =>
            graph_run_device m ls indices

// TODO: Reimplement the rest of the functions so they use row_map and row_reduce instead of the graph nodes directly.
inl row_map_ forall dim a b. f out (x : graph (tensor (int * dim * int) a)) : graph (tensor (int * dim * int) b) = 
    BlockRowMap (exists f, x, out)

inl row_reduce_ forall dim a b. f out (x : graph (tensor (int * dim * int) a)) : graph (tensor (int * dim) b) = 
    BlockRowReduce (exists f, x, out)

inl tanh forall dim t{float}. (x : graph (tensor (int * dim) t)) : graph (tensor (int * dim) t) = BlockMap (exists const tanh, x, None)
inl sigmoid forall dim t{float;number}. (x : graph (tensor (int * dim) t)) : graph (tensor (int * dim) t) = BlockMap (exists const sigmoid, x, None)
inl relu forall dim t{float;number}. (x : graph (tensor (int * dim) t)) : graph (tensor (int * dim) t) = BlockMap (exists const (max 0), x, None)

inl max_ forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) t) = 
    BlockRowReduce (exists (fun _ config x _ _ => primitives.local_max config x), x, out)
inl max' forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) t) = 
    max_ (Some out) x
inl max forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) t) = 
    max_ None x

inl ln_l2 forall dim t{float;number}. (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t) = 
    BlockRowMap (exists (fun _ config x _ _ => primitives.local_ln_l2 config x), x, None)

inl argmax forall dim t{number}. (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) int) =
    BlockRowReduce (exists (fun _ config x _ j_tns => primitives.local_argmax config x j_tns), x, None)

inl softmax_ forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t) = 
    BlockRowMap (exists (fun _ config x _ _ => primitives.local_softmax config x), x, out)
inl softmax' forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t) = 
    softmax_ (Some out) x
inl softmax forall dim t{float;number}. (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t) = 
    softmax_ None x


inl discrete_sample_ forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) int) = 
    inl f (layer_state {rng}) config x i j_tns = primitives.local_discrete_sampling rng config x i j_tns
    BlockRowReduce (exists f, x, out)
inl discrete_sample' forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) int) = 
    discrete_sample_ (Some out) x
inl discrete_sample forall dim t{float;number}. (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) int) = 
    discrete_sample_ None x

inl softmax_and_discrete_sample_ forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t * tensor (int * dim) int) = 
    inl f (layer_state {rng}) config x i j_tns = primitives.local_softmax_and_discrete_sampling rng config x i j_tns
    BlockRowMapReduce (exists f, x, out)
inl softmax_and_discrete_sample' forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) =
    softmax_and_discrete_sample_ (Some out) x
inl softmax_and_discrete_sample forall dim t{float;number}. (x : graph (tensor (int * dim * int) t)) =
    softmax_and_discrete_sample_ None x

inl masked_softmax_ forall dim t{float;number}. size out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t) = 
    BlockRowMap (exists (fun _ config x => primitives.local_masked_softmax (fun config x i j => j < size) config x), x, out)
inl masked_softmax' forall dim t{float;number}. size out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t) = 
    masked_softmax_ size (Some out) x
inl masked_softmax forall dim t{float;number}. size (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t) = 
    masked_softmax_ size None x

inl masked_softmax_and_discrete_sample_ forall dim t{float;number}. size out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t * tensor (int * dim) int) = 
    inl f (layer_state {rng}) config x i j_tns = 
        inl i = primitives.local_masked_softmax_and_discrete_sampling (fun confix x i j => j < size) rng config x i j_tns
        assert (snd i < size) "The masking requirement is violated in masked_softmax_and_discrete_sample_."
        i
    BlockRowMapReduce (exists f, x, out)
inl masked_softmax_and_discrete_sample' forall dim t{float;number}. size out (x : graph (tensor (int * dim * int) t)) =
    masked_softmax_and_discrete_sample_ size (Some out) x
inl masked_softmax_and_discrete_sample forall dim t{float;number}. size (x : graph (tensor (int * dim * int) t)) =
    masked_softmax_and_discrete_sample_ size None x

inl binarize_ forall dim t{float;number}. modulo out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) uint) = 
    BlockRowReduce (exists const (primitives.local_binarize modulo), x, out)
inl binarize forall dim t{float;number}. modulo (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) uint) =
    binarize_ modulo None x
inl binarize' forall dim t{float;number}. modulo out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) uint) =
    binarize_ modulo (Some out) x

inl key_scalar forall t key{symbol}. (key : key) : graph t = KeyScalar(exists key)
inl key_graph forall t key{symbol}. (key : key) (x : graph t) : graph t = KeyGraph((exists key), x)
inl apply forall key{symbol} a b t. (b : key) (a : graph (tensor (a * b) t)) : graph (tensor b t) = Apply(exists a, key_scalar b)
inl pair forall a b. (a : graph a) (b : graph b) : graph (a * b) = Pair(a,b)
inl fst' forall a b. (x : graph (a * b)) : graph a = Fst(exists x)
inl layer id p init = Layer(id, p, init)
inl tensor id dim' init = open partitionm in Layer(id, !dim', init)

// Initializes all the elements of the tensors to zero.
inl init_zero forall t. (x : t) : () = real
    struct.iter (fun x =>
        struct.iter (fun x =>
            $"!(x.array)[:] = 0" : ()
            ) x.bodies
    ) x

// Takes the inverse of the sqrt of the product of the tensor dimensions. Similar to Xavier initialization.
inl weight_init forall dim t. (to : tensor dim t) : () = // TODO: This would be xavier init if we were using 2d matrices. We're also missing a mult by a constant.
    cupy.copy_to { to from=cupy.random_normal{mean=0; std=loop.prod to.dim |> conv |> (/) 1 |> sqrt} to.dim }
    
inl weight' x init =  tensor weight_id() x init
inl weight x = weight' x weight_init
inl matmul (b,a) x = BlockMatmul(x, weight (a,b))
inl matmul_ensemble (c,b,a : int * int * int) x =
    open partitionm
    inl weights = tensor weight_id() (c,a,b) weight_init
    BlockMatmul(x, apply .ensemble weights)
type ensemble_id = int
type cem_weight_layer_dual float = tensor (ensemble_id * (int * int)) float * tensor ((int * int) * ensemble_id) float
inl matmul_ensemble_with_adjoint forall el. (c,b,a : int * int * int) x : graph (tensor (int * int * int) el) =
    inl weights : graph (cem_weight_layer_dual el) = 
        pair (tensor weight_id() (c,(a,b)) weight_init)
             (tensor weight_adjoint_id() ((a,b),c) const() : graph (tensor _ el))
        |> key_graph .cem_weight_layer_dual
    BlockMatmul(x, apply .ensemble (fst' weights))
// Creates a layer in the workspace. Doesn't initialize it.
inl input' x = Layer(workspace_id(), x, const ())
inl input forall dim' t key{symbol}. (key : key) (dim' : dim') : graph (tensor dim' t) = 
    open partitionm
    key_graph key (tensor workspace_id() dim' const())

inl create_model forall t. (graph : graph t) =
    inl dims = compile_time.hashmapm.create()
    inl data = create_graph_data (pass_calculate_partitions graph dims)
    compile_time.hashmapm.set_immutable(dims)
    model {graph data dims}

// The size of the array is bound to num_spaces.
nominal model_ptrs = tuple_list num_spaces ptr

inl model_to_model_ptrs (x : model _) = x.data |> listm.map (fun x => x.array) |> tuple_listm.fromList |> model_ptrs
inl model_ptrs_to_model (graph : graph _) (model_ptrs d) =
    inl dims = compile_time.hashmapm.create()
    inl data =
        (pass_calculate_partitions graph dims, tuple_listm.toList d)
        ||> listm.map2 (fun x array => {x with array})
    compile_time.hashmapm.set_immutable(dims)
    model { graph dims data }

type d2 = int * int
type d3 = int * int * int
type d4 = int * int * int * int

// Tests whether randonmly initializing the params of the graph works.
inl test1() =
    cupy.set_random_seed(cupy.constant_seed())
    testing.redirect_io_into "test_text_outputs/layers/" "test1" "layers.txt" fun () =>
    inl graph : graph (tensor d3 float) =
        input .input (blocks_per_grid(),1,4)
        |> matmul (4,4)
        |> tanh
        |> matmul (4,4)
        |> tanh
        |> matmul (4,2)
        |> tanh

    inl model = create_model graph
    
    console.write_ln "---"
    print_weights model
    pass_init model
    console.write_ln "Done initing."
    print_weights model
    ()

// Tests whether extracting the inputs of the graph works. Also randomly inits them.
inl test2() =
    cupy.set_random_seed(cupy.constant_seed())
    testing.redirect_io_into "test_text_outputs/layers/" "test2" "layers.txt" fun () =>
    inl graph : graph (tensor d3 float) =
        input .input (blocks_per_grid(),1,2)
        |> matmul (2,4)
        |> tanh
        |> matmul (4,4)
        |> tanh
        |> matmul (4,2)
        |> tanh

    inl model = create_model graph
    console.write_ln "---"
    pass_init model
    console.write_ln "Here are the weight matrices."
    print_weights model
    console.write_ln "Here is the input tensor."
    inl input : tensor d3 float = key_extract model .input
    cupy.copy_to {
        from = cupy.random_normal{mean=0; std=1} input.dim
        to = input
    }
    console.write_ln input
    ()

// Tests running a feedforward model which outputs the sampling indices.
inl test3() =
    cupy.set_random_seed(cupy.constant_seed())
    testing.redirect_io_into "test_text_outputs/layers/" "test3" "layers.txt" fun () =>
    inl graph : graph (tensor d3 float * tensor d2 int) =
        (input .input (blocks_per_grid(),64,64) : graph (tensor _ float))
        |> matmul (64,64)
        |> ln_l2
        |> relu
        |> matmul (64,64)
        |> ln_l2
        |> relu
        |> matmul (64,64)
        |> softmax_and_discrete_sample

    inl model = create_model graph
    pass_init model
    console.write_ln "Here are the weight matrices."
    print_weights model
    inl input : tensor d3 float = key_extract model .input
    // Immitates the passing of data into the output vector.
    // In the poker game the serialization function will be responsible for this.
    cupy.copy_to {
        from = cupy.random_normal{mean=0; std=1} input.dim
        to = input
    }
    console.write_ln input
    console.write_ln "Here is the output tensor."
    graph_run_host model {}
    inl tns_output = graph_extract model {}
    console.write_ln tns_output
    console.write_ln "===="
    ()

// Tests running a feedforward ensemble model which keeps around the sampling probabilities.
inl test4() =
    cupy.set_random_seed(cupy.constant_seed())
    testing.redirect_io_into "test_text_outputs/layers/" "test4" "layers.txt" fun () =>
    inl ensemble = 16
    inl graph : graph (tensor d3 float * tensor d2 int) =
        (input .input (blocks_per_grid(),64,64) : graph (tensor _ float))
        |> matmul_ensemble (ensemble,64,64)
        |> ln_l2
        |> relu
        |> matmul_ensemble (ensemble,64,64)
        |> ln_l2
        |> relu
        |> matmul_ensemble (ensemble,64,64)
        |> softmax_and_discrete_sample' (
            pair (input .output_probs   (ensemble,blocks_per_grid(),64,64) |> apply .ensemble) 
                 (input .output_indices (ensemble,blocks_per_grid(),64) |> apply .ensemble)
            )

    inl model = create_model graph
    pass_init model
    console.write_ln "Here are the weight matrices."
    print_weights model
    inl input : tensor d3 float = key_extract model .input
    // Immitates the passing of data into the output vector.
    // In the poker game the serialization function will be responsible for this.
    cupy.copy_to {
        from = cupy.random_normal{mean=0; std=1} input.dim
        to = input
    }
    console.write_ln input
    console.write_ln "Here is the output tensor."
    graph_run_host model {ensemble}
    inl output : tensor d3 int = key_extract model .output_indices
    console.tensor.write_ln limit.max output
    ()

// Tests running a feedforward ensemble model which keeps around the sampling probabilities with masking.
inl test5() =
    cupy.set_random_seed(cupy.constant_seed())
    testing.redirect_io_into "test_text_outputs/layers/" "test5" "layers.txt" fun () =>
    inl ensemble : int = 4
    inl graph =
        (input .input (blocks_per_grid(),64,64) : graph (tensor _ float))
        |> matmul_ensemble (ensemble,64,64)
        // |> ln_l2
        // |> relu
        // |> matmul_ensemble (ensemble,64,64)
        // |> ln_l2
        // |> relu
        // |> matmul_ensemble (ensemble,64,64)
        |> masked_softmax_and_discrete_sample' 11 (
            pair (input .output_probs   (ensemble,blocks_per_grid(),64,64) |> apply .ensemble) 
                 (input .output_indices (ensemble,blocks_per_grid(),64) |> apply .ensemble)
            )

    inl model = create_model graph
    pass_init model
    // console.write_ln "Here are the weight matrices."
    // print_weights model
    inl input : tensor d3 float = key_extract model .input
    // Immitates the passing of data into the output vector.
    // In the poker game the serialization function will be responsible for this.
    cupy.copy_to {
        from = cupy.random_normal{mean=0; std=1} input.dim
        to = input
    }
    graph_run_host model {ensemble}
    // console.write_ln "Here is the input tensor."
    // console.write_ln input
    // console.write_ln "Here is the output tensor."
    // inl output : tensor d4 float = key_extract model .output_probs
    // console.tensor.write_ln limit.max output
    inl output : tensor d3 int = key_extract model .output_indices
    console.tensor.write_ln limit.max output

inl main() = 
    test1()
    test2()
    test3()
    test4()
    test5()