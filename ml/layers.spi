open corebase
open corecuda
open tensorm
open compile_time

type ptr = partitionm.ptr
type size = partitionm.size
type graph_array = ptr * size
type graph_dims = hashmapm.hashmap
type graph_offset = hashmapm.hashmap * size
type graph_data = {array : graph_array; offset : graph_offset}

nominal layer_state = 
    {
        rng : refm.ref random.philox_state
    }

union rec graph t =
    | BlockMap :: forall dim t. (exists a. (layer_state -> a -> t) * graph (tensor (int * dim) a) * option (graph (tensor (int * dim) t))) -> graph (tensor (int * dim) t)
    | BlockRowMap :: forall dim t.
        (exists a. (layer_state -> primitives.row_config -> tensor (int * int) a -> dim -> tensor (int * int) int -> tensor (int * int) t) 
            * graph (tensor (int * dim * int) a)
            * option (graph (tensor (int * dim * int) t)))
        -> graph (tensor (int * dim * int) t)
    | BlockRowReduce :: forall dim t.
        (exists a. (layer_state -> primitives.row_config -> tensor (int * int) a -> dim -> tensor (int * int) int -> t) 
            * graph (tensor (int * dim * int) a)
            * option (graph (tensor (int * dim) t)))
        -> graph (tensor (int * dim) t)
    | BlockMatmul :: forall t. graph (tensor (int * int * int) t) * graph (tensor (int * int) t) -> graph (tensor (int * int * int) t)
    | Zip :: forall dim a b. graph (tensor dim a) * graph (tensor dim b) -> graph (tensor dim (a * b))
    | Pair :: forall a b. graph a * graph b -> graph (a * b)
    | Weight :: forall dim t. dim -> graph (tensor dim t)
    | Input :: forall dim t. dim -> graph (tensor dim t)
    | Apply :: forall b el. (exists a. graph (tensor (a * b) el) * graph a) -> graph (tensor b el)
    | KeyTensor :: forall dim t. (exists key{symbol}. key) * graph (tensor dim t) -> graph (tensor dim t)
    | KeyScalar :: forall t. (exists key{symbol}. key) -> graph t

nominal model t =
    {
        graph : graph t
        dims : graph_dims
        output : graph_data
        param : graph_data
    }

inl memoize (h : hashmapm.hashmap) f k =
    match hashmapm.try_get h k with
    | Some v => v
    | None => inl v = f k in hashmapm.add h k v . v

// Returns the top level dimension. Takes in a hashmap for memoization purposes.
inl rec get_dim forall dim_top t_top. (h : graph_dims) : graph (tensor dim_top t_top) -> dim_top =
    inl f x = get_dim h x
    inl check x out =
        optionm.iter (fun x' => assert (x = f x') "The dimensions coming from the input and the dimensions of the output have to match.") out
        x
    memoize h (function
        | BlockMap(exists a. _, a, out) => 
            inl a = f a
            assert (fst a = blocks_per_grid()) "The batch dimension in the map node needs to equal the number of blocks per grid."
            check a out
        | BlockRowMap(exists a. _, a, out) => 
            inl a = f a
            assert (fst a = blocks_per_grid()) "The batch dimension in the row map node needs to equal the number of blocks per grid."
            check a out
        | BlockRowReduce(exists a. _, a, out) => 
            inl a = f a
            assert (fst a = blocks_per_grid()) "The batch dimension in the row reduce node needs to equal the number of blocks per grid."
            check (a |> fun a,b,_ => a,b) out
        | BlockMatmul(a,b) =>
            inl (q,m,k as a) = f a
            assert (q = blocks_per_grid()) "The batch dimension in the matmul node needs to equal the number of blocks per grid."
            inl (n,k') = f b
            assert (k = k') "The inner dimensions of the matrix multiplication node have to be equal."
            q,m,n
        | Zip(a,b) =>
            inl a,b = f a,f b
            assert (a = b) "The dimensions of the two inputs to the Zip node have to be equal."
            a
        | Weight dim => dim
        | Input dim => dim
        | KeyTensor(_,x) => f x
        | Apply(exists a. a,b) => snd (f a)
        )

// Calculates the parameter offsets.
inl pass_offset_param forall t_top. (x : graph t_top) (dims : graph_dims) : graph_offset =
    inl h_offset_param = hashmapm.create() // stores the offsets pointing to the start of the weight parameter array for each node
    inl h = hashmapm.create()
    inl get_dim x = get_dim dims x
    inl rec g forall t. (offset : size) (out : option (graph t)) : size =
        match out with
        | Some x' => f offset x'
        | None => offset
    and inl f forall t. (offset : size) : graph t -> size =
        memoize h function
            | BlockMap(exists a. _,a,out) => g (f offset a) out
            | BlockRowMap(exists a. _,a,out) => g (f offset a) out
            | BlockRowReduce(exists a. _,a,out) => g (f offset a) out
            | BlockMatmul(a,b) => f (f offset a) b
            | Zip(a,b) => f (f offset a) b
            | Pair(a,b) => f (f offset a) b
            | Weight as k =>
                inl dim = get_dim k
                inl partition_offsets : _ t = partitionm.to_partition dim |> partitionm.calculate_offsets offset 
                hashmapm.add h_offset_param k partition_offsets
                partition_offsets.offset_end
            | Apply(exists a. a,b) => f (f offset a) b
            | KeyTensor(_,x) => f offset x
            | KeyScalar => offset
            | Input => offset
    inl offset = f 0 x
    hashmapm.set_immutable(h_offset_param)
    h_offset_param, offset

// Calculates the output node offsets.
inl pass_offset_output forall t_top. (x : graph t_top) (dims : graph_dims) : graph_offset =
    inl h_offset_output = hashmapm.create() // stores the offsets pointing to the start of the output array for each node
    inl h = hashmapm.create()
    inl get_dim x = get_dim dims x
    inl rec g forall dim t. (offset : size) (k : graph (tensor dim t)) (out : option (graph (tensor dim t))) : size =
        match out with
        | Some x => f offset x
        | None =>
            inl partition_offsets : _ (_ _ t) = partitionm.to_partition (get_dim k) |> partitionm.calculate_offsets offset
            hashmapm.add h_offset_output k partition_offsets
            partition_offsets.offset_end
    and inl f forall t. (offset : size) : graph t -> size =
        memoize h fun k =>
            match k with
            | BlockMap(exists a. _,a,out) => g (f offset a) k out
            | BlockRowMap(exists a. _,a,out) => g (f offset a) k out
            | BlockRowReduce(exists a. _,a,out) => g (f offset a) k out
            | BlockMatmul(a,b) => g (f (f offset a) b) k None
            | Zip(a,b) => f (f offset a) b // passthrough
            | Pair(a,b) => f (f offset a) b // passthrough
            | Weight => offset
            | Apply(exists a. a,b) => f (f offset a) b // passthrough
            | Input => g offset k None
            | KeyTensor(_,x) => f offset x // passthrough
            | KeyScalar(_) => offset
    inl offset = f 0 x
    hashmapm.set_immutable(h_offset_output)
    h_offset_output, offset

// Calculates the max of dynamic shared memory used for all the nodes.
inl pass_calculate_dynamic_shared_memory forall t_top. (x : graph t_top) =
    inl h = hashmapm.create()
    inl rec g forall t. (size : size) (x : option (graph t)) : size =
        match x with
        | None => size
        | Some x => max size (f x)
    and inl f forall t. : graph t -> size =
        memoize h function
            | BlockMap(exists a. _,a,out) => g (f a) out
            | BlockRowMap(exists a. _,a,out) => g (f a) out
            | BlockRowReduce(exists a. _,a,out) => g (f a) out
            | BlockMatmul(a,b) => 
                inl memory : size =
                    inl tf32 () : size = fst(matmul.matmul_tf32' false true).offset_end
                    real
                        typecase t with
                        | tensor (int * int) f32 => tf32()
                        | _ => 
                            !!!!PrintStatic(`t)
                            error_type `(()) "The type is not supported in the matrix multiply node. Only f32 is supported at the moment. Check the terminal to see which type was being passed in."
                max (f a) (f b) |> max memory
            | Zip(a,b) => max (f a) (f b)
            | Pair(a,b) => max (f a) (f b)
            | Apply(exists a. a,b) => max (f a) (f b)
            | KeyTensor(_,x) => f x
            | KeyScalar => 0
            | Weight => 0
            | Input => 0
    f x

// Initializes the parameters of a NN on the host.
inl param_init forall t_top. (model {graph param} : model t_top) =
    assert (snd param.array = snd param.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."
    inl h = hashmapm.create()
    inl get_tensor forall dim t. (k : graph (tensor dim t)) : tensor dim t = partitionm.from_partition_offsets param.array (hashmapm.get (fst param.offset) k)
    inl rec g forall t. : option (graph t) -> () = function
        | None => ()
        | Some x => f x
    and inl f forall t. : graph t -> () =
        memoize h function
            | BlockMap(exists a. _,a,out)  => f a . g out
            | BlockRowMap(exists a. _,a,out) => f a . g out
            | BlockRowReduce(exists a. _,a,out) => f a . g out
            | BlockMatmul(a,b) => f a . f b
            | Zip(a,b) => f a . f b
            | Pair(a,b) => f a . f b
            | Weight as x =>
                inl x = get_tensor x
                cupy.copy_to {from=cupy.random_normal{mean=0; std=1} x.dim; to=x} // TODO: Don't forget to init the layers properly.
            | Apply(exists a. a,b) => f a . f b
            | KeyTensor(_,x) => f x
            | KeyScalar => ()
            | Input => ()
    f graph

// Prints all the weight layers.
inl param_print forall t_top. (model {graph param} : model t_top) =
    assert (snd param.array = snd param.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."
    inl h = hashmapm.create()
    inl get_tensor forall dim t. (k : graph (tensor dim t)) : tensor dim t = partitionm.from_partition_offsets param.array (hashmapm.get (fst param.offset) k)
    inl rec g forall t. : option (graph t) -> () = function
        | None => ()
        | Some x => f x
    and inl f forall t. : graph t -> () =
        memoize h function
            | BlockMap(exists a. _,a,out)  => f a . g out
            | BlockRowMap(exists a. _,a,out) => f a . g out
            | BlockRowReduce(exists a. _,a,out) => f a . g out
            | BlockMatmul(a,b) => f a . f b
            | Zip(a,b) => f a . f b
            | Pair(a,b) => f a . f b
            | Weight as x => get_tensor x |> console.write_ln
            | Apply(exists a. a,b) => f a . f b
            | KeyTensor(_,x) => f x
            | KeyScalar => ()
            | Input => ()
    f graph

// Allocates an array given the graph offset.
inl create_graph_data (offset : graph_offset) : graph_data =
    inl array = partitionm.create_array (snd offset)
    {array offset}

// Extracts the tensor at the top level node.
inl graph_tensor_extract forall t' indices. (model {graph param output} : model t') (indices : indices) : t' =
    inl rec loop forall t. (graph : graph t) : t =
        match graph with
        | BlockMap(exists a. _,_,Some out) => loop out
        | BlockRowMap(exists a. _,_,Some out) => loop out
        | BlockRowReduce(exists a. _,_,Some out) => loop out
        | Zip(a,b) => zip (loop a) (loop b)
        | Pair(a,b) => loop a, loop b
        | Apply(exists a. a,b) => apply (loop b) (loop a)
        | KeyScalar(exists k. k) => real 
            match indices with
            | {$k=x} => x : t
            | _ => real_core.error_type "Cannot extract the node for the graph due to the missing key."
        | KeyTensor(_,x) => loop x
        | _ =>
            match hashmapm.try_get (fst param.offset) graph with
            | Some x => partitionm.from_partition_offsets param.array x
            | None =>
                match hashmapm.try_get (fst output.offset) graph with
                | Some x => partitionm.from_partition_offsets output.array x
                | None => 
                    print_static graph
                    error_type "Cannot find the offset in the dictionaries. Check the output in the terminal to see which node it is."
    loop graph

// Extracts the input tensor from the Key node given the key.
inl key_extract forall t_top key{symbol} dim input. (model ({graph param output} & m) : model t_top) (_ : key) : tensor dim input =
    assert (snd output.array = snd output.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."
    inl h = hashmapm.create()
    inl rec g forall t. (x : option (tensor dim input)) (out : option (graph t)) : option (tensor dim input) =
        match x with
        | Some => x
        | None =>
            match out with
            | Some x => f x
            | None => None
    and inl pair forall a b. (a : graph a) (b : graph b) : option (tensor dim input) =
        match f a with
        | None => f b
        | a => a
    and inl f forall t. : graph t -> option (tensor dim input) =
        memoize h function
            | BlockMap(exists a. _,a,out) => g (f a) out
            | BlockRowMap(exists a. _,a,out) => g (f a) out
            | BlockRowReduce(exists a. _,a,out) => g (f a) out
            | BlockMatmul(a,b) => pair a b
            | Apply(exists a. a,b) => pair a b
            | Zip(a,b) => pair a b
            | Pair(a,b) => pair a b
            | Weight => None
            | Input => None
            | KeyScalar => None
            | KeyTensor((exists key'. _),graph) => 
                if (real open real_core in `key `= `key') then // `= operator compares the two types for equality.
                    Some (graph_tensor_extract (model {m with graph}) {} |> nominal_recreate)
                else f graph
    match f graph with
    | Some x => x
    | None => error_type "Cannot find the input tensor with the given key."

// Runs the graph and returns the result in the top level node. Is intended to be used in device code.
// Does not do any safety checking and is intended for internal use in other graph_run functions.
// Inserts the block index into the indices variable automatically.
inl graph_run forall t_top indices. (model ({graph param output} & m) : model t_top) (ls : layer_state) (indices : indices) : t_top =
    inl h = hashmapm.create()
    real match indices with {block} => real_core.error_type "The block will be automatically inserted in graph_run. It shouldn't be passed into it beforehand." | _ => ()
    inl abi t = apply block_index() t
    inl indices = real {indices with block=block_index()}
    inl tensor_extract forall t. (graph : graph t) : t =
        inl f forall indices'. (indices : indices') : t = graph_tensor_extract (model {m with graph}) indices
        real f `(`indices) indices
    inl rec f forall t. : graph t -> t =
        open primitives
        memoize h fun x =>
            match x with
            | BlockMap(exists a. g,a,_) =>
                inl a, out = f a, tensor_extract x
                inl g = g ls
                join map g (abi a) (abi out)
                out
            | BlockRowMap(exists a. g,a,_) =>
                inl a, out = f a, tensor_extract x
                inl g = g ls
                join row_map g (abi a) (abi out)
                out
            | BlockRowReduce(exists a. g,a,_) =>
                inl a, out = f a, tensor_extract x
                inl g = g ls
                join row_reduce g (abi a) (abi out)
                out
            | BlockMatmul(a,b) =>
                inl a,b,out = f a, f b, tensor_extract x
                inl alpha, beta : float * float = 1, 0
                inl () =
                    inl a,out = abi a,abi out
                    real
                        open real_core
                        typecase t with
                        | tensor (int * int) f32 =>
                            inl memory, body = matmul.matmul_tf32' false true
                            join body alpha a b beta out
                        | _ => 
                            !!!!PrintStatic(`t)
                            error_type "The type is not supported in the matrix multiply node. Only f32 is supported at the moment. Check the terminal to see which type was being passed in."
                out
            // These need to be separate otherwise the typecase inserted by the inference will be overly specific 
            // and we'll get a typecase miss during partial evaluation.
            | Zip(a,b) => zip (f a) (f b)
            | Pair(a,b) => f a, f b
            | Apply(exists a. a,b) => apply (f b) (f a)
            | Weight => tensor_extract x
            | Input => tensor_extract x
            | KeyTensor => tensor_extract x
            | KeyScalar => tensor_extract x
    f graph

// Runs the kernel on the device.
inl graph_run_device forall t_top indices. (model {graph param output} & m : model t_top) (indices : indices) =
    assert (snd param.array = snd param.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."
    assert (snd output.array = snd output.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."

    inl ls : layer_state =
        open random
        layer_state {
            rng = init {seed = clock64(); subsequence = conv rangem.threads_in_grid().from; offset=0}
        }

    inl _ = graph_run m ls indices
    ()

// Just for testing purposes.
inl graph_run_host forall t_top sizes. (model {graph} & m : model t_top) (sizes : sizes) =
    run' {shared_mem=conv (pass_calculate_dynamic_shared_memory graph)} fun () =>
        loop.linear sizes fun indices =>
            graph_run_device m indices

inl tanh forall dim t{float}. (x : graph (tensor (int * dim) t)) : graph (tensor (int * dim) t) = BlockMap (exists const tanh, x, None)
inl sigmoid forall dim t{float;number}. (x : graph (tensor (int * dim) t)) : graph (tensor (int * dim) t) = BlockMap (exists const sigmoid, x, None)
inl relu forall dim t{float;number}. (x : graph (tensor (int * dim) t)) : graph (tensor (int * dim) t) = BlockMap (exists const (max 0), x, None)
inl softmax' forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t) = 
    BlockRowMap (exists (fun _ config x _ _ => primitives.local_softmax config x), x, Some out)
inl softmax forall dim t{float;number}. (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t) = 
    BlockRowMap (exists (fun _ config x _ _ => primitives.local_softmax config x), x, None)
inl ln_l2 forall dim t{float;number}. (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t) = 
    BlockRowMap (exists (fun _ config x _ _ => primitives.local_ln_l2 config x), x, None)
inl argmax forall dim t{number}. (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) int) =
    BlockRowReduce (exists (fun _ config x _ j_tns => primitives.local_argmax config x j_tns), x, None)
inl discrete_sample' forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) int) = 
    inl f (layer_state {rng}) config x i j_tns = primitives.local_discrete_sampling rng config x i j_tns
    BlockRowReduce (exists f, x, Some out)
inl discrete_sample forall dim t{float;number}. (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) int) = 
    inl f (layer_state {rng}) config x i j_tns = primitives.local_discrete_sampling rng config x i j_tns
    BlockRowReduce (exists f, x, None)
inl softmax_and_discrete_sample' forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) int) = 
    inl f (layer_state {rng}) config x i j_tns = primitives.local_softmax_and_discrete_sampling rng config x i j_tns
    BlockRowReduce (exists f, x, Some out)
inl softmax_and_discrete_sample forall dim t{float;number}. (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) int) = 
    inl f (layer_state {rng}) config x i j_tns = primitives.local_softmax_and_discrete_sampling rng config x i j_tns
    BlockRowReduce (exists f, x, None)
inl matmul (b,a) x = BlockMatmul(x, Weight (a,b))
inl key_scalar forall t key{symbol}. (key : key) : graph t = KeyScalar(exists key)
inl matmul_ensemble (c,b,a : int * int * int) x = BlockMatmul(x, Apply(exists Weight (c,a,b), key_scalar .ensemble))
inl input forall dim t key{symbol}. (key : key) (dim : dim) : graph (tensor dim t) = KeyTensor((exists key), Input(dim))
inl apply forall key{symbol} a b t. (b : key) (a : graph (tensor (a * b) t)) : graph (tensor b t) = Apply(exists a, key_scalar b)
inl pair forall a b. (a : graph a) (b : graph b) : graph (a * b) = Pair(a,b)

inl create_model forall t. (graph : graph t) =
    inl dims = hashmapm.create()
    inl param = create_graph_data (pass_offset_param graph dims)
    inl output = create_graph_data (pass_offset_output graph dims)
    hashmapm.set_immutable(dims)
    model {graph param output dims}

nominal model_data = {param : ptr * size; output : ptr * size}

inl model_to_model_data (x : model _) = model_data {param=x.param.array; output=x.output.array}
inl model_data_to_model (graph : graph _) (d : model_data) =
    inl dims = hashmapm.create()
    inl param = pass_offset_param graph dims
    assert (snd param = snd d.param) "The params needs to have matching offsets."
    inl output = pass_offset_output graph dims
    assert (snd output = snd d.output) "The outputs needs to have matching offsets."
    hashmapm.set_immutable(dims)
    model { graph dims
        param = { array = fst d.param, snd param; offset = param }
        output = { array = fst d.output, snd output; offset = output }
        }

// Tests whether randonmly initializing the params of the graph works.
inl test1() =
    inl graph : graph (tensor (int * int * int) float) =
        input .input (blocks_per_grid(),1,4)
        |> matmul (4,4)
        |> tanh
        |> matmul (4,4)
        |> tanh
        |> matmul (4,2)
        |> tanh

    inl model = create_model graph
    
    console.write_ln "---"
    param_print model
    param_init model
    console.write_ln "Done initing."
    param_print model
    ()

// Tests whether extracting the inputs of the graph works. Also randomly inits them.
inl test2() =
    inl graph : graph (tensor (int * int * int) float) =
        input .input (blocks_per_grid(),1,2)
        |> matmul (2,4)
        |> tanh
        |> matmul (4,4)
        |> tanh
        |> matmul (4,2)
        |> tanh

    inl model = create_model graph
    console.write_ln "---"
    param_init model
    console.write_ln "Here are the weight matrices."
    param_print model
    console.write_ln "Here is the input tensor."
    inl input : tensor (int * int) float = key_extract model .input
    cupy.copy_to {
        from = cupy.random_normal{mean=0; std=1} input.dim
        to = input
    }
    console.write_ln input
    ()

// Tests running a feedforward model.
inl test3() =
    inl graph : graph (tensor (int * int) int) =
        (input .input (blocks_per_grid(),16,8) : graph (tensor _ float))
        |> matmul (8,16)
        |> ln_l2
        |> relu
        |> matmul (16,16)
        |> ln_l2
        |> relu
        |> matmul (16,16)
        |> softmax_and_discrete_sample

    inl model = create_model graph
    param_init model
    console.write_ln "Here are the weight matrices."
    param_print model
    inl input : tensor (int * int) float = key_extract model .input
    // Immitates the passing of data into the output vector.
    // In the poker game the serialization function will be responsible for this.
    cupy.copy_to {
        from = cupy.random_normal{mean=0; std=1} input.dim
        to = input
    }
    console.write_ln input
    console.write_ln "Here is the output tensor."
    graph_run_host model {}
    inl tns_output = graph_tensor_extract model {}
    console.write_ln tns_output
    console.write_ln "===="
    ()

// Tests running a feedforward ensemble model.
inl test4() =
    inl ensemble = 16
    inl graph : graph (tensor (int * int) int) =
        (input .input (blocks_per_grid(),16,8) : graph (tensor _ float))
        |> matmul_ensemble (ensemble,8,16)
        |> ln_l2
        |> relu
        |> matmul_ensemble (ensemble,16,16)
        |> ln_l2
        |> relu
        |> matmul_ensemble (ensemble,16,16)
        |> softmax_and_discrete_sample' (input .output (ensemble,16) |> apply .ensemble)

    inl model = create_model graph
    param_init model
    console.write_ln "Here are the weight matrices."
    param_print model
    inl input : tensor (int * int) float = key_extract model .input
    // Immitates the passing of data into the output vector.
    // In the poker game the serialization function will be responsible for this.
    cupy.copy_to {
        from = cupy.random_normal{mean=0; std=1} input.dim
        to = input
    }
    console.write_ln input
    console.write_ln "Here is the output tensor."
    graph_run_host model {ensemble}
    inl output : tensor (int * int) int = key_extract model .output
    printing.tensor_print_ln limit.max output
    ()

// Tests running a feedforward ensemble model which keeps around the path probabilities.
inl test5() =
    inl ensemble : int = 16
    inl graph : graph (tensor (i32 * i32 * i32) f32 * tensor (i32 * i32) i32) =
        inl x =
            (input .input (blocks_per_grid(),16,8) : graph (tensor _ float))
            |> matmul_ensemble (ensemble,8,16)
            |> ln_l2
            |> relu
            |> matmul_ensemble (ensemble,16,16)
            |> ln_l2
            |> relu
            |> matmul_ensemble (ensemble,16,16)
            |> softmax' (input .output_probs (ensemble,blocks_per_grid(),16,16) |> apply .ensemble)
        pair x (discrete_sample' (input .output_indices (ensemble,blocks_per_grid(),16) |> apply .ensemble) x)

    inl model = create_model graph
    param_init model
    console.write_ln "Here are the weight matrices."
    param_print model
    inl input : tensor (int * int) float = key_extract model .input
    // Immitates the passing of data into the output vector.
    // In the poker game the serialization function will be responsible for this.
    cupy.copy_to {
        from = cupy.random_normal{mean=0; std=1} input.dim
        to = input
    }
    graph_run_host model {ensemble}
    console.write_ln "Here is the input tensor."
    console.write_ln input
    console.write_ln "Here is the output tensor."
    inl output : tensor (int * int * int) float = key_extract model .output_probs
    printing.tensor_print_ln (1 <<< 9) output
    inl output : tensor (int * int) int = key_extract model .output_indices
    printing.tensor_print_ln limit.max output

// Tests running a feedforward ensemble model which keeps around the path probabilities. The inputs are spread across the block of a GPU.
inl test6() =
    inl block = blocks_per_grid()
    inl ensemble : int = 16
    inl graph : graph (tensor (i32 * i32) f32 * tensor i32 i32) =
        inl x = // Note: This example has a reasoning error. We shouldn't be applying the block dimension here.
            (input .input (block,16,8) : graph (tensor _ float))
            |> apply .block // The block index gets passed inside graph_run_device automatically.
            |> matmul_ensemble (ensemble,8,16)
            |> ln_l2
            |> relu
            |> matmul_ensemble (ensemble,16,16)
            |> ln_l2
            |> relu
            |> matmul_ensemble (ensemble,16,16)
            |> softmax' (input .output_probs (ensemble,16,16) |> apply .ensemble)
        pair x (discrete_sample' (input .output_indices (ensemble,16) |> apply .ensemble) x)

    inl model = create_model graph
    param_init model
    console.write_ln "Here are the weight matrices."
    param_print model
    inl input : tensor (int * int * int) float = key_extract model .input
    // Immitates the passing of data into the output vector.
    // In the poker game the serialization function will be responsible for this.
    cupy.copy_to {
        from = cupy.random_normal{mean=0; std=1} input.dim
        to = input
    }
    graph_run_host model {ensemble}
    console.write_ln "Here is the input tensor."
    console.write_ln input
    console.write_ln "Here is the output tensor."
    inl output : tensor (int * int * int) float = key_extract model .output_probs
    printing.tensor_print_ln (1 <<< 9) output
    inl output : tensor (int * int) int = key_extract model .output_indices
    printing.tensor_print_ln limit.max output
inl main() = test6()