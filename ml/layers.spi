open corebase
open corecuda
open coreext
open tensorm
open compile_time

type ptr = partitionm.ptr
type size = partitionm.size
type graph_array = ptr * size
type graph_dims = hashmapm.hashmap
type graph_offset = hashmapm.hashmap * size
type graph_data = {array : graph_array; offset : graph_offset}

nominal layer_state = 
    {
        rng : refm.ref random.philox_state
    }

union rec graph t =
    | BlockMap :: forall dim t. 
        (exists a. (layer_state -> a -> t) 
            * graph (tensor (int * dim) a) 
            * option (graph (tensor (int * dim) t))) 
        -> graph (tensor (int * dim) t)
    | BlockRowMap :: forall dim t.
        (exists a. (layer_state -> primitives.row_config -> tensor (int * int) a -> dim -> tensor (int * int) int -> tensor (int * int) t) 
            * graph (tensor (int * dim * int) a)
            * option (graph (tensor (int * dim * int) t)))
        -> graph (tensor (int * dim * int) t)
    | BlockRowReduce :: forall dim t.
        (exists a. (layer_state -> primitives.row_config -> tensor (int * int) a -> dim -> tensor (int * int) int -> t) 
            * graph (tensor (int * dim * int) a)
            * option (graph (tensor (int * dim) t)))
        -> graph (tensor (int * dim) t)
    | BlockRowMapReduce :: forall dim out_a out_b.
        (exists a. (layer_state -> primitives.row_config -> tensor (int * int) a -> dim -> tensor (int * int) int -> tensor (int * int) out_a * out_b) 
            * graph (tensor (int * dim * int) a)
            * option (graph (tensor (int * dim * int) out_a * tensor (int * dim) out_b)))
        -> graph (tensor (int * dim * int) out_a * tensor (int * dim) out_b)
    | BlockMatmul :: forall t. 
        graph (tensor (int * int * int) t) * graph (tensor (int * int) t) 
        -> graph (tensor (int * int * int) t)
    | Zip :: forall dim a b. graph (tensor dim a) * graph (tensor dim b) -> graph (tensor dim (a * b))
    | Pair :: forall a b. graph a * graph b -> graph (a * b)
    | Weight :: forall t. partitionm.partition t * (t -> ()) -> graph t
    | Input :: forall dim t. dim -> graph (tensor dim t)
    | Apply :: forall b el. (exists a. graph (tensor (a * b) el) * graph a) -> graph (tensor b el)
    | KeyGraph :: forall t. (exists key{symbol}. key) * graph t -> graph t
    | KeyScalar :: forall t. (exists key{symbol}. key) -> graph t
union rec dim_of t =
    | D_D :: forall dim t. dim -> dim_of (tensor dim t)
    | D_P :: forall a b. dim_of a * dim_of b -> dim_of (a * b)
union rec partition_of t =
    | P_D :: forall t. partitionm.partition_offsets t -> partition_of t
    | P_P :: forall a b. partition_of a * partition_of b -> partition_of (a * b)

nominal model t =
    {
        graph : graph t
        dims : graph_dims
        output : graph_data
        param : graph_data
    }

inl memoize (h : hashmapm.hashmap) f k =
    match hashmapm.try_get h k with
    | Some v => v
    | None => inl v = f k in hashmapm.add h k v . v

// Returns the top level dimension for an arbitrary graph type. Takes in a hashmap for memoization purposes.
// Results in a type errors error on the KeyScalar node.
inl rec get_dim_pair forall t_top. (h : graph_dims) : graph t_top -> dim_of t_top =
    inl f x = get_dim h x 
    inl f' x = get_dim_pair h x
    inl check x = optionm.iter (fun x' => assert (D_D x = f' x') "The dimensions coming from the input and the dimensions of the output have to match.")
    memoize h (function
        | BlockRowMapReduce(exists a. _, a, out) =>
            inl q,w,e = f a
            assert (q = blocks_per_grid()) "The batch dimension in the row reduce node needs to equal the number of blocks per grid."
            out |> optionm.iter (fun out => 
                inl (D_P(D_D a, D_D b)) = f' out
                assert ((q,w,e) = a) "The dimensions coming from the input and the dimensions of the first output have to match."
                assert ((q,w) = b) "The dimensions coming from the input and the dimensions of the second output have to match."
                )
            D_P(D_D(q,w,e),D_D(q,w))
        | Pair(a,b) => D_P(f' a, f' b)
        | BlockMap(exists a. _, a, out) => 
            inl a = f a
            assert (fst a = blocks_per_grid()) "The batch dimension in the map node needs to equal the number of blocks per grid."
            check a out
            D_D a
        | BlockRowMap(exists a. _, a, out) => 
            inl a = f a
            assert (fst a = blocks_per_grid()) "The batch dimension in the row map node needs to equal the number of blocks per grid."
            check a out
            D_D a
        | BlockRowReduce(exists a. _, a, out) => 
            inl q,w,_ = f a
            assert (q = blocks_per_grid()) "The batch dimension in the row reduce node needs to equal the number of blocks per grid."
            check (q,w) out
            D_D(q,w)
        | BlockMatmul(a,b) =>
            inl (q,m,k as a) = f a
            assert (q = blocks_per_grid()) "The batch dimension in the matmul node needs to equal the number of blocks per grid."
            inl (n,k') = f b
            assert (k = k') "The inner dimensions of the matrix multiplication node have to be equal."
            D_D(q,m,n)
        | Zip(a,b) =>
            inl a,b = f a,f b
            assert (a = b) "The dimensions of the two inputs to the Zip node have to be equal."
            D_D a
        | Weight dim => real
            typecase `dim with
            | _ (tensor ~dim ~t) => D_D `dim `t (partitionm.dim_of dim)
            | _ ~p => 
                !!!!PrintStatic(`p)
                error_type "Can only get the dimension of a singleton weight layer. Check the terminal to see the type of the Weight node."
        | Input dim => D_D dim
        | KeyGraph(_,x) => f' x
        | Apply(exists a. a,b) => D_D(snd (f a))
        )
and inl get_dim forall dim t. (h : graph_dims) (x : graph (tensor dim t)) : dim = match get_dim_pair h x with D_D x => x

// Calculates the parameter offsets.
inl pass_offset_param forall t_top. (x : graph t_top) (dims : graph_dims) : graph_offset =
    inl h_offset_param = hashmapm.create() // stores the offsets pointing to the start of the weight parameter array for each node
    inl h = hashmapm.create()
    inl offset_key = ()
    inl get_offset () : size = match hashmapm.try_get h offset_key with Some x => x | None => 0
    inl set_offset (offset : size) : () = hashmapm.set h offset_key offset
    inl get_dim x = get_dim dims x
    inl rec g forall t. (out : option (graph t)) : () =
        match out with
        | Some x' => f x'
        | None => ()
    and inl f forall t. : graph t -> () =
        memoize h function
            | BlockMap(exists a. _,a,out) => f a . g out
            | BlockRowMap(exists a. _,a,out) => f a . g out
            | BlockRowReduce(exists a. _,a,out) => f a . g out
            | BlockRowMapReduce(exists a. _,a,out) => f a . g out
            | BlockMatmul(a,b) => f a . f b
            | Zip(a,b) => f a . f b
            | Pair(a,b) => f a . f b
            | Weight(p,init) as k =>
                inl partition_offsets : _ t = p |> partitionm.calculate_offsets get_offset()
                hashmapm.add h_offset_param k partition_offsets
                set_offset partition_offsets.offset_end
            | Apply(exists a. a,b) => f a . f b
            | KeyGraph(_,x) => f x
            | KeyScalar => ()
            | Input => ()
    f x
    hashmapm.set_immutable(h_offset_param)
    h_offset_param, get_offset()

// Calculates the output node offsets.
inl pass_offset_output forall t_top. (x : graph t_top) (dims : graph_dims) : graph_offset =
    inl h_offset_output = hashmapm.create() // stores the offsets pointing to the start of the output array for each node
    inl h = hashmapm.create()
    inl offset_key = ()
    inl get_offset () : size = match hashmapm.try_get h offset_key with Some x => x | None => 0
    inl set_offset (offset : size) : () = hashmapm.set h offset_key offset
    inl get_dim x = get_dim dims x
    inl get_dim_pair x = get_dim_pair dims x
    inl rec g forall t. (k : graph t) (out : option (graph t)) : () =
        match out with
        | Some x => f x
        | None =>
            inl rec loop forall t'. (x : dim_of t') : partition_of t' =
                match x with
                | D_P(a,b) => P_P(loop a, loop b)
                | D_D dim =>
                    inl partition_offsets = partitionm.to_partition dim |> partitionm.calculate_offsets get_offset()
                    set_offset partition_offsets.offset_end
                    P_D partition_offsets
            hashmapm.add h_offset_output k (loop (get_dim_pair k))
    and inl f forall t. : graph t -> () =
        memoize h fun k =>
            match k with
            | BlockMap(exists a. _,a,out) => f a . g k out
            | BlockRowMap(exists a. _,a,out) => f a . g k out
            | BlockRowReduce(exists a. _,a,out) => f a . g k out
            | BlockRowMapReduce(exists a. _,a,out) => f a . g k out
            | BlockMatmul(a,b) => f a . f b . g k None
            | Zip(a,b) => f a . f b // passthrough
            | Pair(a,b) => f a . f b // passthrough
            | Weight => ()
            | Apply(exists a. a,b) => f a . f b // passthrough
            | Input => g k None
            | KeyGraph(_,x) => f x // passthrough
            | KeyScalar(_) => ()
        
    f x
    hashmapm.set_immutable(h_offset_output)
    h_offset_output, get_offset()

// Calculates the max of dynamic shared memory used for all the nodes.
inl pass_calculate_dynamic_shared_memory forall t_top. (x : graph t_top) =
    inl h = hashmapm.create()
    inl offset_key = ()
    inl get_offset () : size = match hashmapm.try_get h offset_key with Some x => x | None => 0
    inl set_offset (offset : size) : () = hashmapm.set h offset_key offset
    // Sets the shared memory size to the new value if it's higher than the old one.
    inl update (size : size) = set_offset (max size get_offset())
    inl rec g forall t. (x : option (graph t)) : () =
        match x with
        | None => ()
        | Some x => f x
    and inl f forall t. : graph t -> () =
        memoize h function
            | BlockMap(exists a. _,a,out) => f a . g out
            | BlockRowMap(exists a. _,a,out) => f a . g out
            | BlockRowReduce(exists a. _,a,out) => f a . g out
            | BlockRowMapReduce(exists a. _,a,out) => f a . g out
            | BlockMatmul(a,b) => 
                inl memory : size =
                    inl tf32 () : size = fst(matmul.matmul_tf32' false true).offset_end
                    real
                        typecase t with
                        | tensor _ f32 => tf32()
                        | _ => 
                            !!!!PrintStatic(`t)
                            error_type `(()) "The type is not supported in the matrix multiply node. Only f32 is supported at the moment. Check the terminal to see which type was being passed in."
                f a . f b . update memory
            | Zip(a,b) => f a . f b
            | Pair(a,b) => f a . f b
            | Apply(exists a. a,b) => f a . f b
            | KeyGraph(_,x) => f x
            | KeyScalar => ()
            | Weight => ()
            | Input => ()
    f x
    get_offset()

// Initializes the parameters of a NN on the host.
inl param_init forall t_top. (model {graph param} : model t_top) =
    assert (snd param.array = snd param.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."
    inl h = hashmapm.create()
    inl rec g forall t. : option (graph t) -> () = function
        | None => ()
        | Some x => f x
    and inl f forall t. : graph t -> () =
        memoize h function
            | BlockMap(exists a. _,a,out)  => f a . g out
            | BlockRowMap(exists a. _,a,out) => f a . g out
            | BlockRowReduce(exists a. _,a,out) => f a . g out
            | BlockRowMapReduce(exists a. _,a,out) => f a . g out
            | BlockMatmul(a,b) => f a . f b
            | Zip(a,b) => f a . f b
            | Pair(a,b) => f a . f b
            | Weight(_, init) as k => partitionm.from_partition_offsets param.array (hashmapm.get (fst param.offset) k) |> init
            | Apply(exists a. a,b) => f a . f b
            | KeyGraph(_,x) => f x
            | KeyScalar => ()
            | Input => ()
    f graph

// Prints all the weight layers.
inl param_print forall t_top. (model {graph param} : model t_top) =
    assert (snd param.array = snd param.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."
    inl h = hashmapm.create()
    inl get_partition forall t. (k : graph t) : t = partitionm.from_partition_offsets param.array (hashmapm.get (fst param.offset) k)
    inl rec g forall t. : option (graph t) -> () = function
        | None => ()
        | Some x => f x
    and inl f forall t. : graph t -> () =
        memoize h function
            | BlockMap(exists a. _,a,out)  => f a . g out
            | BlockRowMap(exists a. _,a,out) => f a . g out
            | BlockRowReduce(exists a. _,a,out) => f a . g out
            | BlockRowMapReduce(exists a. _,a,out) => f a . g out
            | BlockMatmul(a,b) => f a . f b
            | Zip(a,b) => f a . f b
            | Pair(a,b) => f a . f b
            | Weight as x => get_partition x |> console.write_ln
            | Apply(exists a. a,b) => f a . f b
            | KeyGraph(_,x) => f x
            | KeyScalar => ()
            | Input => ()
    f graph

// Allocates an array given the graph offset.
inl create_graph_data (offset : graph_offset) : graph_data =
    inl array = partitionm.create_array (snd offset)
    {array offset}

// Extracts the tensor at the top level node.
inl graph_extract forall t_top indices. (model {graph param output} : model t_top) (indices : indices) : t_top =
    inl rec loop forall t. (graph : graph t) : t =
        match graph with
        | BlockMap(exists a. _,_,Some out) => loop out
        | BlockRowMap(exists a. _,_,Some out) => loop out
        | BlockRowReduce(exists a. _,_,Some out) => loop out
        | BlockRowMapReduce(exists a. _,_,Some out) => loop out
        | Zip(a,b) => zip (loop a) (loop b)
        | Pair(a,b) => loop a, loop b
        | Apply(exists a. a,b) => apply (loop b) (loop a)
        | KeyScalar(exists k. k) => real 
            match indices with
            | {$k=x} => x : t
            | _ => real_core.error_type "Cannot extract the node for the graph due to the missing key."
        | KeyGraph(_,x) => loop x
        | _ =>
            match hashmapm.try_get (fst param.offset) graph with
            | Some x => partitionm.from_partition_offsets param.array x
            | None =>
                match hashmapm.try_get (fst output.offset) graph with
                | Some (x : partition_of t) => 
                    inl rec f forall t'. : partition_of t' -> t' = function
                        | P_D x => partitionm.from_partition_offsets output.array x
                        | P_P(a,b) => f a, f b
                    f x
                | None =>
                    print_static graph
                    error_type "Cannot find the offset in the dictionaries. Check the output in the terminal to see which node it is."
    loop graph

// Extracts the input tensor from the Key node given the key.
inl key_extract forall t_top key{symbol} input. (model ({graph param output} & m) : model t_top) (_ : key) : input =
    assert (snd output.array = snd output.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."
    inl h = hashmapm.create()
    inl rec g forall t. (x : option input) (out : option (graph t)) : option input =
        match x with
        | Some => x
        | None =>
            match out with
            | Some x => f x
            | None => None
    and inl pair forall a b. (a : graph a) (b : graph b) : option input =
        match f a with
        | None => f b
        | a => a
    and inl f forall t. : graph t -> option input =
        memoize h function
            | BlockMap(exists a. _,a,out) => g (f a) out
            | BlockRowMap(exists a. _,a,out) => g (f a) out
            | BlockRowReduce(exists a. _,a,out) => g (f a) out
            | BlockRowMapReduce(exists a. _,a,out) => g (f a) out
            | BlockMatmul(a,b) => pair a b
            | Apply(exists a. a,b) => pair a b
            | Zip(a,b) => pair a b
            | Pair(a,b) => pair a b
            | Weight => None
            | Input => None
            | KeyScalar => None
            | KeyGraph((exists key'. _),graph) =>
                if (real open real_core in `key `= `key') then // `= operator compares the two types for equality.
                    Some (graph_extract (model {m with graph}) {} |> nominal_recreate)
                else f graph
    match f graph with
    | Some x => x
    | None => error_type "Cannot find the input tensor with the given key."

// Runs the graph and returns the result in the top level node. Is intended to be used in device code.
// Does not do any safety checking and is intended for internal use in other graph_run functions.
// Inserts the block index into the indices variable automatically.
inl graph_run forall t_top indices. (model ({graph param output} & m) : model t_top) (ls : layer_state) (indices : indices) : t_top =
    inl h = hashmapm.create()
    inl abi t = apply block_index() t
    inl tensor_extract forall t. (graph : graph t) : t = graph_extract (model {m with graph}) indices
    inl rec f forall t. : graph t -> t =
        open primitives
        memoize h fun x =>
            // inl debug str f =
            //     if thread_index() = 0 then console.write_ln str
            //     barrier_cta_sync 0
            //     inl x = f()
            //     if thread_index() = 0 then console.write_ln "done"
            //     barrier_cta_sync 0
            //     x
            match x with
            | BlockMap(exists a. g,a,_) =>
                inl a, out = f a, tensor_extract x
                inl g = g ls
                join map g (abi a) (abi out)
                out
            | BlockRowMap(exists a. g,a,_) =>
                inl a, out = f a, tensor_extract x
                inl g = g ls
                join row_map g (abi a) (abi out)
                out
            | BlockRowReduce(exists a. g,a,_) =>
                inl a, out = f a, tensor_extract x
                inl g = g ls
                join row_reduce g (abi a) (abi out)
                out
            | BlockRowMapReduce(exists a. g,a,_) =>
                inl a, (out_map, out_reduce as out) = f a, tensor_extract x
                inl g = g ls
                join row_map_reduce g (abi a) (abi out_map) (abi out_reduce)
                out
            | BlockMatmul(a,b) =>
                inl a,b,out = f a, f b, tensor_extract x
                inl alpha, beta : float * float = 1, 0
                inl () =
                    inl a, out = abi a, abi out
                    real
                        open real_core
                        typecase t with
                        | tensor _ f32 =>
                            inl memory, body = matmul.matmul_tf32' false true
                            join body alpha a b beta out
                        | _ => 
                            !!!!PrintStatic(`t)
                            error_type "The type is not supported in the matrix multiply node. Only f32 is supported at the moment. Check the terminal to see which type was being passed in."
                out
            // These need to be separate otherwise the typecase inserted by the inference will be overly specific 
            // and we'll get a typecase miss during partial evaluation.
            | Zip(a,b) => zip (f a) (f b)
            | Pair(a,b) => f a, f b
            | Apply(exists a. a,b) => apply (f b) (f a)
            | Weight => tensor_extract x
            | Input => tensor_extract x
            | KeyGraph => tensor_extract x
            | KeyScalar => tensor_extract x
    f graph

// Runs the kernel on the device.
inl graph_run_device forall t_top indices. (model {graph param output} & m : model t_top) (ls : layer_state) (indices : indices) =
    assert (snd param.array = snd param.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."
    assert (snd output.array = snd output.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."
    inl _ = graph_run m ls indices
    ()

// Creates the layer state.
inl create_layer_state() =
    open random
    layer_state {
        rng = init {seed = clock64(); subsequence = conv rangem.threads_in_grid().from; offset=0}
    }

// Runs the graph on the host.
inl graph_run_host forall t_top sizes. (model {graph} & m : model t_top) (sizes : sizes) =
    run' {shared_mem=conv (pass_calculate_dynamic_shared_memory graph)} fun () =>
        inl ls : layer_state = create_layer_state()
        loop.linear sizes fun indices =>
            graph_run_device m ls indices

// TODO: Reimplement the rest of the functions so the use row_map and row_reduce instead of the graph nodes directly.
inl row_map_ forall dim a b. f out (x : graph (tensor (int * dim * int) a)) : graph (tensor (int * dim * int) b) = 
    BlockRowMap (exists f, x, out)

inl row_reduce_ forall dim a b. f out (x : graph (tensor (int * dim * int) a)) : graph (tensor (int * dim) b) = 
    BlockRowReduce (exists f, x, out)

inl tanh forall dim t{float}. (x : graph (tensor (int * dim) t)) : graph (tensor (int * dim) t) = BlockMap (exists const tanh, x, None)
inl sigmoid forall dim t{float;number}. (x : graph (tensor (int * dim) t)) : graph (tensor (int * dim) t) = BlockMap (exists const sigmoid, x, None)
inl relu forall dim t{float;number}. (x : graph (tensor (int * dim) t)) : graph (tensor (int * dim) t) = BlockMap (exists const (max 0), x, None)

inl max_ forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) t) = 
    BlockRowReduce (exists (fun _ config x _ _ => primitives.local_max config x), x, out)
inl max' forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) t) = 
    max_ (Some out) x
inl max forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) t) = 
    max_ None x

inl ln_l2 forall dim t{float;number}. (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t) = 
    BlockRowMap (exists (fun _ config x _ _ => primitives.local_ln_l2 config x), x, None)

inl argmax forall dim t{number}. (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) int) =
    BlockRowReduce (exists (fun _ config x _ j_tns => primitives.local_argmax config x j_tns), x, None)

inl softmax_ forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t) = 
    BlockRowMap (exists (fun _ config x _ _ => primitives.local_softmax config x), x, out)
inl softmax' forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t) = 
    softmax_ (Some out) x
inl softmax forall dim t{float;number}. (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t) = 
    softmax_ None x


inl discrete_sample_ forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) int) = 
    inl f (layer_state {rng}) config x i j_tns = primitives.local_discrete_sampling rng config x i j_tns
    BlockRowReduce (exists f, x, out)
inl discrete_sample' forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) int) = 
    discrete_sample_ (Some out) x
inl discrete_sample forall dim t{float;number}. (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) int) = 
    discrete_sample_ None x

inl softmax_and_discrete_sample_ forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t * tensor (int * dim) int) = 
    inl f (layer_state {rng}) config x i j_tns = primitives.local_softmax_and_discrete_sampling rng config x i j_tns
    BlockRowMapReduce (exists f, x, out)
inl softmax_and_discrete_sample' forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) =
    softmax_and_discrete_sample_ (Some out) x
inl softmax_and_discrete_sample forall dim t{float;number}. (x : graph (tensor (int * dim * int) t)) =
    softmax_and_discrete_sample_ None x

inl masked_softmax_ forall dim t{float;number}. size out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t) = 
    BlockRowMap (exists (fun _ config x => primitives.local_masked_softmax (fun config x i j => j < size) config x), x, out)
inl masked_softmax' forall dim t{float;number}. size out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t) = 
    masked_softmax_ size (Some out) x
inl masked_softmax forall dim t{float;number}. size (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t) = 
    masked_softmax_ size None x

inl masked_softmax_and_discrete_sample_ forall dim t{float;number}. size out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t * tensor (int * dim) int) = 
    inl f (layer_state {rng}) config x i j_tns = 
        inl i = primitives.local_masked_softmax_and_discrete_sampling (fun confix x i j => j < size) rng config x i j_tns
        assert (snd i < size) "The masking requirement is violated in masked_softmax_and_discrete_sample_."
        i
    BlockRowMapReduce (exists f, x, out)
inl masked_softmax_and_discrete_sample' forall dim t{float;number}. size out (x : graph (tensor (int * dim * int) t)) =
    masked_softmax_and_discrete_sample_ size (Some out) x
inl masked_softmax_and_discrete_sample forall dim t{float;number}. size (x : graph (tensor (int * dim * int) t)) =
    masked_softmax_and_discrete_sample_ size None x

// Takes the sqrt of the product of the tensor dimensions.
inl weight_init forall dim t. (x : tensor dim t) : () =
    cupy.copy_to {from=cupy.random_normal{mean=0; std=loop.prod x.dim |> conv |> sqrt} x.dim; to=x}
inl matmul (b,a) x =
    open partitionm
    BlockMatmul(x, Weight(!(a,b), weight_init))
inl key_scalar forall t key{symbol}. (key : key) : graph t = KeyScalar(exists key)
inl matmul_ensemble (c,b,a : int * int * int) x =
    open partitionm
    BlockMatmul(x, Apply(exists Weight(!(c,a,b), weight_init), key_scalar .ensemble))
inl input forall dim t key{symbol}. (key : key) (dim : dim) : graph (tensor dim t) = KeyGraph((exists key), Input(dim))
inl apply forall key{symbol} a b t. (b : key) (a : graph (tensor (a * b) t)) : graph (tensor b t) = Apply(exists a, key_scalar b)
inl pair forall a b. (a : graph a) (b : graph b) : graph (a * b) = Pair(a,b)

inl create_model forall t. (graph : graph t) =
    inl dims = hashmapm.create()
    inl param = create_graph_data (pass_offset_param graph dims)
    inl output = create_graph_data (pass_offset_output graph dims)
    hashmapm.set_immutable(dims)
    model {graph param output dims}

nominal model_data = {param : ptr * size; output : ptr * size}

inl model_to_model_data (x : model _) = model_data {param=x.param.array; output=x.output.array}
inl model_data_to_model (graph : graph _) (d : model_data) =
    inl dims = hashmapm.create()
    inl param = pass_offset_param graph dims
    assert (snd param = snd d.param) "The params needs to have matching offsets."
    inl output = pass_offset_output graph dims
    assert (snd output = snd d.output) "The outputs needs to have matching offsets."
    hashmapm.set_immutable(dims)
    model { graph dims
        param = { array = fst d.param, snd param; offset = param }
        output = { array = fst d.output, snd output; offset = output }
        }

type d2 = int * int
type d3 = int * int * int
type d4 = int * int * int * int

// Tests whether randonmly initializing the params of the graph works.
inl test1() =
    inl graph : graph (tensor d3 float) =
        input .input (blocks_per_grid(),1,4)
        |> matmul (4,4)
        |> tanh
        |> matmul (4,4)
        |> tanh
        |> matmul (4,2)
        |> tanh

    inl model = create_model graph
    
    console.write_ln "---"
    param_print model
    param_init model
    console.write_ln "Done initing."
    param_print model
    ()

// Tests whether extracting the inputs of the graph works. Also randomly inits them.
inl test2() =
    inl graph : graph (tensor d3 float) =
        input .input (blocks_per_grid(),1,2)
        |> matmul (2,4)
        |> tanh
        |> matmul (4,4)
        |> tanh
        |> matmul (4,2)
        |> tanh

    inl model = create_model graph
    console.write_ln "---"
    param_init model
    console.write_ln "Here are the weight matrices."
    param_print model
    console.write_ln "Here is the input tensor."
    inl input : tensor d3 float = key_extract model .input
    cupy.copy_to {
        from = cupy.random_normal{mean=0; std=1} input.dim
        to = input
    }
    console.write_ln input
    ()

// Tests running a feedforward model which outputs the sampling indices.
inl test3() =
    inl graph : graph (tensor d3 float * tensor d2 int) =
        (input .input (blocks_per_grid(),16,8) : graph (tensor _ float))
        |> matmul (8,16)
        |> ln_l2
        |> relu
        |> matmul (16,16)
        |> ln_l2
        |> relu
        |> matmul (16,16)
        |> softmax_and_discrete_sample

    inl model = create_model graph
    param_init model
    console.write_ln "Here are the weight matrices."
    param_print model
    inl input : tensor d3 float = key_extract model .input
    // Immitates the passing of data into the output vector.
    // In the poker game the serialization function will be responsible for this.
    cupy.copy_to {
        from = cupy.random_normal{mean=0; std=1} input.dim
        to = input
    }
    console.write_ln input
    console.write_ln "Here is the output tensor."
    graph_run_host model {}
    inl tns_output = graph_extract model {}
    console.write_ln tns_output
    console.write_ln "===="
    ()

// Tests running a feedforward ensemble model which keeps around the sampling probabilities.
inl test4() =
    inl ensemble = 16
    inl graph : graph (tensor d3 float * tensor d2 int) =
        (input .input (blocks_per_grid(),16,8) : graph (tensor _ float))
        |> matmul_ensemble (ensemble,8,16)
        |> ln_l2
        |> relu
        |> matmul_ensemble (ensemble,16,16)
        |> ln_l2
        |> relu
        |> matmul_ensemble (ensemble,16,16)
        |> softmax_and_discrete_sample' (
            pair (input .output_probs   (ensemble,blocks_per_grid(),16,16) |> apply .ensemble) 
                 (input .output_indices (ensemble,blocks_per_grid(),16) |> apply .ensemble)
            )

    inl model = create_model graph
    param_init model
    console.write_ln "Here are the weight matrices."
    param_print model
    inl input : tensor d3 float = key_extract model .input
    // Immitates the passing of data into the output vector.
    // In the poker game the serialization function will be responsible for this.
    cupy.copy_to {
        from = cupy.random_normal{mean=0; std=1} input.dim
        to = input
    }
    console.write_ln input
    console.write_ln "Here is the output tensor."
    graph_run_host model {ensemble}
    inl output : tensor d3 int = key_extract model .output_indices
    console.tensor.write_ln limit.max output
    ()

// Tests running a feedforward ensemble model which keeps around the sampling probabilities with masking.
inl test5() =
    inl inner : int = 32 * 4 * 64
    inl ensemble : int = 4
    inl graph =
        (input .input (blocks_per_grid(),threads_per_block(),inner) : graph (tensor _ float))
        |> matmul_ensemble (ensemble,inner,inner)
        // |> ln_l2
        // |> relu
        // |> matmul_ensemble (ensemble,inner,inner)
        // |> ln_l2
        // |> relu
        // |> matmul_ensemble (ensemble,inner,inner)
        |> masked_softmax_and_discrete_sample' 11 (
            pair (input .output_probs   (ensemble,blocks_per_grid(),threads_per_block(),inner) |> apply .ensemble) 
                 (input .output_indices (ensemble,blocks_per_grid(),threads_per_block()) |> apply .ensemble)
            )

    inl model = create_model graph
    param_init model
    // console.write_ln "Here are the weight matrices."
    // param_print model
    inl input : tensor d3 float = key_extract model .input
    // Immitates the passing of data into the output vector.
    // In the poker game the serialization function will be responsible for this.
    cupy.copy_to {
        from = cupy.random_normal{mean=0; std=1} input.dim
        to = input
    }
    graph_run_host model {ensemble}
    // console.write_ln "Here is the input tensor."
    // console.write_ln input
    // console.write_ln "Here is the output tensor."
    // inl output : tensor d4 float = key_extract model .output_probs
    // console.tensor.write_ln limit.max output
    // inl output : tensor d3 int = key_extract model .output_indices
    // console.tensor.write_ln limit.max output

inl main() = test5()