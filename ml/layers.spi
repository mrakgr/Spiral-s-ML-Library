open corebase
open corecuda
open tensorm
open compile_time

type ptr = partitionm.ptr
type size = partitionm.size
type graph_array = ptr * size
type graph_dims = hashmapm.hashmap
type graph_offset = hashmapm.hashmap * size
type graph_data = {array : graph_array; offset : graph_offset}

nominal layer_state = 
    {
        rng : refm.ref random.philox_state
    }

union rec graph t =
    // | Reduce :: forall t. (exists dim a. (layer_state -> a -> t) * graph (tensor dim a)) -> graph t
    | Map :: forall dim t. (exists a. (layer_state -> a -> t) * graph (tensor dim a) * option (graph (tensor dim t))) -> graph (tensor dim t)
    | RowMap :: forall t.
        (exists a. (layer_state -> primitives.row_config -> tensor (int * int) a -> int -> tensor (int * int) int -> tensor (int * int) t) 
            * graph (tensor (int * int) a)
            * option (graph (tensor (int * int) t)))
        -> graph (tensor (int * int) t)
    | RowReduce :: forall t. 
        (exists a. (layer_state -> primitives.row_config -> tensor (int * int) a -> int -> tensor (int * int) int -> t) 
            * graph (tensor (int * int) a)
            * option (graph (tensor int t)))
        -> graph (tensor int t)
    | Matmul :: forall t. graph (tensor (int * int) t) * graph (tensor (int * int) t) -> graph (tensor (int * int) t)
    | Zip :: forall dim a b. graph (tensor dim a) * graph (tensor dim b) -> graph (tensor dim (a * b))
    | Pair :: forall a b. graph a * graph b -> graph (a * b)
    | Weight :: forall dim t. dim -> graph (tensor dim t)
    | Input :: forall dim t. (exists key{symbol}. key) * dim -> graph (tensor dim t)
    | Apply :: forall b el. (exists a. graph (tensor (a * b) el) * graph a) -> graph (tensor b el)
    | InputScalar :: forall a. (exists key{symbol}. key) -> graph a

nominal model t =
    {
        graph : graph t
        dims : graph_dims
        output : graph_data
        param : graph_data
    }

inl memoize (h : hashmapm.hashmap) f k =
    match hashmapm.try_get h k with
    | Some v => v
    | None => inl v = f k in hashmapm.add h k v . v

// Returns the top level dimension. Takes in a hashmap for memoization purposes.
inl rec get_dim forall dim_top t_top. (h : graph_dims) : graph (tensor dim_top t_top) -> dim_top =
    inl f x = get_dim h x
    inl check x out =
        optionm.iter (fun x' => assert (x = f x') "The dimensions coming from the input and the dimensions of the output have to match.") out
        x
    memoize h (function
        | Map(exists a. _, a, out) => check (f a) out
        | RowMap(exists a. _, a, out) => check (f a) out
        | RowReduce(exists a. _, a, out) => check (fst (f a)) out
        | Zip(a,b) =>
            inl a = f a
            inl b = f b
            assert (a = b) "The dimensions of the two inputs to the Zip node have to be equal."
            a
        | Matmul(a,b) =>
            inl (m,k as a) = f a
            inl (n,k') = f b
            assert (k = k') "The inner dimensions of the matrix multiplication node have to be equal."
            m,n
        | Weight dim => dim
        | Input (_, dim) => dim
        | Apply(exists a. a,b) => snd (f a)
        )

// Calculates the parameter offsets.
inl pass_offset_param forall t_top. (x : graph t_top) (dims : graph_dims) : graph_offset =
    inl h_offset_param = hashmapm.create() // stores the offsets pointing to the start of the weight parameter array for each node
    inl h = hashmapm.create()
    inl get_dim x = get_dim dims x
    inl rec g forall t. (offset : size) (out : option (graph t)) : size =
        match out with
        | Some x' => f offset x'
        | None => offset
    and inl f forall t. (offset : size) : graph t -> size =
        memoize h function
            | Map(exists a. _,a,out) => g (f offset a) out
            | RowMap(exists a. _,a,out) => g (f offset a) out
            | RowReduce(exists a. _,a,out) => g (f offset a) out
            | Zip(a,b) => f (f offset a) b
            | Pair(a,b) => f (f offset a) b
            | Matmul(a,b) => f (f offset a) b
            | Weight as k =>
                inl dim = get_dim k
                inl partition_offsets : _ t = partitionm.to_partition dim |> partitionm.calculate_offsets offset 
                hashmapm.add h_offset_param k partition_offsets
                partition_offsets.offset_end
            | Apply(exists a. a,b) => f (f offset a) b
            | InputScalar => offset
            | Input => offset
    inl offset = f 0 x
    hashmapm.set_immutable(h_offset_param)
    h_offset_param, offset

// Calculates the output node offsets.
inl pass_offset_output forall t_top. (x : graph t_top) (dims : graph_dims) : graph_offset =
    inl h_offset_output = hashmapm.create() // stores the offsets pointing to the start of the output array for each node
    inl h = hashmapm.create()
    inl get_dim x = get_dim dims x
    inl rec g forall dim t. (offset : size) (k : graph (tensor dim t)) (out : option (graph (tensor dim t))) : size =
        match out with
        | Some x => f offset x
        | None =>
            inl partition_offsets : _ (_ _ t) = partitionm.to_partition (get_dim k) |> partitionm.calculate_offsets offset
            hashmapm.add h_offset_output k partition_offsets
            partition_offsets.offset_end
    and inl f forall t. (offset : size) : graph t -> size =
        memoize h fun k =>
            match k with
            | Map(exists a. _,a,out) => g (f offset a) k out
            | RowMap(exists a. _,a,out) => g (f offset a) k out
            | RowReduce(exists a. _,a,out) => g (f offset a) k out
            | Zip(a,b) => f (f offset a) b // passthrough
            | Pair(a,b) => f (f offset a) b // passthrough
            | Matmul(a,b) => g (f (f offset a) b) k None
            | Weight => offset
            | Apply(exists a. a,b) => f (f offset a) b // passthrough
            | Input => g offset k None
            | InputScalar => offset
    inl offset = f 0 x
    hashmapm.set_immutable(h_offset_output)
    h_offset_output, offset

// Calculates the max of dynamic shared memory used for all the nodes.
inl pass_calculate_dynamic_shared_memory forall t_top. (x : graph t_top) =
    inl h = hashmapm.create()
    inl rec g forall t. (size : size) (x : option (graph t)) : size =
        match x with
        | None => size
        | Some x => max size (f x)
    and inl f forall t. : graph t -> size =
        memoize h function
            | Map(exists a. _,a,out) => g (f a) out
            | RowMap(exists a. _,a,out) => g (f a) out
            | RowReduce(exists a. _,a,out) => g (f a) out
            | Zip(a,b) => max (f a) (f b)
            | Pair(a,b) => max (f a) (f b)
            | Matmul(a,b) => 
                inl memory : size =
                    inl tf32 () : size = fst(matmul.matmul_tf32' false true).offset_end
                    real
                        typecase t with
                        | tensor (int * int) f32 => tf32()
                        | _ => 
                            !!!!PrintStatic(`t)
                            error_type `(()) "The type is not supported in the matrix multiply node. Only f32 is supported at the moment. Check the terminal to see which type was being passed in."
                max (f a) (f b) |> max memory
            | Apply(exists a. a,b) => max (f a) (f b)
            | InputScalar => 0
            | Weight => 0
            | Input => 0
    f x

// Initializes the parameters of a NN on the host.
inl param_init forall t_top. (model {graph param} : model t_top) =
    assert (snd param.array = snd param.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."
    inl h = hashmapm.create()
    inl get_tensor forall dim t. (k : graph (tensor dim t)) : tensor dim t = partitionm.from_partition_offsets param.array (hashmapm.get (fst param.offset) k)
    inl rec g forall t. : option (graph t) -> () = function
        | None => ()
        | Some x => f x
    and inl f forall t. : graph t -> () =
        memoize h function
            | Map(exists a. _,a,out)  => f a . g out
            | RowMap(exists a. _,a,out) => f a . g out
            | RowReduce(exists a. _,a,out) => f a . g out
            | Zip(a,b) => f a . f b
            | Pair(a,b) => f a . f b
            | Matmul(a,b) => f a . f b
            | Weight as x =>
                inl x = get_tensor x
                cupy.copy_to {from=cupy.random_normal{mean=0; std=1} x.dim; to=x} // TODO: Don't forget to init the layers properly.
            | Apply(exists a. a,b) => f a . f b
            | InputScalar => ()
            | Input => ()
    f graph

// Prints all the weight layers.
inl param_print forall t_top. (model {graph param} : model t_top) =
    assert (snd param.array = snd param.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."
    inl h = hashmapm.create()
    inl get_tensor forall dim t. (k : graph (tensor dim t)) : tensor dim t = partitionm.from_partition_offsets param.array (hashmapm.get (fst param.offset) k)
    inl rec g forall t. : option (graph t) -> () = function
        | None => ()
        | Some x => f x
    and inl f forall t. : graph t -> () =
        memoize h function
            | Map(exists a. _,a,out)  => f a . g out
            | RowMap(exists a. _,a,out) => f a . g out
            | RowReduce(exists a. _,a,out) => f a . g out
            | Zip(a,b) => f a . f b
            | Pair(a,b) => f a . f b
            | Matmul(a,b) => f a . f b
            | Weight as x => get_tensor x |> console.write_ln
            | Apply(exists a. a,b) => f a . f b
            | InputScalar => ()
            | Input => ()
    f graph

// Allocates an array given the graph offset.
inl create_graph_data (offset : graph_offset) : graph_data =
    inl array = partitionm.create_array (snd offset)
    {array offset}

// Extracts the input tensor given the key.
inl input_extract forall t_top key{symbol} dim input. (model {graph output} : model t_top) (_ : key) : tensor dim input =
    assert (snd output.array = snd output.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."
    inl h = hashmapm.create()
    inl get_tensor forall dim' t'. (k : graph (tensor dim' t')) : tensor dim' t' = partitionm.from_partition_offsets output.array (hashmapm.get (fst output.offset) k)
    inl rec g forall t. (x : option (tensor dim input)) (out : option (graph t)) : option (tensor dim input) =
        match x with
        | Some => x
        | None =>
            match out with
            | Some x => f x
            | None => None
    and inl pair forall a b. (a : graph a) (b : graph b) : option (tensor dim input) =
        match f a with
        | None => f b
        | a => a
    and inl f forall t. : graph t -> option (tensor dim input) =
        memoize h function
            | Map(exists a. _,a,out) => g (f a) out
            | RowMap(exists a. _,a,out) => g (f a) out
            | RowReduce(exists a. _,a,out) => g (f a) out
            | Matmul(a,b) => pair a b
            | Apply(exists a. a,b) => pair a b
            | Zip(a,b) => pair a b
            | Pair(a,b) => pair a b
            | Weight _ => None
            | Input((exists key'. _), _) as x =>
                if (real open real_core in `key `= `key') then // `= operator compares the two types for equality.
                    Some (get_tensor x |> nominal_recreate)
                else None
            | InputScalar => None
    match f graph with
    | Some x => x
    | None => error_type "Cannot find the input tensor with the given key."

// Extracts the tensor at the top level node.
// Will give a type error on passthrough nodes like Zip and Apply.
inl graph_tensor_extract forall t' indices. (model {graph param output} : model t') (indices : indices) : t' =
    inl rec loop forall t. (graph : graph t) : t =
        match graph with
        | Zip(a,b) => zip (loop a) (loop b)
        | Pair(a,b) => loop a, loop b
        | Apply(exists a. a,b) => apply (loop b) (loop a)
        | InputScalar(exists k. k) => real indices k : t
        | Map(exists a. _,_,Some out) => loop out
        | RowMap(exists a. _,_,Some out) => loop out
        | RowReduce(exists a. _,_,Some out) => loop out
        | _ =>
            match hashmapm.try_get (fst param.offset) graph with
            | Some x => partitionm.from_partition_offsets param.array x
            | None =>
                match hashmapm.try_get (fst output.offset) graph with
                | Some x => partitionm.from_partition_offsets output.array x
                | None => 
                    print_static graph
                    error_type "Cannot find the offset in the dictionaries. Check the output in the terminal to see which node it is."
    loop graph

// Runs the graph and returns the result in the top level node. Is intended to be used in device code.
// Does not do any safety checking and is intended for internal use in other graph_run functions.
inl graph_run forall t_top indices. (model ({graph param output} & m) : model t_top) (ls : layer_state) (indices : indices) : t_top =
    inl h = hashmapm.create()
    inl tensor_extract graph = graph_tensor_extract (model {m with graph}) indices
    inl rec f forall t. : graph t -> t =
        open primitives
        memoize h fun x =>
            match x with
            | Map(exists a. g,a,_) =>
                inl a, out = f a, tensor_extract x
                inl g = g ls
                join map g a out
                out
            | RowMap(exists a. g,a,_) =>
                inl a, out = f a, tensor_extract x
                inl g = g ls
                join row_map g a out
                out
            | RowReduce(exists a. g,a,_) =>
                inl a, out = f a, tensor_extract x
                inl g = g ls
                join row_reduce g a out
                out
            | Matmul(a,b) =>
                inl a,b,out = f a, f b, tensor_extract x
                inl alpha, beta : float * float = 1, 0
                real
                    open real_core
                    typecase t with
                    | tensor (int * int) f32 =>
                        inl memory, body = matmul.matmul_tf32' false true
                        join body alpha a b beta out
                    | _ => 
                        !!!!PrintStatic(`t)
                        error_type "The type is not supported in the matrix multiply node. Only f32 is supported at the moment. Check the terminal to see which type was being passed in."
                out
            // These need to be separate otherwise the typecase inserted by the inference will be overly specific 
            // and we'll get a typecase miss during partial evaluation.
            | Zip(a,b) => zip (f a) (f b)
            | Pair(a,b) => f a, f b
            | Apply(exists a. a,b) => apply (f b) (f a)
            | Weight => tensor_extract x
            | Input => tensor_extract x
            | InputScalar => tensor_extract x
    f graph

// Runs the kernel on the device.
inl graph_run_device forall t_top indices. (model ({graph param output} & m) : model t_top) (indices : indices) =
    assert (snd param.array = snd param.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."
    assert (snd output.array = snd output.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."

    inl ls : layer_state =
        open random
        layer_state {
            rng = init {seed = clock64(); subsequence = conv rangem.threads_in_grid().from; offset=0}
        }

    inl _ = graph_run (model m) ls indices
    ()

// Just for testing purposes.
inl graph_run_host forall t_top sizes. (model {graph} & m : model t_top) (sizes : sizes) =
    run' {shared_mem=conv (pass_calculate_dynamic_shared_memory graph)} fun () =>
        loop.linear sizes fun indices =>
            graph_run_device m indices

inl tanh forall dim t{float}. (x : graph (tensor dim t)) : graph (tensor dim t) = Map (exists const tanh, x, None)
inl sigmoid forall dim t{float;number}. (x : graph (tensor dim t)) : graph (tensor dim t) = Map (exists const sigmoid, x, None)
inl relu forall dim t{float;number}. (x : graph (tensor dim t)) : graph (tensor dim t) = Map (exists const (max 0), x, None)
inl softmax' forall t{float;number}. out (x : graph (tensor (int * int) t)) : graph (tensor (int * int) t) = 
    RowMap (exists (fun _ config x _ _ => primitives.local_softmax config x), x, Some out)
inl softmax forall t{float;number}. (x : graph (tensor (int * int) t)) : graph (tensor (int * int) t) = 
    RowMap (exists (fun _ config x _ _ => primitives.local_softmax config x), x, None)
inl ln_l2 forall t{float;number}. (x : graph (tensor (int * int) t)) : graph (tensor (int * int) t) = 
    RowMap (exists (fun _ config x _ _ => primitives.local_ln_l2 config x), x, None)
inl argmax forall t{number}. (x : graph (tensor (int * int) t)) : graph (tensor int int) = RowReduce (exists (fun _ config x _ j_tns => primitives.local_argmax config x j_tns), x, None)
inl discrete_sample' forall t{float;number}. out (x : graph (tensor (int * int) t)) : graph (tensor int int) = 
    inl f (layer_state {rng}) config x i j_tns = primitives.local_discrete_sampling rng config x i j_tns
    RowReduce (exists f, x, Some out)
inl discrete_sample forall t{float;number}. (x : graph (tensor (int * int) t)) : graph (tensor int int) = 
    inl f (layer_state {rng}) config x i j_tns = primitives.local_discrete_sampling rng config x i j_tns
    RowReduce (exists f, x, None)
inl softmax_and_discrete_sample' forall t{float;number}. out (x : graph (tensor (int * int) t)) : graph (tensor int int) = 
    inl f (layer_state {rng}) config x i j_tns = primitives.local_softmax_and_discrete_sampling rng config x i j_tns
    RowReduce (exists f, x, Some out)
inl softmax_and_discrete_sample forall t{float;number}. (x : graph (tensor (int * int) t)) : graph (tensor int int) = 
    inl f (layer_state {rng}) config x i j_tns = primitives.local_softmax_and_discrete_sampling rng config x i j_tns
    RowReduce (exists f, x, None)
inl matmul (b,a) x = Matmul(x, Weight (a,b))
inl input_scalar forall t key{symbol}. (key : key) : graph t = InputScalar(exists key)
inl matmul_ensemble (c,b,a : int * int * int) x = Matmul(x, Apply(exists Weight (c,a,b), input_scalar .ensemble))
inl input forall dim t key{symbol}. (key : key) (dim : dim) : graph (tensor dim t) = Input((exists key), dim)
inl apply forall a b t. (a : graph (tensor (a * b) t)) (b : graph a) : graph (tensor b t) = Apply(exists a,b)
inl pair forall a b. (a : graph a) (b : graph b) : graph (a * b) = Pair(a,b)

inl create_model forall t. (graph : graph t) =
    inl dims = hashmapm.create()
    inl param = create_graph_data (pass_offset_param graph dims)
    inl output = create_graph_data (pass_offset_output graph dims)
    hashmapm.set_immutable(dims)
    model {graph param output dims}

// Tests whether randonmly initializing the params of the graph works.
inl test1() =
    inl graph : graph (tensor (int * int) float) =
        input .input (1,4)
        |> matmul (4,4)
        |> tanh
        |> matmul (4,4)
        |> tanh
        |> matmul (4,2)
        |> tanh

    inl model = create_model graph
    
    console.write_ln "---"
    param_print model
    param_init model
    console.write_ln "Done initing."
    param_print model
    ()

// Tests whether extracting the inputs of the graph works. Also randomly inits them.
inl test2() =
    inl graph : graph (tensor (int * int) float) =
        input .input (1,2)
        |> matmul (2,4)
        |> tanh
        |> matmul (4,4)
        |> tanh
        |> matmul (4,2)
        |> tanh

    inl model = create_model graph
    console.write_ln "---"
    param_init model
    console.write_ln "Here are the weight matrices."
    param_print model
    console.write_ln "Here is the input tensor."
    inl input : tensor (int * int) float = input_extract model .input
    cupy.copy_to {
        from = cupy.random_normal{mean=0; std=1} input.dim
        to = input
    }
    console.write_ln input
    ()

// Tests running a feedforward model.
inl test3() =
    inl graph : graph (tensor int int) =
        (input .input (16,8) : graph (tensor _ float))
        |> matmul (8,16)
        |> ln_l2
        |> relu
        |> matmul (16,16)
        |> ln_l2
        |> relu
        |> matmul (16,16)
        |> softmax_and_discrete_sample

    inl model = create_model graph
    param_init model
    console.write_ln "Here are the weight matrices."
    param_print model
    inl input : tensor (int * int) float = input_extract model .input
    // Immitates the passing of data into the output vector.
    // In the poker game the serialization function will be responsible for this.
    cupy.copy_to {
        from = cupy.random_normal{mean=0; std=1} input.dim
        to = input
    }
    console.write_ln input
    console.write_ln "Here is the output tensor."
    graph_run_host model {}
    inl tns_output = graph_tensor_extract model {}
    console.write_ln tns_output
    console.write_ln "===="
    ()

// Tests running a feedforward ensemble model.
inl test4() =
    inl ensemble = 16
    inl graph : graph (tensor int int) =
        (input .input (16,8) : graph (tensor _ float))
        |> matmul_ensemble (ensemble,8,16)
        |> ln_l2
        |> relu
        |> matmul_ensemble (ensemble,16,16)
        |> ln_l2
        |> relu
        |> matmul_ensemble (ensemble,16,16)
        |> softmax_and_discrete_sample' (apply (input .output (ensemble,16)) (input_scalar .ensemble))

    inl model = create_model graph
    param_init model
    console.write_ln "Here are the weight matrices."
    param_print model
    inl input : tensor (int * int) float = input_extract model .input
    // Immitates the passing of data into the output vector.
    // In the poker game the serialization function will be responsible for this.
    cupy.copy_to {
        from = cupy.random_normal{mean=0; std=1} input.dim
        to = input
    }
    console.write_ln input
    console.write_ln "Here is the output tensor."
    graph_run_host model {ensemble}
    inl output : tensor (int * int) int = input_extract model .output
    printing.tensor_print_ln limit.max output
    ()

// Tests running a feedforward ensemble model which keeps around the path probabilities.
inl test5() =
    inl ensemble : int = 16
    inl graph : graph (tensor (i32 * i32) f32 * tensor i32 i32) =
        inl x =
            (input .input (16,8) : graph (tensor _ float))
            |> matmul_ensemble (ensemble,8,16)
            |> ln_l2
            |> relu
            |> matmul_ensemble (ensemble,16,16)
            |> ln_l2
            |> relu
            |> matmul_ensemble (ensemble,16,16)
            |> softmax' (apply (input .output_probs (ensemble,16,16)) (input_scalar .ensemble))
        pair x (discrete_sample' (apply (input .output_indices (ensemble,16)) (input_scalar .ensemble)) x)

    inl model = create_model graph
    param_init model
    console.write_ln "Here are the weight matrices."
    param_print model
    inl input : tensor (int * int) float = input_extract model .input
    // Immitates the passing of data into the output vector.
    // In the poker game the serialization function will be responsible for this.
    cupy.copy_to {
        from = cupy.random_normal{mean=0; std=1} input.dim
        to = input
    }
    graph_run_host model {ensemble}
    console.write_ln "Here is the input tensor."
    console.write_ln input
    console.write_ln "Here is the output tensor."
    inl output : tensor (int * int * int) float = input_extract model .output_probs
    printing.tensor_print_ln (1 <<< 9) output
    inl output : tensor (int * int) int = input_extract model .output_indices
    printing.tensor_print_ln limit.max output
inl main() = test5()