open corebase
open corecuda
open coreext
open tensorm

type space_id = int
inl weight_id() : space_id = 0
inl output_id() : space_id = 1
inl weight_adjoint_id() : space_id = 2
inl num_spaces() : space_id = 3

type graph_dims = compile_time.hashmap
type graph_partitions = list {partition : compile_time.hashmap; size : size}
type graph_data = list {array : ptr; partition : compile_time.hashmap; size : size}

nominal layer_state = 
    {
        rng : ref random.philox_state
    }

union rec graph t =
    | BlockMap :: forall dim t. 
        (exists a. (layer_state -> a -> t) 
            * graph (tensor (int * dim) a) 
            * option (graph (tensor (int * dim) t))) 
        -> graph (tensor (int * dim) t)
    | BlockRowMap :: forall dim t.
        (exists a. (layer_state -> primitives.row_config -> tensor (int * int) a -> dim -> tensor (int * int) int -> tensor (int * int) t) 
            * graph (tensor (int * dim * int) a)
            * option (graph (tensor (int * dim * int) t)))
        -> graph (tensor (int * dim * int) t)
    | BlockRowReduce :: forall dim t.
        (exists a. (layer_state -> primitives.row_config -> tensor (int * int) a -> dim -> tensor (int * int) int -> t) 
            * graph (tensor (int * dim * int) a)
            * option (graph (tensor (int * dim) t)))
        -> graph (tensor (int * dim) t)
    | BlockRowMapReduce :: forall dim out_a out_b.
        (exists a. (layer_state -> primitives.row_config -> tensor (int * int) a -> dim -> tensor (int * int) int -> tensor (int * int) out_a * out_b) 
            * graph (tensor (int * dim * int) a)
            * option (graph (tensor (int * dim * int) out_a * tensor (int * dim) out_b)))
        -> graph (tensor (int * dim * int) out_a * tensor (int * dim) out_b)
    | BlockMatmul :: forall t. 
        graph (tensor (int * int * int) t) * graph (tensor (int * int) t) 
        -> graph (tensor (int * int * int) t)
    | Zip :: forall dim a b. graph (tensor dim a) * graph (tensor dim b) -> graph (tensor dim (a * b))
    | Pair :: forall a b. graph a * graph b -> graph (a * b)
    | Layer :: forall t. space_id * partitionm.partition t * (t -> ()) -> graph t
    | Apply :: forall b el. (exists a. graph (tensor (a * b) el) * graph a) -> graph (tensor b el)
    | KeyGraph :: forall t. (exists key{symbol}. key) * graph t -> graph t
    | KeyScalar :: forall t. (exists key{symbol}. key) -> graph t
union rec dim_of t =
    | D_D :: forall dim t. dim -> dim_of (tensor dim t)
    | D_P :: forall a b. dim_of a * dim_of b -> dim_of (a * b)

nominal model t =
    {
        graph : graph t
        dims : graph_dims
        data : graph_data
    }

inl memoize (h : compile_time.hashmap) f k =
    match compile_time.hashmapm.try_get h k with
    | Some v => v
    | None => inl v = f k in compile_time.hashmapm.add h k v . v

// Returns the top level dimension for an arbitrary graph type. Takes in a hashmap for memoization purposes.
// Results in a type errors error on the KeyScalar node.
inl rec get_dim_pair forall t_top. (h : graph_dims) : graph t_top -> dim_of t_top =
    inl f x = get_dim h x
    inl f' x = get_dim_pair h x
    inl check x = optionm.iter (fun x' => assert (D_D x = f' x') "The dimensions coming from the input and the dimensions of the output have to match.")
    memoize h (function
        | BlockRowMapReduce(exists a. _, a, out) =>
            inl q,w,e = f a
            assert (q = blocks_per_grid()) "The batch dimension in the row reduce node needs to equal the number of blocks per grid."
            out |> optionm.iter (fun out => 
                inl (D_P(D_D a, D_D b)) = f' out
                assert ((q,w,e) = a) "The dimensions coming from the input and the dimensions of the first output have to match."
                assert ((q,w) = b) "The dimensions coming from the input and the dimensions of the second output have to match."
                )
            D_P(D_D(q,w,e),D_D(q,w))
        | Pair(a,b) => D_P(f' a, f' b)
        | BlockMap(exists a. _, a, out) => 
            inl a = f a
            assert (fst a = blocks_per_grid()) "The batch dimension in the map node needs to equal the number of blocks per grid."
            check a out
            D_D a
        | BlockRowMap(exists a. _, a, out) => 
            inl a = f a
            assert (fst a = blocks_per_grid()) "The batch dimension in the row map node needs to equal the number of blocks per grid."
            check a out
            D_D a
        | BlockRowReduce(exists a. _, a, out) => 
            inl q,w,_ = f a
            assert (q = blocks_per_grid()) "The batch dimension in the row reduce node needs to equal the number of blocks per grid."
            check (q,w) out
            D_D(q,w)
        | BlockMatmul(a,b) =>
            inl (q,m,k as a) = f a
            assert (q = blocks_per_grid()) "The batch dimension in the matmul node needs to equal the number of blocks per grid."
            inl (n,k') = f b
            assert (k = k') "The inner dimensions of the matrix multiplication node have to be equal."
            D_D(q,m,n)
        | Zip(a,b) =>
            inl a,b = f a,f b
            assert (a = b) "The dimensions of the two inputs to the Zip node have to be equal."
            D_D a
        | Layer(sid, dim, init) => real
            typecase `dim with
            | _ (tensor ~dim ~t) => D_D `dim `t (partitionm.dim_of `dim `t dim)
            | _ ~p => 
                !!!!PrintStatic(`p)
                error_type "Can only get the dimension of a singleton layer. Check the terminal to see the type of the Layer node."
        | KeyGraph(_,x) => f' x
        | Apply(exists a. a,b) => D_D(snd (f a))
        )
and inl get_dim forall dim t. (h : graph_dims) (x : graph (tensor dim t)) : dim = match get_dim_pair h x with D_D x => x

inl pass_calculate_partitions forall t_top. (x : graph t_top) (dims : graph_dims) : graph_partitions =
    inl h_data = listm.init num_spaces() fun _ => compile_time.hashmapm.create()
    inl offset_key = .offset

    inl h = compile_time.hashmapm.create()
    inl get_offset' h_data : size = match compile_time.hashmapm.try_get h_data offset_key with Some x => x | None => 0
    inl get_offset (sid : space_id) : size = get_offset' (index h_data sid)
    inl set_offset (sid : space_id) (offset : size) : () = compile_time.hashmapm.set (index h_data sid) offset_key offset
    inl get_dim x = get_dim dims x
    inl get_dim_pair x = get_dim_pair dims x
    inl rec g forall t. (k : graph t) (out : option (graph t)) : () =
        match out with
        | Some x => f x
        | None =>
            inl sid = output_id()
            inl rec loop forall t'. (x : dim_of t') : partition_offsets t' =
                match x with
                | D_P(a,b) => open partitionm in loop a +. loop b
                | D_D dim =>
                    inl partition_offsets = partitionm.to_partition dim |> partitionm.calculate_offsets get_offset(sid)
                    set_offset sid partition_offsets.offset_end
                    partition_offsets
            compile_time.hashmapm.add (index h_data sid) k (loop (get_dim_pair k))
    and inl f forall t. : graph t -> () =
        memoize h fun k =>
            match k with
            | BlockMap(exists a. _,a,out) => f a . g k out
            | BlockRowMap(exists a. _,a,out) => f a . g k out
            | BlockRowReduce(exists a. _,a,out) => f a . g k out
            | BlockRowMapReduce(exists a. _,a,out) => f a . g k out
            | BlockMatmul(a,b) => f a . f b . g k None
            | Zip(a,b) => f a . f b // passthrough
            | Pair(a,b) => f a . f b // passthrough
            | Apply(exists a. a,b) => f a . f b // passthrough
            | Layer(sid,p,_) as k =>
                inl partition_offsets : _ t = p |> partitionm.calculate_offsets get_offset(sid)
                compile_time.hashmapm.add (index h_data sid) k partition_offsets
                set_offset sid partition_offsets.offset_end
            | Apply(exists a. a,b) => f a . f b
            | KeyGraph(_,x) => f x
            | KeyScalar => ()
    f x
    listm.iter compile_time.hashmapm.set_immutable h_data
    listm.map (fun h => {partition=h; size=get_offset' h}) h_data

// Initializes the parameters of a NN on the host.
inl pass_init forall t_top. (model {graph data} : model t_top) =
    inl h = compile_time.hashmapm.create()
    inl rec g forall t. : option (graph t) -> () = function
        | None => ()
        | Some x => f x
    and inl f forall t. : graph t -> () =
        memoize h function
            | BlockMap(exists a. _,a,out)  => f a . g out
            | BlockRowMap(exists a. _,a,out) => f a . g out
            | BlockRowReduce(exists a. _,a,out) => f a . g out
            | BlockRowMapReduce(exists a. _,a,out) => f a . g out
            | BlockMatmul(a,b) => f a . f b
            | Zip(a,b) => f a . f b
            | Pair(a,b) => f a . f b
            | Layer(sid, _, init) as k => 
                inl x = index data sid
                partitionm.from_partition_offsets (x.array, x.size) (compile_time.hashmapm.get x.partition k) |> init . ()
            | Apply(exists a. a,b) => f a . f b
            | KeyGraph(_,x) => f x
            | KeyScalar => ()
    f graph

// Prints all the weight layers.
inl param_print forall t_top. (model {graph data} : model t_top) =
    inl h = compile_time.hashmapm.create()
    inl h_data = index data weight_id()
    inl get_partition forall t. (k : graph t) : t = partitionm.from_partition_offsets (h_data.array, h_data.size) (compile_time.hashmapm.get h_data.partition k)
    inl rec g forall t. : option (graph t) -> () = function
        | None => ()
        | Some x => f x
    and inl f forall t. : graph t -> () =
        memoize h function
            | BlockMap(exists a. _,a,out)  => f a . g out
            | BlockRowMap(exists a. _,a,out) => f a . g out
            | BlockRowReduce(exists a. _,a,out) => f a . g out
            | BlockRowMapReduce(exists a. _,a,out) => f a . g out
            | BlockMatmul(a,b) => f a . f b
            | Zip(a,b) => f a . f b
            | Pair(a,b) => f a . f b
            | Layer(sid,_,_) as x => if sid = weight_id() then get_partition x |> console.write_ln
            | Apply(exists a. a,b) => f a . f b
            | KeyGraph(_,x) => f x
            | KeyScalar => ()
    f graph

// Allocates an array given the graph offset.
inl create_graph_data (offset : graph_partitions) : graph_data =
    offset |> listm.map fun x =>
        inl array,size = partitionm.create_array x.size
        assert (size >= x.size) "The created array has to be equal or greater to the given size."
        {x with array}

// Extracts the tensor at the top level node.
inl graph_extract forall t_top indices. (model {graph data} : model t_top) (indices : indices) : t_top =
    inl rec loop forall t. (graph : graph t) : t =
        match graph with
        | BlockMap(exists a. _,_,Some out) => loop out
        | BlockRowMap(exists a. _,_,Some out) => loop out
        | BlockRowReduce(exists a. _,_,Some out) => loop out
        | BlockRowMapReduce(exists a. _,_,Some out) => loop out
        | Zip(a,b) => zip (loop a) (loop b)
        | Pair(a,b) => loop a, loop b
        | Apply(exists a. a,b) => apply (loop b) (loop a)
        | KeyScalar(exists k. k) => real 
            match indices with
            | {$k=x} => x : t
            | _ => real_core.error_type "Cannot extract the node for the graph due to the missing key."
        | KeyGraph(_,x) => loop x
        | _ =>
            data |> listm.chooseFirst (fun x => 
                compile_time.hashmapm.try_get x.partition graph 
                |> optionm.map (partitionm.from_partition_offsets (x.array, x.size))
                )
            |> function
            | Some x => x
            | None => 
                print_static graph
                error_type "Cannot find the offset in the dictionaries. Check the output in the terminal to see which node it is."
    loop graph

// Extracts the input tensor from the Key node given the key.
inl key_extract forall t_top key{symbol} input. (model ({graph data} & m) : model t_top) (_ : key) : input =
    inl h = compile_time.hashmapm.create()
    inl rec g forall t. (x : option input) (out : option (graph t)) : option input =
        match x with
        | Some => x
        | None =>
            match out with
            | Some x => f x
            | None => None
    and inl pair forall a b. (a : graph a) (b : graph b) : option input =
        match f a with
        | None => f b
        | a => a
    and inl f forall t. : graph t -> option input =
        memoize h function
            | BlockMap(exists a. _,a,out) => g (f a) out
            | BlockRowMap(exists a. _,a,out) => g (f a) out
            | BlockRowReduce(exists a. _,a,out) => g (f a) out
            | BlockRowMapReduce(exists a. _,a,out) => g (f a) out
            | BlockMatmul(a,b) => pair a b
            | Apply(exists a. a,b) => pair a b
            | Zip(a,b) => pair a b
            | Pair(a,b) => pair a b
            | Layer => None
            | KeyScalar => None
            | KeyGraph((exists key'. _),graph) =>
                if (real open real_core in `key `= `key') then // `= operator compares the two types for equality.
                    inl g = graph_extract (model {m with graph}) {}
                    Some (real g : input)
                else f graph
    match f graph with
    | Some x => x
    | None => error_type "Cannot find the input tensor with the given key."

// Runs the graph and returns the result in the top level node. Is intended to be used in device code.
// Does not do any safety checking and is intended for internal use in other graph_run functions.
// Inserts the block index into the indices variable automatically.
inl graph_run forall t_top indices. (model ({graph data} & m) : model t_top) (ls : layer_state) (indices : indices) : t_top =
    inl h = compile_time.hashmapm.create()
    inl abi t = apply block_index() t
    inl tensor_extract forall t. (graph : graph t) : t = graph_extract (model {m with graph}) indices
    inl rec f forall t. : graph t -> t =
        open primitives
        memoize h fun x =>
            match x with
            | BlockMap(exists a. g,a,_) =>
                inl a, out = f a, tensor_extract x
                inl g = g ls
                join map g (abi a) (abi out)
                out
            | BlockRowMap(exists a. g,a,_) =>
                inl a, out = f a, tensor_extract x
                inl g = g ls
                join row_map g (abi a) (abi out)
                out
            | BlockRowReduce(exists a. g,a,_) =>
                inl a, out = f a, tensor_extract x
                inl g = g ls
                join row_reduce g (abi a) (abi out)
                out
            | BlockRowMapReduce(exists a. g,a,_) =>
                inl a, (out_map, out_reduce as out) = f a, tensor_extract x
                inl g = g ls
                join row_map_reduce g (abi a) (abi out_map) (abi out_reduce)
                out
            | BlockMatmul(a,b) =>
                inl a,b,out = f a, f b, tensor_extract x
                inl alpha, beta : float * float = 1, 0
                inl () =
                    inl a, out = abi a, abi out
                    real
                        open real_core
                        typecase t with
                        | tensor _ f32 =>
                            inl noinline_graph_run_matmul () = matmul.matmul_tf32' false true alpha a b beta out
                            noinline_graph_run_matmul()
                        | _ => 
                            !!!!PrintStatic(`t)
                            error_type "The type is not supported in the matrix multiply node. Only f32 is supported at the moment. Check the terminal to see which type was being passed in."
                out
            // These need to be separate otherwise the typecase inserted by the inference will be overly specific 
            // and we'll get a typecase miss during partial evaluation.
            | Zip(a,b) => zip (f a) (f b)
            | Pair(a,b) => f a, f b
            | Apply(exists a. a,b) => apply (f b) (f a)
            | Layer => tensor_extract x
            | KeyGraph => tensor_extract x
            | KeyScalar => tensor_extract x
    f graph

// Runs the kernel on the device.
inl graph_run_device forall t_top indices. (model {graph data} & m : model t_top) (ls : layer_state) (indices : indices) =
    inl _ = graph_run m ls indices
    ()

// Creates the layer state.
inl create_layer_state rng =
    layer_state {
        rng
    }

// Runs the graph on the host.
inl graph_run_host forall t_top sizes. (model {graph} & m : model t_top) (sizes : sizes) =
    run' fun () =>
        inl rng = random.init {seed = cupy.constant_seed(); subsequence = conv rangem.threads_in_grid().from; offset=0}
        inl ls : layer_state = create_layer_state rng
        loop.linear sizes fun indices =>
            graph_run_device m ls indices

// TODO: Reimplement the rest of the functions so they use row_map and row_reduce instead of the graph nodes directly.
inl row_map_ forall dim a b. f out (x : graph (tensor (int * dim * int) a)) : graph (tensor (int * dim * int) b) = 
    BlockRowMap (exists f, x, out)

inl row_reduce_ forall dim a b. f out (x : graph (tensor (int * dim * int) a)) : graph (tensor (int * dim) b) = 
    BlockRowReduce (exists f, x, out)

inl tanh forall dim t{float}. (x : graph (tensor (int * dim) t)) : graph (tensor (int * dim) t) = BlockMap (exists const tanh, x, None)
inl sigmoid forall dim t{float;number}. (x : graph (tensor (int * dim) t)) : graph (tensor (int * dim) t) = BlockMap (exists const sigmoid, x, None)
inl relu forall dim t{float;number}. (x : graph (tensor (int * dim) t)) : graph (tensor (int * dim) t) = BlockMap (exists const (max 0), x, None)

inl max_ forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) t) = 
    BlockRowReduce (exists (fun _ config x _ _ => primitives.local_max config x), x, out)
inl max' forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) t) = 
    max_ (Some out) x
inl max forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) t) = 
    max_ None x

inl ln_l2 forall dim t{float;number}. (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t) = 
    BlockRowMap (exists (fun _ config x _ _ => primitives.local_ln_l2 config x), x, None)

inl argmax forall dim t{number}. (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) int) =
    BlockRowReduce (exists (fun _ config x _ j_tns => primitives.local_argmax config x j_tns), x, None)

inl softmax_ forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t) = 
    BlockRowMap (exists (fun _ config x _ _ => primitives.local_softmax config x), x, out)
inl softmax' forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t) = 
    softmax_ (Some out) x
inl softmax forall dim t{float;number}. (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t) = 
    softmax_ None x


inl discrete_sample_ forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) int) = 
    inl f (layer_state {rng}) config x i j_tns = primitives.local_discrete_sampling rng config x i j_tns
    BlockRowReduce (exists f, x, out)
inl discrete_sample' forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) int) = 
    discrete_sample_ (Some out) x
inl discrete_sample forall dim t{float;number}. (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) int) = 
    discrete_sample_ None x

inl softmax_and_discrete_sample_ forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t * tensor (int * dim) int) = 
    inl f (layer_state {rng}) config x i j_tns = primitives.local_softmax_and_discrete_sampling rng config x i j_tns
    BlockRowMapReduce (exists f, x, out)
inl softmax_and_discrete_sample' forall dim t{float;number}. out (x : graph (tensor (int * dim * int) t)) =
    softmax_and_discrete_sample_ (Some out) x
inl softmax_and_discrete_sample forall dim t{float;number}. (x : graph (tensor (int * dim * int) t)) =
    softmax_and_discrete_sample_ None x

inl masked_softmax_ forall dim t{float;number}. size out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t) = 
    BlockRowMap (exists (fun _ config x => primitives.local_masked_softmax (fun config x i j => j < size) config x), x, out)
inl masked_softmax' forall dim t{float;number}. size out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t) = 
    masked_softmax_ size (Some out) x
inl masked_softmax forall dim t{float;number}. size (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t) = 
    masked_softmax_ size None x

inl masked_softmax_and_discrete_sample_ forall dim t{float;number}. size out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim * int) t * tensor (int * dim) int) = 
    inl f (layer_state {rng}) config x i j_tns = 
        inl i = primitives.local_masked_softmax_and_discrete_sampling (fun confix x i j => j < size) rng config x i j_tns
        assert (snd i < size) "The masking requirement is violated in masked_softmax_and_discrete_sample_."
        i
    BlockRowMapReduce (exists f, x, out)
inl masked_softmax_and_discrete_sample' forall dim t{float;number}. size out (x : graph (tensor (int * dim * int) t)) =
    masked_softmax_and_discrete_sample_ size (Some out) x
inl masked_softmax_and_discrete_sample forall dim t{float;number}. size (x : graph (tensor (int * dim * int) t)) =
    masked_softmax_and_discrete_sample_ size None x

inl binarize_ forall dim t{float;number}. modulo out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) uint) = 
    BlockRowReduce (exists const (primitives.local_binarize modulo), x, out)
inl binarize forall dim t{float;number}. modulo (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) uint) =
    binarize_ modulo None x
inl binarize' forall dim t{float;number}. modulo out (x : graph (tensor (int * dim * int) t)) : graph (tensor (int * dim) uint) =
    binarize_ modulo (Some out) x

// Takes the sqrt of the product of the tensor dimensions.
inl weight_init forall dim t. (x : tensor dim t) : () =
    cupy.copy_to {from=cupy.random_normal{mean=0; std=loop.prod x.dim |> conv |> (/) 1 |> sqrt} x.dim; to=x}
inl matmul (b,a) x =
    open partitionm
    BlockMatmul(x, Layer(weight_id(), !(a,b), weight_init))
inl key_scalar forall t key{symbol}. (key : key) : graph t = KeyScalar(exists key)
inl matmul_ensemble (c,b,a : int * int * int) x =
    open partitionm
    BlockMatmul(x, Apply(exists Layer(weight_id(), !(c,a,b), weight_init), key_scalar .ensemble))
inl key_graph forall key{symbol}. (key : key) x = KeyGraph((exists key), x)
inl input forall dim' t key{symbol}. (key : key) (dim' : dim') : graph (tensor dim' t) = 
    open partitionm
    KeyGraph((exists key), Layer(output_id(), !dim', const ()))
inl apply forall key{symbol} a b t. (b : key) (a : graph (tensor (a * b) t)) : graph (tensor b t) = Apply(exists a, key_scalar b)
inl pair forall a b. (a : graph a) (b : graph b) : graph (a * b) = Pair(a,b)

inl create_model forall t. (graph : graph t) =
    inl dims = compile_time.hashmapm.create()
    inl data = create_graph_data (pass_calculate_partitions graph dims)
    compile_time.hashmapm.set_immutable(dims)
    model {graph data dims}

// The size of the array is bound to num_spaces.
nominal model_ptrs = array ptr

inl model_to_model_data (x : model _) = model_ptrs (arraym.init num_spaces() (index x.data >> fun x => x.array))
inl model_data_to_model (graph : graph _) (model_ptrs d) =
    inl dims = compile_time.hashmapm.create()
    inl data = 
        pass_calculate_partitions graph dims
        |> listm.mapi (fun i x => {x with array = index d i})
    compile_time.hashmapm.set_immutable(dims)
    model { graph dims data }

type d2 = int * int
type d3 = int * int * int
type d4 = int * int * int * int

// Tests whether randonmly initializing the params of the graph works.
inl test1() =
    cupy.set_random_seed(cupy.constant_seed())
    testing.redirect_io_into "test_text_outputs/layers/" "test1" "layers.txt" fun () =>
    inl graph : graph (tensor d3 float) =
        input .input (blocks_per_grid(),1,4)
        |> matmul (4,4)
        |> tanh
        |> matmul (4,4)
        |> tanh
        |> matmul (4,2)
        |> tanh

    inl model = create_model graph
    
    console.write_ln "---"
    param_print model
    pass_init model
    console.write_ln "Done initing."
    param_print model
    ()

// Tests whether extracting the inputs of the graph works. Also randomly inits them.
inl test2() =
    cupy.set_random_seed(cupy.constant_seed())
    testing.redirect_io_into "test_text_outputs/layers/" "test2" "layers.txt" fun () =>
    inl graph : graph (tensor d3 float) =
        input .input (blocks_per_grid(),1,2)
        |> matmul (2,4)
        |> tanh
        |> matmul (4,4)
        |> tanh
        |> matmul (4,2)
        |> tanh

    inl model = create_model graph
    console.write_ln "---"
    pass_init model
    console.write_ln "Here are the weight matrices."
    param_print model
    console.write_ln "Here is the input tensor."
    inl input : tensor d3 float = key_extract model .input
    cupy.copy_to {
        from = cupy.random_normal{mean=0; std=1} input.dim
        to = input
    }
    console.write_ln input
    ()

// Tests running a feedforward model which outputs the sampling indices.
inl test3() =
    cupy.set_random_seed(cupy.constant_seed())
    testing.redirect_io_into "test_text_outputs/layers/" "test3" "layers.txt" fun () =>
    inl graph : graph (tensor d3 float * tensor d2 int) =
        (input .input (blocks_per_grid(),64,64) : graph (tensor _ float))
        |> matmul (64,64)
        |> ln_l2
        |> relu
        |> matmul (64,64)
        |> ln_l2
        |> relu
        |> matmul (64,64)
        |> softmax_and_discrete_sample

    inl model = create_model graph
    pass_init model
    console.write_ln "Here are the weight matrices."
    param_print model
    inl input : tensor d3 float = key_extract model .input
    // Immitates the passing of data into the output vector.
    // In the poker game the serialization function will be responsible for this.
    cupy.copy_to {
        from = cupy.random_normal{mean=0; std=1} input.dim
        to = input
    }
    console.write_ln input
    console.write_ln "Here is the output tensor."
    graph_run_host model {}
    inl tns_output = graph_extract model {}
    console.write_ln tns_output
    console.write_ln "===="
    ()

// Tests running a feedforward ensemble model which keeps around the sampling probabilities.
inl test4() =
    cupy.set_random_seed(cupy.constant_seed())
    testing.redirect_io_into "test_text_outputs/layers/" "test4" "layers.txt" fun () =>
    inl ensemble = 16
    inl graph : graph (tensor d3 float * tensor d2 int) =
        (input .input (blocks_per_grid(),64,64) : graph (tensor _ float))
        |> matmul_ensemble (ensemble,64,64)
        |> ln_l2
        |> relu
        |> matmul_ensemble (ensemble,64,64)
        |> ln_l2
        |> relu
        |> matmul_ensemble (ensemble,64,64)
        |> softmax_and_discrete_sample' (
            pair (input .output_probs   (ensemble,blocks_per_grid(),64,64) |> apply .ensemble) 
                 (input .output_indices (ensemble,blocks_per_grid(),64) |> apply .ensemble)
            )

    inl model = create_model graph
    pass_init model
    console.write_ln "Here are the weight matrices."
    param_print model
    inl input : tensor d3 float = key_extract model .input
    // Immitates the passing of data into the output vector.
    // In the poker game the serialization function will be responsible for this.
    cupy.copy_to {
        from = cupy.random_normal{mean=0; std=1} input.dim
        to = input
    }
    console.write_ln input
    console.write_ln "Here is the output tensor."
    graph_run_host model {ensemble}
    inl output : tensor d3 int = key_extract model .output_indices
    console.tensor.write_ln limit.max output
    ()

// Tests running a feedforward ensemble model which keeps around the sampling probabilities with masking.
inl test5() =
    cupy.set_random_seed(cupy.constant_seed())
    testing.redirect_io_into "test_text_outputs/layers/" "test5" "layers.txt" fun () =>
    inl ensemble : int = 4
    inl graph =
        (input .input (blocks_per_grid(),64,64) : graph (tensor _ float))
        |> matmul_ensemble (ensemble,64,64)
        // |> ln_l2
        // |> relu
        // |> matmul_ensemble (ensemble,64,64)
        // |> ln_l2
        // |> relu
        // |> matmul_ensemble (ensemble,64,64)
        |> masked_softmax_and_discrete_sample' 11 (
            pair (input .output_probs   (ensemble,blocks_per_grid(),64,64) |> apply .ensemble) 
                 (input .output_indices (ensemble,blocks_per_grid(),64) |> apply .ensemble)
            )

    inl model = create_model graph
    pass_init model
    // console.write_ln "Here are the weight matrices."
    // param_print model
    inl input : tensor d3 float = key_extract model .input
    // Immitates the passing of data into the output vector.
    // In the poker game the serialization function will be responsible for this.
    cupy.copy_to {
        from = cupy.random_normal{mean=0; std=1} input.dim
        to = input
    }
    graph_run_host model {ensemble}
    // console.write_ln "Here is the input tensor."
    // console.write_ln input
    // console.write_ln "Here is the output tensor."
    // inl output : tensor d4 float = key_extract model .output_probs
    // console.tensor.write_ln limit.max output
    inl output : tensor d3 int = key_extract model .output_indices
    console.tensor.write_ln limit.max output

inl main() = 
    test1()
    test2()
    test3()
    test4()
    test5()