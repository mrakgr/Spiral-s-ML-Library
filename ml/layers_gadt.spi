open corebase
open corecuda
open tensorm
open compile_time

type ptr = partitionm.ptr
type size = partitionm.size
type graph_array = ptr * size
type graph_dims = hashmapm.hashmap
type graph_offset = hashmapm.hashmap * size
type graph_data = {array : graph_array; offset : graph_offset}

nominal layer_state = 
    {
        rng : refm.ref random.philox_state
    }

union rec graph t =
    // | Reduce :: forall t. (exists dim a. (layer_state -> a -> t) * graph (tensor dim a)) -> graph t
    | Map :: forall dim t. (exists a. (layer_state -> a -> t) * graph (tensor dim a)) -> graph (tensor dim t)
    | RowMap :: forall t. 
        (exists a. (layer_state -> primitives.row_config -> tensor (int * int) a -> int -> tensor (int * int) int -> tensor (int * int) t) * graph (tensor (int * int) a))
        -> graph (tensor (int * int) t)
    | RowReduce :: forall t. 
        (exists a. (layer_state -> primitives.row_config -> tensor (int * int) a -> int -> tensor (int * int) int -> t) * graph (tensor (int * int) a))
        -> graph (tensor int t)
    | Zip :: forall dim a b. graph (tensor dim a) * graph (tensor dim b) -> graph (tensor dim (a * b))
    | Matmul :: forall t. graph (tensor (int * int) t) * graph (tensor (int * int) t) -> graph (tensor (int * int) t)
    | Weight :: forall dim t. dim -> graph (tensor dim t)
    | Input :: forall dim t. (exists key{symbol}. key) * dim -> graph (tensor dim t)
    | Apply :: forall b el. (exists a. graph (tensor (a * b) el) * graph a) -> graph (tensor b el)
    | InputScalar :: forall a. (exists key{symbol}. key) -> graph a

nominal model t =
    {
        graph : graph t
        dims : graph_dims
        output : graph_data
        param : graph_data
    }

inl memoize (h : hashmapm.hashmap) f k =
    match hashmapm.try_get h k with
    | Some v => v
    | None => inl v = f k in hashmapm.add h k v . v

// Returns the top level dimension. Takes in a hashmap for memoization purposes.
inl rec get_dim forall dim_top t_top. (h : graph_dims) : graph (tensor dim_top t_top) -> dim_top =
    inl f x = get_dim h x
    memoize h (function
        | Map(exists a. _, a) => f a
        | RowMap(exists a. _, a) => f a
        | RowReduce(exists a. _, a) => fst (f a)
        | Zip(a,b) =>
            inl a = f a
            inl b = f b
            assert (a = b) "The dimensions of the two inputs to the Zip node have to be equal."
            a
        | Matmul(a,b) =>
            inl (m,k as a) = f a
            inl (n,k') = f b
            assert (k = k') "The inner dimensions of the matrix multiplication node have to be equal."
            m,n
        | Weight dim => dim
        | Input (_, dim) => dim
        | Apply(exists a. a,b) => snd (f a)
        | InputScalar => error_type "Not supposed to have this node in pass_dim."
        )

// Calculates the parameter offsets.
inl pass_offset_param forall t_top. (x : graph t_top) (dims : graph_dims) : graph_offset =
    inl h_offset_param = hashmapm.create() // stores the offsets pointing to the start of the weight parameter array for each node
    inl h = hashmapm.create()
    inl get_dim x = get_dim dims x
    inl rec f forall t. (offset : size) : graph t -> size =
        memoize h function
            | Map(exists a. _,a) => f offset a
            | RowMap(exists a. _,a) => f offset a
            | RowReduce(exists a. _,a) => f offset a
            | Zip(a,b) => f (f offset a) b
            | Matmul(a,b) => f (f offset a) b
            | Weight as k =>
                inl dim = get_dim k
                inl partition_offsets : _ t = partitionm.to_partition dim |> partitionm.calculate_offsets offset 
                hashmapm.add h_offset_param k partition_offsets
                partition_offsets.offset_end
            | Apply(exists a. a,b) => f (f offset a) b
            | InputScalar | Input => offset
    inl offset = f 0 x
    hashmapm.set_immutable(h_offset_param)
    h_offset_param, offset

// Calculates the output node offsets.
inl pass_offset_output forall t_top. (x : graph t_top) (dims : graph_dims) : graph_offset =
    inl h_offset_output = hashmapm.create() // stores the offsets pointing to the start of the output array for each node
    inl h = hashmapm.create()
    inl get_dim x = get_dim dims x
    inl g forall dim t. (offset : size) (k : graph (tensor dim t)) : size =
        inl partition_offsets : _ (_ _ t) = partitionm.to_partition (get_dim k) |> partitionm.calculate_offsets offset
        hashmapm.add h_offset_output k partition_offsets
        partition_offsets.offset_end
    inl rec f forall t. (offset : size) : graph t -> size =
        memoize h fun k =>
            match k with
            | Map(exists a. _,a) => g (f offset a) k
            | RowMap(exists a. _,a) => g (f offset a) k
            | RowReduce(exists a. _,a) => g (f offset a) k
            | Zip(a,b) => f (f offset a) b // passthrough
            | Matmul(a,b) => g (f (f offset a) b) k
            | Weight => offset
            | Apply(exists a. a,b) => f (f offset a) b // passthrough
            | Input => g offset k
            | InputScalar => offset
    inl offset = f 0 x
    hashmapm.set_immutable(h_offset_output)
    h_offset_output, offset

// Calculates the max of dynamic shared memory used for all the nodes.
inl pass_calculate_dynamic_shared_memory forall t_top. (x : graph t_top) =
    inl h = hashmapm.create()
    inl rec f forall t. : graph t -> size =
        memoize h function
            | Map(exists a. _,a) => f a
            | RowMap(exists a. _,a) => f a
            | RowReduce(exists a. _,a) => f a
            | Zip(a,b) => max (f a) (f b)
            | Matmul(a,b) => 
                inl memory : size =
                    inl tf32 () : size = fst(matmul.matmul_tf32' false true).offset_end
                    real
                        typecase t with
                        | tensor (int * int) f32 => tf32()
                        | _ => 
                            !!!!PrintStatic(`t)
                            error_type `(()) "The type is not supported in the matrix multiply node. Only f32 is supported at the moment. Check the terminal to see which type was being passed in."
                max (f a) (f b) |> max memory
            | Apply(exists a. a,b) => max (f a) (f b)
            | InputScalar | Weight | Input => 0
    f x

// Initializes the parameters of a NN on the host.
inl param_init forall t_top. (model {graph param} : model t_top) =
    assert (snd param.array = snd param.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."
    inl h = hashmapm.create()
    inl get_tensor forall dim t. (k : graph (tensor dim t)) : tensor dim t = partitionm.from_partition_offsets param.array (hashmapm.get (fst param.offset) k)
    inl rec f forall t. : graph t -> () =
        memoize h function
            | Map(exists a. _,a)  => f a
            | RowMap(exists a. _,a) => f a
            | RowReduce(exists a. _,a) => f a
            | Zip(a,b) => f a . f b
            | Matmul(a,b) => f a . f b
            | Weight as x =>
                inl x = get_tensor x
                cupy.copy_to {from=cupy.random_normal{mean=0; std=1} x.dim; to=x} // TODO: Don't forget to init the layers properly.
            | Apply(exists a. a,b) => f a . f b
            | InputScalar | Input => ()
    f graph

// Prints all the weight layers.
inl param_print forall t_top. (model {graph param} : model t_top) =
    assert (snd param.array = snd param.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."
    inl h = hashmapm.create()
    inl get_tensor forall dim t. (k : graph (tensor dim t)) : tensor dim t = partitionm.from_partition_offsets param.array (hashmapm.get (fst param.offset) k)
    inl rec f forall t. : graph t -> () =
        memoize h function
            | Map(exists a. _,a)  => f a
            | RowMap(exists a. _,a) => f a
            | RowReduce(exists a. _,a) => f a
            | Zip(a,b) => f a . f b
            | Matmul(a,b) => f a . f b
            | Weight as x => get_tensor x |> console.write_ln
            | Apply(exists a. a,b) => f a . f b
            | InputScalar | Input => ()
    f graph

// Allocates an array given the graph offset.
inl create_graph_data (offset : graph_offset) : graph_data =
    inl array = partitionm.create_array (snd offset)
    {array offset}

// Extracts the input tensor given the key.
inl input_extract forall t_top key{symbol} dim input. (model {graph output} : model t_top) (_ : key) : tensor dim input =
    assert (snd output.array = snd output.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."
    inl h = hashmapm.create()
    inl get_tensor forall dim' t'. (k : graph (tensor dim' t')) : tensor dim' t' = partitionm.from_partition_offsets output.array (hashmapm.get (fst output.offset) k)
    inl rec f forall t. : graph t -> option (tensor dim input) =
        memoize h function
            | Map(exists a. _,a)  => f a
            | RowMap(exists a. _,a) => f a
            | RowReduce(exists a. _,a) => f a
            | Zip(a,b) => 
                match f a with
                | None => f b
                | a => a
            | Matmul(a,b) =>
                match f a with
                | None => f b
                | a => a
            | Weight _ => None
            | Input((exists key'. _), _) as x =>
                if (real open real_core in `key `= `key') then // `= operator compares the two types for equality.
                    Some (get_tensor x |> nominal_recreate)
                else None
    match f graph with
    | Some x => x
    | None => error_type "Cannot find the input tensor with the given key."

// Extracts the tensor at the top level node.
// Will give a type error on passthrough nodes like Zip and Apply.
inl graph_tensor_extract forall t' indices. (model {graph param output} : model t') (indices : indices) : t' =
    inl rec loop forall t. (x : graph t) : t =
        match x with
        | Zip(a,b) => zip (loop a) (loop b)
        | Apply(exists a. a,b) => apply (loop b) (loop a)
        | InputScalar(exists k. k) => real indices k : t
        | _ =>
            match hashmapm.try_get (fst param.offset) graph with
            | Some x => partitionm.from_partition_offsets param.array x
            | None =>
                match hashmapm.try_get (fst output.offset) graph with
                | Some x => partitionm.from_partition_offsets output.array x
                | None => error_type "Cannot find the offset in the dictionaries"
    loop graph

// Runs the graph and returns the result in the top level node. Is intended to be used in device code.
// Does not do any safety checking and is intended for internal use in other graph_run functions.
inl graph_run forall t_top indices. (model ({graph param output} & m) : model t_top) (ls : layer_state) (indices : indices) : t_top =
    inl h = hashmapm.create()
    inl tensor_extract graph = graph_tensor_extract (model {m with graph}) indices
    inl rec f forall t. : graph t -> t =
        open primitives
        memoize h fun x =>
            match x with
            | Map(exists a. g,a) =>
                inl a, out = f a, tensor_extract x
                inl g = g ls
                join map g a out
                out
            | RowMap(exists a. g,a) =>
                inl a, out = f a, tensor_extract x
                inl g = g ls
                join row_map g a out
                out
            | RowReduce(exists a. g,a) =>
                inl a, out = f a, tensor_extract x
                inl g = g ls
                join row_reduce g a out
                out
            | Matmul(a,b) =>
                inl a,b,out = f a, f b, tensor_extract x
                inl alpha, beta : float * float = 1, 0
                real
                    open real_core
                    typecase t with
                    | tensor (int * int) f32 =>
                        inl memory, body = matmul.matmul_tf32' false true
                        join body alpha a b beta out
                    | _ => 
                        !!!!PrintStatic(`t)
                        error_type "The type is not supported in the matrix multiply node. Only f32 is supported at the moment. Check the terminal to see which type was being passed in."
                out
            // These need to be separate otherwise the typecase inserted by the inference will be overly specific 
            // and we'll get a typecase miss during partial evaluation.
            | Zip => tensor_extract x
            | Apply => tensor_extract x
            | Weight => tensor_extract x
            | Input => tensor_extract x
            | InputScalar => tensor_extract x
    f graph

// Runs the kernel on the device.
inl graph_run_device forall t_top indices. (model ({graph param output} & m) : model t_top) (indices : indices) =
    assert (snd param.array = snd param.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."
    assert (snd output.array = snd output.offset) "The sizes of both the pointer storing the data for the graph, and the graph size must be the same."

    inl ls : layer_state =
        open random
        layer_state {
            rng = init {seed = clock64(); subsequence = conv rangem.threads_in_grid().from; offset=0}
        }

    inl _ = graph_run (model m) ls indices
    ()

// Just for testing purposes.
inl graph_run_host forall t_top. (model {graph} & m : model t_top) =
    run' {shared_mem=conv (pass_calculate_dynamic_shared_memory graph)} fun () =>
        graph_run_device m {}

inl tanh forall dim t{float}. (x : graph (tensor dim t)) : graph (tensor dim t) = Map (exists const tanh, x)
inl sigmoid forall dim t{float;number}. (x : graph (tensor dim t)) : graph (tensor dim t) = Map (exists const sigmoid, x)
inl relu forall dim t{float;number}. (x : graph (tensor dim t)) : graph (tensor dim t) = Map (exists const (max 0), x)
inl softmax forall t{float;number}. (x : graph (tensor (int * int) t)) : graph (tensor (int * int) t) = 
    RowMap (exists (fun _ config x _ _ => primitives.local_softmax config x), x)
inl ln_l2 forall t{float;number}. (x : graph (tensor (int * int) t)) : graph (tensor (int * int) t) = 
    RowMap (exists (fun _ config x _ _ => primitives.local_ln_l2 config x), x)
inl argmax forall t{number}. (x : graph (tensor (int * int) t)) : graph (tensor int int) = RowReduce (exists (fun _ config x _ j_tns => primitives.local_argmax config x j_tns), x)
inl discrete_sample forall t{float;number}. (x : graph (tensor (int * int) t)) : graph (tensor int int) = 
    inl f (layer_state {rng}) config x i j_tns = primitives.local_discrete_sampling rng config x i j_tns
    RowReduce (exists f, x)
inl matmul (a,b) x = Matmul(x, Weight (b,a))
inl input forall dim t key{symbol}. (key : key) (dim : dim) : graph (tensor dim t) = Input((exists key), dim)

inl create_model forall t. (graph : graph t) =
    inl dims = hashmapm.create()
    inl param = create_graph_data (pass_offset_param graph dims)
    inl output = create_graph_data (pass_offset_output graph dims)
    hashmapm.set_immutable(dims)
    model {graph param output dims}

// Tests whether randonmly initializing the params of the graph works.
inl test1() =
    inl graph : graph (tensor (int * int) float) =
        input .input (1,4)
        |> matmul (4,4)
        |> tanh
        |> matmul (4,4)
        |> tanh
        |> matmul (4,2)
        |> tanh

    inl model = create_model graph
    
    console.write_ln "---"
    param_print model
    param_init model
    console.write_ln "Done initing."
    param_print model
    ()

// Tests whether extracting the inputs of the graph works. Also randomly inits them.
inl test2() =
    inl graph : graph (tensor (int * int) float) =
        input .input (1,2)
        |> matmul (2,4)
        |> tanh
        |> matmul (4,4)
        |> tanh
        |> matmul (4,2)
        |> tanh

    inl model = create_model graph
    console.write_ln "---"
    param_init model
    console.write_ln "Here are the weight matrices."
    param_print model
    console.write_ln "Here is the input tensor."
    inl input : tensor (int * int) float = input_extract model .input
    cupy.copy_to {
        from = cupy.random_normal{mean=0; std=1} input.dim
        to = input
    }
    console.write_ln input
    ()

// Tests whether extracting the graph can be executed.
inl test3() =
    inl graph : graph (tensor int int) =
        (input .input (16,8) : graph (tensor _ float))
        |> matmul (8,16)
        |> ln_l2
        |> relu
        |> matmul (16,16)
        |> ln_l2
        |> relu
        |> matmul (16,16)
        |> discrete_sample

    inl model = create_model graph
    param_init model
    console.write_ln "Here are the weight matrices."
    param_print model
    inl input : tensor (int * int) float = input_extract model .input
    // Immitates the passing of data into the output vector.
    // In the poker game the serialization function will be responsible for this.
    cupy.copy_to {
        from = cupy.random_normal{mean=0; std=1} input.dim
        to = input
    }
    console.write_ln input
    console.write_ln "Here is the output tensor."
    graph_run_host model
    inl tns_output = graph_tensor_extract model {}
    console.write_ln tns_output
    console.write_ln "===="
    ()

inl main() = test3()