// The Counter Factual Regret module

open corebase
open corecuda
open coreext
open tensorm

// Decays the average policy norm to this value after it exceeds it.
inl average_threeshold() : float = 100

type ensemble_id = int
type world_id = int
type player_id = int
type action_id = int
type seq_id = int
type thread_id = int
type prob = float
type log_prob = f64
type reward = float
type count = float
type log_path_prob = {sampling : log_prob; policy : log_prob}
type action_prob = {sampling : prob; policy : prob}
type trace_elem =
    {
        world_id : world_id
        action_id : action_id
        player_id : player_id
        action_sampling_prob : prob
    }
type trace =
    {
        actions : tensor (ensemble_id * seq_id * thread_id) trace_elem
        log_path_probs : tensor (ensemble_id * seq_id * thread_id * player_id) log_path_prob
    }
type trace_state =
    {
        log_path_probs : tensor (ensemble_id * thread_id * player_id) log_path_prob
        seq_id : tensor (ensemble_id * thread_id) seq_id
    }
type model =
    {
        exploratory_ensemble_id : tensor {} ensemble_id
        table : tensor (ensemble_id * world_id * action_id) {
            policy : {
                average : prob
                current : prob
                update : prob
            }
            value : {
                current : (reward * count)
                update : (reward * count)
                }
        }
    }
type sizes =
    {
        ensemble_id : ensemble_id
        world_id : world_id
        player_id : player_id
        action_id : action_id
        action_mask : int
        seq_id : seq_id
        thread_id : thread_id
    }

inl graph (size : sizes) : _ (model * trace * trace_state) =
    open partitionm
    assert (size.action_mask <= size.action_id) "The number of masked in actions (action_mask) must be less than or equal the size of the action_id tensor dimension."
    inl model : partition model = 
        inl table = !(size.ensemble_id, size.world_id, size.action_id)
        inl exploratory_ensemble_id = !{}
        exploratory_ensemble_id *. table 
        |> reorder (fun exploratory_ensemble_id, table => {exploratory_ensemble_id table})
    inl trace : partition trace = 
        !(size.ensemble_id, size.seq_id, size.thread_id) *. !(size.ensemble_id, size.seq_id, size.thread_id, size.player_id)
        |> reorder (fun (actions, log_path_probs) => { actions log_path_probs })
    inl trace_state : partition trace_state =
        !(size.ensemble_id, size.thread_id, size.player_id) *. !(size.ensemble_id, size.thread_id)
        |> reorder (fun (log_path_probs, seq_id) => {log_path_probs seq_id})
    // Initializes all the elements of the arrays to zero.
    inl init_zero forall t. (x : t) : () = real
        struct.iter (fun x =>
            struct.iter (fun x =>
                $"!(x.array)[:] = 0" : ()
                ) x.bodies
        ) x
    // Initializes all the model average to the average_threeshold divided by the size of the innermost dimension.
    inl init_average ({table} : model) : () =
        inl average = table |> rezip (fun x => x.policy.average)
        inl init_value = average_threeshold() / conv size.action_mask
        $"!(real average.bodies.array)[:] = !init_value"

    inl (<|) a b = layers.pair a b
    (layers.key_graph .cfr_model (layers.weight' model (fun x => init_zero x . init_average x)))
    <| (layers.key_graph .cfr_trace (layers.input' trace))
    <| (layers.key_graph .cfr_trace_state (layers.weight' trace_state init_zero))

inl get_prob config (action_id : action_id) (probs : tensor _ (prob * action_id)) =
    open primitives
    local_reduce config (0,limit.max) (fun a b =>
        if snd a = action_id then a
        elif snd b = action_id then b
        else a
        ) probs
    |> fun prob,i => assert (i <> limit.max) "Expected a valid action id in get_prob." . prob

inl mask_of action_mask (_ : primitives.row_config) _ (_ : int) (i : int) = i < action_mask

// Returns policy probability of an action given the ensemble_id, world_id and the action_id.
// If the first argument is true, it does that using the `policy.current` else it uses `policy.average`.
inl get_policy_probs_ is_current action_mask (model : model) ~(ensemble_id, world_id : ensemble_id * world_id) (action_id : action_id) : prob = join
    open primitives
    model.table
    |> apply ensemble_id
    |> apply world_id
    |> rezip (fun {policy} => if is_current then policy.current else policy.average)
    |> row_gather_reduce () (fun () config policy i j_tns =>
        open random
        inl policy_probs = local_masked_regret_matching (mask_of action_mask) config policy i j_tns
        get_prob config action_id (zip policy_probs j_tns)
        )

// Returns current policy probability of an action given the ensemble_id, world_id and the action_id.
inl get_policy_probs x = get_policy_probs_ true x

// Returns average policy probability of an action given the ensemble_id, world_id and the action_id.
inl get_average_probs x = get_policy_probs_ false x

// Samples an action from the model using the average policy and returns both the action and the probabilities of it being selected.
inl get_action rng action_mask (model : model) ~(ensemble_id, world_id : ensemble_id * world_id) : prob * action_id = join
    open primitives
    model.table
    |> apply ensemble_id
    |> apply world_id
    |> rezip (fun {policy} => policy.average, policy.current)
    |> row_gather_reduce rng (fun rng config x i j_tns =>
        open random
        inl sampling, policy = unzip x
        inl sampling_probs, action_id = local_masked_regret_matching_and_discrete_sampling (mask_of action_mask) rng config sampling i j_tns
        inl policy_probs = local_masked_regret_matching (mask_of action_mask) config policy i j_tns
        inl get_prob probs = get_prob config action_id (zip probs j_tns)
        get_prob sampling_probs, action_id
        )

// Sets the log_path_probs and seq_id to 0.
inl reset_trace_state (trace_state : trace_state) (ensemble_id : ensemble_id) =
    open primitives
    inl thread_id = rangem.threads_in_grid().from
    inl log_path_probs = trace_state.log_path_probs |> apply ensemble_id |> apply thread_id |> apply_ptr
    // Sets the log_path_probs to 0.
    (log_path_probs,log_path_probs) ||> row_gather_map () (fun () config x i j_tns => local_map (const loop.zeroes) x)
    // Sets the seq_id to 0.
    tensor_set (ensemble_id, thread_id) 0 trace_state.seq_id

// Calculates the policy and the value array updates.
// Resets the trace state afterwards.
inl calculate_updates forall dim. 
        (size : sizes) 
        (model : model) (trace : trace) (trace_state : trace_state) 
        (ensemble_id : ensemble_id) 
        (reward : sa dim reward) =
    open primitives
    inl thread_id = rangem.threads_in_grid().from
    inl rewards : _ _ reward = tensorm.from reward
    assert (rewards.dim = size.player_id) "The rewards have to equal the number of players."

    // Calculates the updates.
    loop.forDown' {nearFrom=tensor_index (ensemble_id, thread_id) trace_state.seq_id; to=0} (fun seq_id =>
        inl {action_id player_id action_sampling_prob world_id} = tensor_index (ensemble_id, seq_id, thread_id) trace.actions
        inl reward = tensor_index player_id rewards
        inl table =
            model.table
            |> apply ensemble_id
            |> apply world_id
            |> apply_ptr
        inl path_prob =
            inl log_path_probs = 
                trace.log_path_probs
                |> apply ensemble_id
                |> apply seq_id
                |> apply thread_id
            inl log_path_policy_prob =
                log_path_probs
                |> rezip (fun x => x.policy)
                |> tensorm.mapi (fun i x => if player_id = i then 0 else x)
                |> tensorm.fold 0 (+)
            inl log_path_sampling_prob =
                log_path_probs
                |> rezip (fun x => x.sampling)
                |> tensorm.fold 0 (+)
            conv (exp (log_path_policy_prob - log_path_sampling_prob))

        inl () = // Calculates the value update
            inl value_update = table |> rezip (fun x => x.value.update)
            tensor_cuda.tensor_atomic_add action_id (reward * path_prob, path_prob) value_update
        
        inl policy_update = table |> rezip (fun x => x.policy.update)

        table
        |> rezip (fun x => x.policy.current, x.value.current)
        |> row_gather_reduce () (fun () config x thread_id j_tns => // row_gather shares the vars in lexical scope via shared memory
            inl policy_current, value_current = unzip x
            inl values = local_map (fun reward, count => if count <> 0 then reward / count else 0) value_current
            inl action_probs = local_masked_regret_matching (mask_of size.action_mask) config policy_current thread_id j_tns
            inl weighted_values =
                local_map (fun value, j => 
                    (if action_id = j then (reward - value) / action_sampling_prob else 0) + value
                    ) (zip values j_tns)
            inl expected_value =
                local_map (fun action_prob, value => action_prob * value) (zip action_probs weighted_values)
                |> local_sum config
            inl () = // Calculates the policy updates
                local_iter (fun value, action_id =>
                    tensor_cuda.tensor_atomic_add action_id (path_prob * (value - expected_value)) policy_update
                    ) (zip weighted_values j_tns)
            expected_value
            )
        |> fun (reward : reward) => 
            tensor_set player_id reward rewards
        )

    reset_trace_state trace_state ensemble_id

// Applies the policy and the value array updates.
// Clamps the average policy vector if applicable.
// Also randomizes the `exploratory_ensemble_id`.
inl apply_updates grid rng (model : model) =
    open tensor_cuda
    open primitives

    cooperative_groups.sync grid // TODO: This shouldn't be done here.

    inl table = model.table |> reorder (fun a,b,c => (a,b),c)

    if rangem.threads_in_grid().from = 0 then // We want to only change the id after all the threads have finished running the game.
        tensor_set {} (random.int_range {from=0; nearTo=fst model.table.dim} rng) model.exploratory_ensemble_id // Pick a random ensemble id
    __syncwarp() // Converge the first warp.
    
    (table, table) ||> grid_row_map grid (fun config table i j_tns =>
        inl has_been_updated = 
            table
            |> rezip (fun x => x.policy.update)
            |> local_map (fun x => x <> 0)
            |> local_reduce config false (fun a b => a || b)

        if has_been_updated then // This check here is to avoid adding noise to the average policy.
            inl policy =
                inl current =
                    table
                    |> rezip (fun {policy={average current update} value} => current, update)
                    |> local_map (fun current, update => max 0 (current + update)) // Clamps the policy array to 0 in addition to adding the policy updates.
                inl average =
                    inl action_probs = local_regret_matching config current
                    zip (table |> rezip (fun {policy={average}} => average)) 
                        action_probs
                    |> local_map (fun average,action_prob => average + action_prob)
                    |> local_l1_normalize average_threeshold() config // Decays the average policy if its norm is above the threeshold.
                zip current average
                |> rezip (fun current, average => {current average})
            inl value_current =
                table 
                |> local_map (fun {value={current update}} => loop.add current update) // Adds the value numerator and the denominator.
            (zip policy value_current, table)
            ||> local_map' (fun policy, current => { value = {current update = loop.zeroes}; policy = {policy with update = 0}}) 
        table
        )

// Updates the trace state log path probabilities.
inl update_trace_state_path_probs (trace_state : trace_state) {thread_id ensemble_id player_id} (action_prob, action_id : action_prob * action_id) : () =
    inl v = {
        sampling = log (conv action_prob.sampling)
        policy = log (conv action_prob.policy)
    }
    // Updates the log path probs in the trace_state.
    tensor_update (ensemble_id, thread_id, player_id) (loop.add v) trace_state.log_path_probs

// Pushes the ids into the trace along with the sampling probability for the selected action id.
// Increments the seq_id in the trace state and updates the log path probabilities.
inl push_trace (trace : trace) (trace_state : trace_state) {thread_id ensemble_id world_id player_id} (action_prob, action_id : action_prob * action_id) : () =
    inl seq_id = tensor_index (ensemble_id, thread_id) trace_state.seq_id
    // Increments the seq_id.
    tensor_set (ensemble_id, thread_id) (seq_id+1) trace_state.seq_id
    // Pushes the ids into the trace.
    tensor_set (ensemble_id, seq_id, thread_id) {world_id player_id action_id action_sampling_prob=action_prob.sampling} trace.actions
    inl trace_state_log_path_probs = trace_state.log_path_probs |> apply ensemble_id |> apply thread_id
    inl trace_log_path_probs = trace.log_path_probs |> apply ensemble_id |> apply seq_id |> apply thread_id
    // Copies trace_state_log_path_probs into trace_log_path_probs.
    (trace_state_log_path_probs, trace_log_path_probs) ||> primitives.row_gather_map () (fun () config x i j_tns => x)
    // Updates the log path probs in the trace_state.
    update_trace_state_path_probs trace_state {thread_id ensemble_id player_id} (action_prob, action_id)

inl test1() =
    open refm
    inl mask config x i j = j < 3
    inl sizes : sizes =
        {
            ensemble_id = 4
            world_id = 1 <<< 12
            player_id = 2
            action_id = 1 <<< 2
            action_mask = 3
            seq_id = 1 <<< 4
            thread_id = rangem.threads_in_grid().by // number of threads in grid
        }

    inl graph = graph(sizes)
    inl model = layers.create_model graph
    layers.pass_init model

    run fun () =>
        inl grid = cooperative_groups.create_grid()
        inl rng : _ random.philox_state = random.init {seed = random.clock64(); subsequence=conv rangem.threads_in_grid().from; offset=0}
        inl model, trace, trace_state : model * trace * trace_state = layers.graph_extract model {}

        inl thread_id = rangem.threads_in_grid().from

        let push (ensemble_id, world_id, player_id : ensemble_id * world_id * player_id) = 
            inl sampling, action_id = get_action rng sizes.action_mask model (ensemble_id, world_id)
            inl policy = get_policy_probs sizes.action_mask model (ensemble_id, world_id) action_id
            push_trace trace trace_state {thread_id ensemble_id world_id player_id} ({policy sampling}, action_id)

        push (0, 235, 0)
        push (0, 212, 1)
        push (0, 790, 0)
        push (0, 343, 1)
        push (0, 457, 0)
        push (0, 3447, 1)

        inl rewards : sa 2 reward = arraym.fromList [13; -13]
        calculate_updates sizes model trace trace_state 0 rewards
        apply_updates grid rng model

inl main() = test1()