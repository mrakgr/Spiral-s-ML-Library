// The Counter Factual Regret module

open corebase
open corecuda
open coreext
open tensorm

// Decays the average policy norm to this value after it exceeds it.
inl average_threeshold() : float = 100

type ensemble_id = int
type world_id = int
type player_id = int
type action_id = int
type seq_id = int
type thread_id = int
type prob = float
type log_prob = f64
type reward = float
type count = float
type log_path_prob = {sampling : log_prob; policy : log_prob}
type action_prob = {sampling : prob; policy : prob}

type trace_elem =
    {
        world_id : world_id
        action_id : action_id
        player_id : player_id
        action_sampling_prob : prob
    }

type trace =
    {
        actions : tensor (ensemble_id * seq_id * thread_id) trace_elem
        log_path_probs : tensor (ensemble_id * seq_id * thread_id * player_id) log_path_prob
    }

type model =
    tensor (ensemble_id * world_id * action_id) {
        policy : {
            average : prob
            current : prob
            update : prob
        }
        value : {
            current : (reward * count)
            update : (reward * count)
            }
    }

type sizes =
    {
        ensemble_id : int
        world_id : int
        player_id : int
        action_id : int
        seq_id : int
        thread_id : int
    }

inl graph (size : sizes) : _ (model * trace) =
    open partitionm
    inl model : partition model = !(size.ensemble_id, size.world_id, size.action_id)
    inl trace : partition trace = 
        !(size.ensemble_id, size.seq_id, size.thread_id) *. !(size.ensemble_id, size.seq_id, size.thread_id, size.player_id)
        |> reorder (fun (actions, log_path_probs) => { actions log_path_probs })

    inl init (x : model) : () = real
        struct.iter (fun x =>
            struct.iter (fun x =>
                $"!(x.array)[:] = 0" : ()
                ) x.bodies
        ) x
    layers.pair
        (layers.key_graph .cfr_model layers.Weight(model, init))
        (layers.key_graph .cfr_trace layers.Input(trace))

inl get_prob config (action_id : action_id) (probs : tensor _ (prob * action_id)) =
    open primitives
    local_reduce config (0,limit.max) (fun a b =>
        if snd a = action_id then a
        elif snd b = action_id then b
        else a
        ) probs
    |> fun prob,i => assert (i <> limit.max) "Expected a valid action id in get_action." . prob

// Samples an action from the model using the average policy (of the first thread) and returns both the action and the probabilities of it being selected.
inl get_policy_probs mask (model : model) ~(ensemble_id, world_id : ensemble_id * world_id) (action_id : action_id) : prob = join
    open primitives
    model
    |> apply ensemble_id
    |> apply world_id
    |> rezip (fun {policy} => policy.current)
    |> row_gather_reduce (fun config policy i j_tns =>
        open random
        inl policy_probs = local_masked_regret_matching mask config policy i j_tns
        get_prob config action_id (zip policy_probs j_tns)
        )

// Samples an action from the model using the average policy and returns both the action and the probabilities of it being selected.
inl get_action rng mask (model : model) ~(ensemble_id, world_id : ensemble_id * world_id) : action_prob * action_id = join
    open primitives
    model
    |> apply ensemble_id
    |> apply world_id
    |> rezip (fun {policy} => policy.average, policy.current)
    |> row_gather_reduce (fun config x i j_tns =>
        open random
        inl sampling, policy = unzip x
        inl sampling_probs, action_id = local_masked_regret_matching_and_discrete_sampling mask rng config sampling i j_tns
        // console.write_ln {tid=thread_index(); sampling_probs action_id}
        inl policy_probs = local_masked_regret_matching mask config policy i j_tns
        inl get_prob probs = get_prob config action_id (zip probs j_tns)
        // TODO: Get rid of returning the policy probs here.
        // The training loops needs to get all of them from the ensemble anyway, and the play loop does not need probs at all.
        {sampling=get_prob sampling_probs; policy=get_prob policy_probs}, action_id
        )

// Samples an action from the model using the average policy (of the first ensemble, and the first thread) 
// and returns both the action and the probabilities of it being selected.
// Takes in a `tensor ensemble_id world_id` as the last argument.
inl get_action_shared rng mask (model : model) ~(world_ids : tensor ensemble_id world_id) : {policy : tensor int prob; sampling : prob; action_id : action_id} = join
    open primitives
    inl {sampling}, action_id = 
        // TODO: Maybe this would lead to better exploration than just taking the first ensemble.
        // inl ensemble_id = random.int_range {from=0; nearTo=world_ids.dim} rng |> transposing_loop.shuffle 0
        inl ensemble_id = 0
        inl world_id = tensor_index ensemble_id world_ids
        inl {sampling}, action_id = get_action rng mask model (ensemble_id, world_id)
        transposing_loop.shuffle 0 ({sampling}, action_id)
    world_ids
    |> tensorm.mapi (fun ensemble_id world_id => get_policy_probs mask model (ensemble_id, world_id) action_id)
    |> fun policy => {policy sampling action_id}

// Calculates the policy and the value array updates.
inl calculate_updates mask (size : sizes) (model : model) (trace : trace) (ensemble_id, seq_id : ensemble_id * seq_id) (reward : list reward) =
    open primitives
    inl thread_id = rangem.threads_in_grid().from
    inl rewards : _ _ reward = tensorm.fromList reward
    assert (rewards.dim = size.player_id) "The rewards have to equal the number of players."

    loop.forDown' {nearFrom=seq_id; to=0} (fun seq_id =>
        inl {action_id player_id action_sampling_prob world_id} = tensor_index (ensemble_id, seq_id, thread_id) trace.actions
        inl reward = tensor_index player_id rewards
        inl model =
            model
            |> apply ensemble_id
            |> apply world_id
            |> apply_ptr
        inl log_path_probs =
            trace.log_path_probs
            |> apply ensemble_id
            |> apply seq_id
            |> apply_ptr
        inl path_prob =
            inl log_path_probs = log_path_probs |> apply thread_id
            inl log_path_policy_prob =
                log_path_probs
                |> rezip (fun x => x.policy)
                |> tensorm.mapi (fun i x => if player_id = i then 0 else x)
                |> tensorm.fold 0 (+)
            inl log_path_sampling_prob =
                log_path_probs
                |> rezip (fun x => x.sampling)
                |> tensorm.fold 0 (+)
            conv (exp (log_path_policy_prob - log_path_sampling_prob))

        inl () = // Calculates the value update
            inl value_update = model |> rezip (fun x => x.value.update)
            tensor_cuda.tensor_atomic_add action_id (reward * path_prob, path_prob) value_update
        
        inl policy_update = model |> rezip (fun x => x.policy.update)

        model
        |> rezip (fun x => x.policy.current, x.value.current)
        |> row_gather_reduce (fun config x thread_id j_tns => // row_gather shares the vars in lexical scope via shared memory
            inl policy_current, value_current = unzip x
            inl values = local_map (fun reward, count => if count <> 0 then reward / count else 0) value_current
            inl action_probs = local_masked_regret_matching mask config policy_current thread_id j_tns
            inl weighted_values =
                local_map (fun value, j => 
                    (if action_id = j then (reward - value) / action_sampling_prob else 0) + value
                    ) (zip values j_tns)
            inl expected_value =
                local_map (fun action_prob, value => action_prob * value) (zip action_probs weighted_values)
                |> local_sum config
            inl () = // Calculates the policy updates
                local_iter (fun value, action_id =>
                    tensor_cuda.tensor_atomic_add action_id (path_prob * (value - expected_value)) policy_update
                    ) (zip weighted_values j_tns)
            expected_value
            )
        |> fun (reward : reward) => 
            tensor_set player_id reward rewards
            // console.write_ln {rewards}
        )

// Applies the policy and the value array updates.
// Clamps the average policy vector if applicable.
inl apply_updates grid (model : model) =
    open tensor_cuda
    open primitives

    inl model = model |> reorder (fun a,b,c => (a,b),c)

    cooperative_groups.sync grid // Need to wait for all the atomics to calculate the updates
    (model, model) ||> grid_row_map grid (fun config model i j_tns =>
        inl policy =
            inl current =
                model
                |> rezip (fun {policy={average current update} value} => current, update)
                |> local_map (fun current, update => max 0 (current + update)) // Clamps the policy array to 0 in addition to adding the policy updates.
            inl average =
                inl action_probs = local_regret_matching config current
                zip (model |> rezip (fun {policy={average}} => average)) 
                    action_probs
                |> local_map (fun average,action_prob => average + action_prob)
                |> local_l1_normalize average_threeshold() config // Decays the average policy if its norm is above the threeshold.
            zip current average
            |> rezip (fun current, average => {current average})
        inl value_current =
            model 
            |> local_map (fun {value={current update}} => loop.add current update) // Adds the value numerator and the denominator.
        (zip policy value_current)
        |> local_map (fun policy, current => { value = {current update = loop.zeroes}; policy = {policy with update = 0} }) 
        )

inl test1() =
        open refm

        inl mask config x i j = j < 3

        inl sizes : sizes =
            {
                ensemble_id = 4
                world_id = 1 <<< 12
                player_id = 2
                action_id = 1 <<< 2
                seq_id = 1 <<< 4
                thread_id = rangem.threads_in_grid().by // number of threads in grid
            }

        inl graph = graph(sizes)
        inl model = layers.create_model graph
        layers.param_init model

        run fun () =>
            inl grid = cooperative_groups.create_grid()
            inl rng : _ random.philox_state = random.init {seed = $"clock64()"; subsequence=conv rangem.threads_in_grid().from; offset=0}
            inl model, trace : model * trace = layers.graph_extract model {}

            inl thread_id = rangem.threads_in_grid().from
            inl seq_id = ref (0 : int)

            inl log_path_prob : _ _ log_path_prob = tensor_create sizes.player_id
            loop.linear log_path_prob.dim fun i => tensor_set i {sampling=0; policy=0} log_path_prob

            inl push' (ensemble_id, world_id, player_id : ensemble_id * world_id * player_id) (action_prob, action_id : action_prob * action_id) : () =
                inl i = #seq_id
                seq_id <-# i+1
                tensor_set (ensemble_id, i, thread_id) {world_id player_id action_id action_sampling_prob=action_prob.sampling} trace.actions
                inl v = {
                    sampling = log (conv action_prob.sampling)
                    policy = log (conv action_prob.policy)
                }
                tensor_update player_id (loop.add v) log_path_prob
                inl trace_log_path_prob = trace.log_path_probs |> apply ensemble_id |> apply i |> apply thread_id
                loop.linear trace_log_path_prob.dim fun i =>
                    tensor_set i (tensor_index i log_path_prob) trace_log_path_prob

            let push (ensemble_id, world_id, player_id : ensemble_id * world_id * player_id) = push' (ensemble_id,world_id,player_id) (get_action rng mask model (ensemble_id, world_id))
            push (0, 235, 0)
            push (0, 212, 1)
            push (0, 790, 0)
            push (0, 343, 1)
            push (0, 457, 0)
            push (0, 3447, 1)

            calculate_updates mask sizes model trace (0, #seq_id) [13; -13]
            apply_updates grid model

inl main() = test1()